{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FractureDetection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOjrBFyPDDcexli1MwSNjkz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsaif/FractureDetection/blob/master/FractureDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOAC6mlB733f",
        "colab_type": "text"
      },
      "source": [
        "###Authenticate to access Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atu2tmvF0WgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdLFxEgD79Y_",
        "colab_type": "text"
      },
      "source": [
        "###Import supporting libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6P3EWR58Eb4",
        "colab_type": "code",
        "outputId": "268a5d4f-1227-4601-dbbb-4b9b4f95a7a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection\n",
        "from sklearn.preprocessing import MinMaxScaler, label_binarize\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import timeit\n",
        "\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "device = 'cpu'#torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vFbkvFe9TlS",
        "colab_type": "text"
      },
      "source": [
        "###Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ4mO-Ju9SW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LoadData(url,filename):\n",
        "  fluff, id = url.split('=')\n",
        "  downloaded = drive.CreateFile({'id':id}) \n",
        "  downloaded.GetContentFile(filename)\n",
        "  return filename\n",
        "\n",
        "LoadData(url=\"https://drive.google.com/open?id=1qnGa_lKiTHcmclC-__yLMV8QYksMFeLO\", filename=\"fractureintensitydata.csv\")\n",
        "dset = pd.read_csv(\"fractureintensitydata.csv\").set_index('WELL')\n",
        "print(dset.head(5))\n",
        "dset = dset.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwPqsVca-5xt",
        "colab_type": "text"
      },
      "source": [
        "###Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGelv1flAD-i",
        "colab_type": "text"
      },
      "source": [
        "####Calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhsC8o3X-5AO",
        "colab_type": "code",
        "outputId": "07f6192e-0628-4b3c-e039-b54cbe3361f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gr_sh = 250\n",
        "res_sh = 100\n",
        "dset['RES']=dset.ILD / dset.PROX\n",
        "dset['DENS']=dset.RHOB-dset.RHO_COR\n",
        "dset['PI']=((dset.GR/gr_sh)+(dset.ILD/res_sh))/(dset.RHOB/1000)\n",
        "dset['FRAC']=dset['INTENSITY'].apply(lambda x: (1 if x >1 else 0))\n",
        "\n",
        "#Threshold\n",
        "s = nn.Sigmoid()\n",
        "gr = s(torch.Tensor((dset.GR-50)/10))\n",
        "ild = s(torch.Tensor(np.log10(dset.ILD/dset.PROX)-0.05))\n",
        "rhob = s(torch.Tensor((2500-dset.RHOB)/1000))\n",
        "spi = s(torch.Tensor((dset.SPI-0.05)*100))\n",
        "dt = s(torch.Tensor((300-dset.DT)/100))\n",
        "Pb = 1-((1-gr)*(1-ild)*(1-rhob)*(1-spi)*(1-dt))\n",
        "Pce = 1-(np.log10(-np.log(gr)-np.log(ild)-np.log(rhob)-np.log(spi)-np.log(dt)))\n",
        "data = pd.DataFrame(np.vstack((gr, ild, rhob, spi, dt, Pb, Pce))).T\n",
        "data = data.rename(columns={0: \"GR_p\", 1: \"ILD_p\",2:\"RHOB_p\",3:\"SPI_p\",4:\"DT_p\",5:\"Pb\", 6:\"Pce\"})\n",
        "\n",
        "dset['Pb'] = Pb\n",
        "dset['Pce'] = Pce\n",
        "\n",
        "print(data.describe())\n",
        "print(dset.describe())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              GR_p        ILD_p       RHOB_p        SPI_p         DT_p  \\\n",
            "count  4856.000000  4856.000000  4856.000000  4856.000000  4856.000000   \n",
            "mean      0.904672     0.489055     0.518469     0.296750     0.662190   \n",
            "std       0.204153     0.090357     0.034597     0.379796     0.085580   \n",
            "min       0.078941     0.086147     0.424627     0.006693     0.290524   \n",
            "25%       0.945088     0.441610     0.500740     0.006693     0.627650   \n",
            "50%       0.999528     0.487779     0.520979     0.046934     0.682637   \n",
            "75%       0.999992     0.537176     0.541304     0.647985     0.730187   \n",
            "max       1.000000     0.865987     0.620947     0.999998     0.764620   \n",
            "\n",
            "                Pb          Pce  \n",
            "count  4856.000000  4856.000000  \n",
            "mean      0.992245     0.390669  \n",
            "std       0.017538     0.260525  \n",
            "min       0.903756     0.020705  \n",
            "25%       0.996304     0.152807  \n",
            "50%       0.999993     0.308459  \n",
            "75%       1.000000     0.666637  \n",
            "max       1.000000     0.956028  \n",
            "             DEPTH         CALI           DT           GR          ILD  \\\n",
            "count  4856.000000  4856.000000  4856.000000  4856.000000  4856.000000   \n",
            "mean   2163.469836     6.566341   230.815273   131.877921    54.787851   \n",
            "std      86.407012     0.815145    37.326497    61.723732   163.854279   \n",
            "min    1968.085200     5.628980   182.182000    25.431700     0.278767   \n",
            "25%    2103.517147     6.199063   200.442750    78.455450     7.016967   \n",
            "50%    2167.321980     6.303175   223.408350   126.588900    16.436000   \n",
            "75%    2239.288830     6.430793   247.785000   166.786500    51.642975   \n",
            "max    2311.899600    10.746400   389.284000   260.249000  2030.218000   \n",
            "\n",
            "              PROX         RHOB      RHO_COR          SPI    INTENSITY  \\\n",
            "count  4856.000000  4856.000000  4856.000000  4856.000000  4856.000000   \n",
            "mean     59.868507  2425.813571  2425.868243     0.031863     0.616557   \n",
            "std     134.669313   139.140582   139.113980     0.036335     0.912216   \n",
            "min       0.631018  2006.432000  2008.363000     0.000000     0.000000   \n",
            "25%       5.863890  2334.407750  2334.446750     0.000000     0.000000   \n",
            "50%      14.273300  2416.033000  2416.033000     0.019890     0.000000   \n",
            "75%      44.723525  2497.040000  2497.040000     0.056102     1.000000   \n",
            "max    1742.589000  2803.806000  2803.806000     0.182391     6.000000   \n",
            "\n",
            "               RES         DENS           PI         FRAC           Pb  \\\n",
            "count  4856.000000  4856.000000  4856.000000  4856.000000  4856.000000   \n",
            "mean      1.544031    -0.054671     0.441613     0.136120     0.992245   \n",
            "std       2.676245     0.558405     0.695107     0.342952     0.017538   \n",
            "min       0.004880   -15.075000     0.075874     0.000000     0.903756   \n",
            "25%       0.653689     0.000000     0.212936     0.000000     0.996304   \n",
            "50%       1.002549     0.000000     0.301751     0.000000     0.999993   \n",
            "75%       1.581165     0.000000     0.437780     0.000000     1.000000   \n",
            "max      82.400782     0.000000     8.656152     1.000000     1.000000   \n",
            "\n",
            "               Pce  \n",
            "count  4856.000000  \n",
            "mean      0.390669  \n",
            "std       0.260525  \n",
            "min       0.020705  \n",
            "25%       0.152807  \n",
            "50%       0.308459  \n",
            "75%       0.666637  \n",
            "max       0.956028  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZfM1jlJ_03K",
        "colab_type": "code",
        "outputId": "6ddf1270-8e56-4317-84ec-2f6e757dde75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "dset.to_csv(\"Pb_Pce.csv\")\n",
        "!cp Pb_Pce.csv \"/content/gdrive/My Drive/FracDet\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4P1-W5c_B8o",
        "colab_type": "text"
      },
      "source": [
        "####Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSIOu540_JOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.set()\n",
        "sns.pairplot(dset)\n",
        "\n",
        "corr = dset.corr()\n",
        "sns.heatmap(corr, cmap = 'YlGn', xticklabels=corr.columns,yticklabels=corr.columns, annot=True, linewidth = 5) \n",
        "\n",
        "#sns.pairplot(dset[[\"GR\", \"RHOB\", \"RHO_COR\", \"ILD\", \"PROX\", \"SPI\", \"DT\", \"CALI\", \"INTENSITY\"]], diag_kind=\"kde\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgdaL3zG_FSn",
        "colab_type": "text"
      },
      "source": [
        "####Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQbRJQNs_KA5",
        "colab_type": "code",
        "outputId": "ca966146-2242-4f27-8520-2c0f0c195491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "dataset = dset[['DEPTH','GR','RES','DENS','RHOB','SPI','DT','PI','INTENSITY','Pb','Pce','FRAC']].copy()\n",
        "db = dataset[dataset.index == 'JTB-118']\n",
        "#db = pd.concat([db,dataset[dataset.index == 'JTB-093']],sort=False)\n",
        "X = db.iloc[:,1:8].values\n",
        "Y = db.iloc[:,-1].values.reshape(-1,1)\n",
        "#Y = label_binarize(Y, classes=[0,1])\n",
        "\n",
        "test = 0.20\n",
        "seed = 0\n",
        "batch = 500\n",
        "X_train, X_val, Y_train, Y_val = model_selection.train_test_split(X, Y, test_size=test, random_state=seed)\n",
        "\n",
        "#scaler = MinMaxScaler()\n",
        "#Y_train = scaler.fit_transform(Y_train)\n",
        "#Y_val = scaler.transform(Y_val)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "X_train = torch.from_numpy(X_train).float().to(device)\n",
        "Y_train = torch.from_numpy(Y_train).to(device)\n",
        "X_val = torch.from_numpy(X_val).float().to(device)\n",
        "Y_val = torch.from_numpy(Y_val).to(device)\n",
        "\n",
        "trainset = TensorDataset(X_train, Y_train)\n",
        "trainloader = DataLoader(trainset, batch_size=batch, shuffle=True)\n",
        "\n",
        "valset = TensorDataset(X_val, Y_val)\n",
        "valloader = DataLoader(valset, batch_size=batch, shuffle=True)\n",
        "\n",
        "print (Y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1],\n",
            "        [0],\n",
            "        [0],\n",
            "        ...,\n",
            "        [0],\n",
            "        [1],\n",
            "        [0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyWmQ2peA26W",
        "colab_type": "text"
      },
      "source": [
        "###Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-i4tCIRA8Mn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.norm = nn.BatchNorm1d(7)\n",
        "        self.hidden = nn.Linear(7, 5)\n",
        "        self.output = nn.Linear(5, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.logsoftmax = nn.LogSoftmax()\n",
        "        self.softmax = nn.Softmax()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        x = self.hidden(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.output(x)\n",
        "        x = self.logsoftmax(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yhfy4WYUBLMq",
        "colab_type": "text"
      },
      "source": [
        "###Train, test, predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnhrGCOJBZ0i",
        "colab_type": "code",
        "outputId": "04c85ad5-91aa-45bf-fb09-e26dd5f39b8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Network()\n",
        "#model.cuda()\n",
        "Xtrain, Ytrain = next(iter(trainloader))\n",
        "Xval, Yval = next(iter(valloader))\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(),lr= 0.001)\n",
        "epoch = 2000\n",
        "loss_ = []\n",
        "loss_test = []\n",
        "for i in range(epoch):\n",
        "    running_loss = 0\n",
        "    runningloss_test = 0\n",
        "    for Xtrain, Ytrain in trainloader:\n",
        "        Ytrain = Ytrain.view(-1)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(Xtrain)\n",
        "        #print(out.shape)\n",
        "        #print(Ytrain.shape)\n",
        "        loss = criterion(out,Ytrain)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        Yval = Yval.view(-1)\n",
        "        with torch.no_grad():\n",
        "          logps = model(Xval)\n",
        "        predict = torch.exp(logps).argmax(1)\n",
        "        runningloss_test = criterion(logps,Yval).cpu().numpy()\n",
        "        #loss_test += runningloss_test.item()\n",
        "        print(f\"Training loss: {running_loss/len(trainloader)}\")\n",
        "        print(f\"Val loss: {runningloss_test/len(valloader)}\")\n",
        "        loss_test.append(runningloss_test/len(valloader))\n",
        "        loss_.append(running_loss/len(trainloader))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training loss: 0.76650337378184\n",
            "Val loss: 0.7586061358451843\n",
            "Training loss: 0.7578711907068888\n",
            "Val loss: 0.7498055100440979\n",
            "Training loss: 0.7493775884310404\n",
            "Val loss: 0.7411872148513794\n",
            "Training loss: 0.7408887346585592\n",
            "Val loss: 0.7327303290367126\n",
            "Training loss: 0.7325194279352824\n",
            "Val loss: 0.7244328856468201\n",
            "Training loss: 0.7244000832239786\n",
            "Val loss: 0.7162908315658569\n",
            "Training loss: 0.7162408630053202\n",
            "Val loss: 0.7083063721656799\n",
            "Training loss: 0.7084654966990153\n",
            "Val loss: 0.7004724144935608\n",
            "Training loss: 0.7007182041803995\n",
            "Val loss: 0.6927797794342041\n",
            "Training loss: 0.692744235197703\n",
            "Val loss: 0.6852160692214966\n",
            "Training loss: 0.685737152894338\n",
            "Val loss: 0.6777889132499695\n",
            "Training loss: 0.6784660816192627\n",
            "Val loss: 0.6704841256141663\n",
            "Training loss: 0.6710504492123922\n",
            "Val loss: 0.6633282899856567\n",
            "Training loss: 0.6644445260365804\n",
            "Val loss: 0.6562906503677368\n",
            "Training loss: 0.6568192640940348\n",
            "Val loss: 0.6493300795555115\n",
            "Training loss: 0.6506486932436625\n",
            "Val loss: 0.6424506902694702\n",
            "Training loss: 0.6429289182027181\n",
            "Val loss: 0.6356340646743774\n",
            "Training loss: 0.6361968318621317\n",
            "Val loss: 0.628944456577301\n",
            "Training loss: 0.6300030748049418\n",
            "Val loss: 0.6223013401031494\n",
            "Training loss: 0.6239240964253744\n",
            "Val loss: 0.6157621145248413\n",
            "Training loss: 0.6168799797693888\n",
            "Val loss: 0.6093037128448486\n",
            "Training loss: 0.6106182336807251\n",
            "Val loss: 0.6029342412948608\n",
            "Training loss: 0.6041218042373657\n",
            "Val loss: 0.5966576337814331\n",
            "Training loss: 0.5977567434310913\n",
            "Val loss: 0.5904590487480164\n",
            "Training loss: 0.5916053851445516\n",
            "Val loss: 0.5843598246574402\n",
            "Training loss: 0.5861483613650004\n",
            "Val loss: 0.5783141851425171\n",
            "Training loss: 0.5795002579689026\n",
            "Val loss: 0.5724071860313416\n",
            "Training loss: 0.5733452836672465\n",
            "Val loss: 0.5665836334228516\n",
            "Training loss: 0.5685923099517822\n",
            "Val loss: 0.5608462691307068\n",
            "Training loss: 0.5615360140800476\n",
            "Val loss: 0.5552123188972473\n",
            "Training loss: 0.5564675132433573\n",
            "Val loss: 0.5496548414230347\n",
            "Training loss: 0.5503626863161722\n",
            "Val loss: 0.5442121624946594\n",
            "Training loss: 0.5454186002413431\n",
            "Val loss: 0.5388322472572327\n",
            "Training loss: 0.5401509801546732\n",
            "Val loss: 0.5335326194763184\n",
            "Training loss: 0.5346574187278748\n",
            "Val loss: 0.5283213257789612\n",
            "Training loss: 0.5297696590423584\n",
            "Val loss: 0.5231834053993225\n",
            "Training loss: 0.5238587657610575\n",
            "Val loss: 0.5181540250778198\n",
            "Training loss: 0.5189053018887838\n",
            "Val loss: 0.5132063031196594\n",
            "Training loss: 0.5152159333229065\n",
            "Val loss: 0.5083392858505249\n",
            "Training loss: 0.508798082669576\n",
            "Val loss: 0.5036295056343079\n",
            "Training loss: 0.5038563013076782\n",
            "Val loss: 0.4990387260913849\n",
            "Training loss: 0.4991060396035512\n",
            "Val loss: 0.4945529103279114\n",
            "Training loss: 0.4946068227291107\n",
            "Val loss: 0.49012285470962524\n",
            "Training loss: 0.48960381746292114\n",
            "Val loss: 0.485819548368454\n",
            "Training loss: 0.48631444573402405\n",
            "Val loss: 0.481632262468338\n",
            "Training loss: 0.48208027084668476\n",
            "Val loss: 0.4775635302066803\n",
            "Training loss: 0.47718512018521625\n",
            "Val loss: 0.4736325144767761\n",
            "Training loss: 0.47298262516657513\n",
            "Val loss: 0.4697974622249603\n",
            "Training loss: 0.4685683349768321\n",
            "Val loss: 0.4661136269569397\n",
            "Training loss: 0.4654879967371623\n",
            "Val loss: 0.462525874376297\n",
            "Training loss: 0.46186383565266925\n",
            "Val loss: 0.45903685688972473\n",
            "Training loss: 0.4571143686771393\n",
            "Val loss: 0.4556868076324463\n",
            "Training loss: 0.45374812682469684\n",
            "Val loss: 0.45244812965393066\n",
            "Training loss: 0.45117419958114624\n",
            "Val loss: 0.4492941200733185\n",
            "Training loss: 0.44824743270874023\n",
            "Val loss: 0.44627171754837036\n",
            "Training loss: 0.445930153131485\n",
            "Val loss: 0.44330835342407227\n",
            "Training loss: 0.44145192702611286\n",
            "Val loss: 0.4405023455619812\n",
            "Training loss: 0.4385611911614736\n",
            "Val loss: 0.43780648708343506\n",
            "Training loss: 0.4355756839116414\n",
            "Val loss: 0.4352101683616638\n",
            "Training loss: 0.43331024050712585\n",
            "Val loss: 0.4327313303947449\n",
            "Training loss: 0.4321424663066864\n",
            "Val loss: 0.4303288757801056\n",
            "Training loss: 0.4277304808298747\n",
            "Val loss: 0.428049772977829\n",
            "Training loss: 0.426017165184021\n",
            "Val loss: 0.4258664846420288\n",
            "Training loss: 0.42430410782496136\n",
            "Val loss: 0.423797070980072\n",
            "Training loss: 0.42133862773577374\n",
            "Val loss: 0.4218190908432007\n",
            "Training loss: 0.41924471656481427\n",
            "Val loss: 0.41989386081695557\n",
            "Training loss: 0.4180776874224345\n",
            "Val loss: 0.41803696751594543\n",
            "Training loss: 0.4151853720347087\n",
            "Val loss: 0.4162779450416565\n",
            "Training loss: 0.4141986072063446\n",
            "Val loss: 0.4145865738391876\n",
            "Training loss: 0.4143494665622711\n",
            "Val loss: 0.4129815101623535\n",
            "Training loss: 0.4109813670317332\n",
            "Val loss: 0.41146066784858704\n",
            "Training loss: 0.40824273228645325\n",
            "Val loss: 0.41002973914146423\n",
            "Training loss: 0.40809311469395954\n",
            "Val loss: 0.4086547791957855\n",
            "Training loss: 0.4081740478674571\n",
            "Val loss: 0.40734389424324036\n",
            "Training loss: 0.40374388297398883\n",
            "Val loss: 0.4061378240585327\n",
            "Training loss: 0.4034743706385295\n",
            "Val loss: 0.40498703718185425\n",
            "Training loss: 0.40162961681683856\n",
            "Val loss: 0.4039049744606018\n",
            "Training loss: 0.4022953013579051\n",
            "Val loss: 0.40285956859588623\n",
            "Training loss: 0.3997998535633087\n",
            "Val loss: 0.40189477801322937\n",
            "Training loss: 0.39890538652737934\n",
            "Val loss: 0.40097305178642273\n",
            "Training loss: 0.3962554434935252\n",
            "Val loss: 0.40009331703186035\n",
            "Training loss: 0.3981282413005829\n",
            "Val loss: 0.3992631137371063\n",
            "Training loss: 0.3968992332617442\n",
            "Val loss: 0.39846131205558777\n",
            "Training loss: 0.3950256605943044\n",
            "Val loss: 0.3977176249027252\n",
            "Training loss: 0.39560434222221375\n",
            "Val loss: 0.3970048427581787\n",
            "Training loss: 0.3956935703754425\n",
            "Val loss: 0.3963511884212494\n",
            "Training loss: 0.39385536313056946\n",
            "Val loss: 0.3957042992115021\n",
            "Training loss: 0.39298603932062787\n",
            "Val loss: 0.3951025605201721\n",
            "Training loss: 0.3936146895090739\n",
            "Val loss: 0.39452555775642395\n",
            "Training loss: 0.3896702428658803\n",
            "Val loss: 0.3939830958843231\n",
            "Training loss: 0.3911438783009847\n",
            "Val loss: 0.3934553265571594\n",
            "Training loss: 0.38985782861709595\n",
            "Val loss: 0.39297404885292053\n",
            "Training loss: 0.39011673132578534\n",
            "Val loss: 0.39250266551971436\n",
            "Training loss: 0.38868897159894306\n",
            "Val loss: 0.3920711576938629\n",
            "Training loss: 0.390237033367157\n",
            "Val loss: 0.39163535833358765\n",
            "Training loss: 0.38784374793370563\n",
            "Val loss: 0.39122670888900757\n",
            "Training loss: 0.38933658599853516\n",
            "Val loss: 0.3908296823501587\n",
            "Training loss: 0.38727356990178424\n",
            "Val loss: 0.39044973254203796\n",
            "Training loss: 0.38691752155621845\n",
            "Val loss: 0.3900913596153259\n",
            "Training loss: 0.3872535030047099\n",
            "Val loss: 0.38976556062698364\n",
            "Training loss: 0.38745593031247455\n",
            "Val loss: 0.38943254947662354\n",
            "Training loss: 0.3862534165382385\n",
            "Val loss: 0.38911011815071106\n",
            "Training loss: 0.38732749223709106\n",
            "Val loss: 0.3887956142425537\n",
            "Training loss: 0.3868074317773183\n",
            "Val loss: 0.3884923458099365\n",
            "Training loss: 0.38699163993199664\n",
            "Val loss: 0.3881949186325073\n",
            "Training loss: 0.38386958837509155\n",
            "Val loss: 0.3879132866859436\n",
            "Training loss: 0.38506317138671875\n",
            "Val loss: 0.38763800263404846\n",
            "Training loss: 0.3828015824158986\n",
            "Val loss: 0.3873597979545593\n",
            "Training loss: 0.3846903244654338\n",
            "Val loss: 0.3870987892150879\n",
            "Training loss: 0.3821140428384145\n",
            "Val loss: 0.38683998584747314\n",
            "Training loss: 0.3835474451382955\n",
            "Val loss: 0.38659346103668213\n",
            "Training loss: 0.3827207883199056\n",
            "Val loss: 0.3863523602485657\n",
            "Training loss: 0.384006808201472\n",
            "Val loss: 0.3861321210861206\n",
            "Training loss: 0.38265883922576904\n",
            "Val loss: 0.38590505719184875\n",
            "Training loss: 0.38302314281463623\n",
            "Val loss: 0.38567009568214417\n",
            "Training loss: 0.38263994455337524\n",
            "Val loss: 0.3854438066482544\n",
            "Training loss: 0.38215654095013935\n",
            "Val loss: 0.38520926237106323\n",
            "Training loss: 0.381946861743927\n",
            "Val loss: 0.3849678933620453\n",
            "Training loss: 0.38066835204760235\n",
            "Val loss: 0.38474151492118835\n",
            "Training loss: 0.38160036007563275\n",
            "Val loss: 0.38452982902526855\n",
            "Training loss: 0.37936047712961835\n",
            "Val loss: 0.38431990146636963\n",
            "Training loss: 0.38057247797648114\n",
            "Val loss: 0.38411468267440796\n",
            "Training loss: 0.3803631166617076\n",
            "Val loss: 0.3839071989059448\n",
            "Training loss: 0.3792375127474467\n",
            "Val loss: 0.3837113082408905\n",
            "Training loss: 0.3811490635077159\n",
            "Val loss: 0.38352447748184204\n",
            "Training loss: 0.3791596591472626\n",
            "Val loss: 0.3833351135253906\n",
            "Training loss: 0.3794827659924825\n",
            "Val loss: 0.38314345479011536\n",
            "Training loss: 0.37896058956782025\n",
            "Val loss: 0.382935106754303\n",
            "Training loss: 0.379265159368515\n",
            "Val loss: 0.38275134563446045\n",
            "Training loss: 0.3780188361803691\n",
            "Val loss: 0.38256776332855225\n",
            "Training loss: 0.3800317943096161\n",
            "Val loss: 0.3823643922805786\n",
            "Training loss: 0.37637953956921893\n",
            "Val loss: 0.3821781575679779\n",
            "Training loss: 0.37886034448941547\n",
            "Val loss: 0.3819797933101654\n",
            "Training loss: 0.37787912289301556\n",
            "Val loss: 0.3817957043647766\n",
            "Training loss: 0.3778660496075948\n",
            "Val loss: 0.3816109001636505\n",
            "Training loss: 0.3785013258457184\n",
            "Val loss: 0.38141560554504395\n",
            "Training loss: 0.3781019647916158\n",
            "Val loss: 0.38121432065963745\n",
            "Training loss: 0.3779609004656474\n",
            "Val loss: 0.3810296654701233\n",
            "Training loss: 0.37694888313611347\n",
            "Val loss: 0.38085123896598816\n",
            "Training loss: 0.37733890612920123\n",
            "Val loss: 0.3806787133216858\n",
            "Training loss: 0.3757604459921519\n",
            "Val loss: 0.3804910182952881\n",
            "Training loss: 0.3755149443944295\n",
            "Val loss: 0.3803088963031769\n",
            "Training loss: 0.3750593463579814\n",
            "Val loss: 0.3801194429397583\n",
            "Training loss: 0.37668561935424805\n",
            "Val loss: 0.3799245357513428\n",
            "Training loss: 0.37484272321065265\n",
            "Val loss: 0.379726380109787\n",
            "Training loss: 0.37597978115081787\n",
            "Val loss: 0.37955060601234436\n",
            "Training loss: 0.3755764166514079\n",
            "Val loss: 0.37936052680015564\n",
            "Training loss: 0.3752782344818115\n",
            "Val loss: 0.3791674077510834\n",
            "Training loss: 0.3774745563666026\n",
            "Val loss: 0.3789542615413666\n",
            "Training loss: 0.37719473242759705\n",
            "Val loss: 0.37876150012016296\n",
            "Training loss: 0.3759480019410451\n",
            "Val loss: 0.3785524368286133\n",
            "Training loss: 0.3743521471818288\n",
            "Val loss: 0.37835660576820374\n",
            "Training loss: 0.3768170475959778\n",
            "Val loss: 0.37817853689193726\n",
            "Training loss: 0.37516068418820697\n",
            "Val loss: 0.3780113756656647\n",
            "Training loss: 0.37467987338701886\n",
            "Val loss: 0.37785783410072327\n",
            "Training loss: 0.373002549012502\n",
            "Val loss: 0.37770819664001465\n",
            "Training loss: 0.373290220896403\n",
            "Val loss: 0.3775474727153778\n",
            "Training loss: 0.3742186327775319\n",
            "Val loss: 0.37739673256874084\n",
            "Training loss: 0.3721121648947398\n",
            "Val loss: 0.3772289454936981\n",
            "Training loss: 0.3720748722553253\n",
            "Val loss: 0.3770855963230133\n",
            "Training loss: 0.37116310993830365\n",
            "Val loss: 0.37693148851394653\n",
            "Training loss: 0.3740595579147339\n",
            "Val loss: 0.3767765164375305\n",
            "Training loss: 0.37210842967033386\n",
            "Val loss: 0.3766428232192993\n",
            "Training loss: 0.37188853820164997\n",
            "Val loss: 0.3764878213405609\n",
            "Training loss: 0.3723620076974233\n",
            "Val loss: 0.37635043263435364\n",
            "Training loss: 0.37045784791310626\n",
            "Val loss: 0.37619760632514954\n",
            "Training loss: 0.37172144651412964\n",
            "Val loss: 0.3760523200035095\n",
            "Training loss: 0.37101927399635315\n",
            "Val loss: 0.3759153187274933\n",
            "Training loss: 0.3713999291261037\n",
            "Val loss: 0.3757568895816803\n",
            "Training loss: 0.3709431787331899\n",
            "Val loss: 0.3756016790866852\n",
            "Training loss: 0.37367204825083417\n",
            "Val loss: 0.3754303753376007\n",
            "Training loss: 0.37124399344126385\n",
            "Val loss: 0.37525537610054016\n",
            "Training loss: 0.3720163305600484\n",
            "Val loss: 0.3750848174095154\n",
            "Training loss: 0.3709729313850403\n",
            "Val loss: 0.374937504529953\n",
            "Training loss: 0.3708864053090413\n",
            "Val loss: 0.3747897446155548\n",
            "Training loss: 0.3707027832667033\n",
            "Val loss: 0.37464237213134766\n",
            "Training loss: 0.3697417577107747\n",
            "Val loss: 0.37452030181884766\n",
            "Training loss: 0.3698434631029765\n",
            "Val loss: 0.3743651211261749\n",
            "Training loss: 0.3691280682881673\n",
            "Val loss: 0.3742208778858185\n",
            "Training loss: 0.36970239877700806\n",
            "Val loss: 0.3740849196910858\n",
            "Training loss: 0.369755357503891\n",
            "Val loss: 0.3739449083805084\n",
            "Training loss: 0.3697640399138133\n",
            "Val loss: 0.3738184869289398\n",
            "Training loss: 0.3690889577070872\n",
            "Val loss: 0.3736793100833893\n",
            "Training loss: 0.3690288265546163\n",
            "Val loss: 0.37354663014411926\n",
            "Training loss: 0.3691554069519043\n",
            "Val loss: 0.3733600974082947\n",
            "Training loss: 0.3688021202882131\n",
            "Val loss: 0.3731849491596222\n",
            "Training loss: 0.37010907133420307\n",
            "Val loss: 0.37300798296928406\n",
            "Training loss: 0.3689564863840739\n",
            "Val loss: 0.3728208839893341\n",
            "Training loss: 0.37011855840682983\n",
            "Val loss: 0.3726344704627991\n",
            "Training loss: 0.3681578536828359\n",
            "Val loss: 0.3724503517150879\n",
            "Training loss: 0.3682161867618561\n",
            "Val loss: 0.3722588121891022\n",
            "Training loss: 0.36763885617256165\n",
            "Val loss: 0.372079998254776\n",
            "Training loss: 0.36770660678545636\n",
            "Val loss: 0.37190037965774536\n",
            "Training loss: 0.3676170806090037\n",
            "Val loss: 0.3717392683029175\n",
            "Training loss: 0.36773768067359924\n",
            "Val loss: 0.3715982437133789\n",
            "Training loss: 0.36749377846717834\n",
            "Val loss: 0.3714419901371002\n",
            "Training loss: 0.36662232875823975\n",
            "Val loss: 0.37128472328186035\n",
            "Training loss: 0.3680435021718343\n",
            "Val loss: 0.3711327314376831\n",
            "Training loss: 0.36824139952659607\n",
            "Val loss: 0.37099629640579224\n",
            "Training loss: 0.36726683378219604\n",
            "Val loss: 0.3708600401878357\n",
            "Training loss: 0.3662325342496236\n",
            "Val loss: 0.37071967124938965\n",
            "Training loss: 0.3679921329021454\n",
            "Val loss: 0.37055879831314087\n",
            "Training loss: 0.3664301534493764\n",
            "Val loss: 0.3704463839530945\n",
            "Training loss: 0.3658199707667033\n",
            "Val loss: 0.3703249394893646\n",
            "Training loss: 0.3657420078913371\n",
            "Val loss: 0.37018316984176636\n",
            "Training loss: 0.3664344747861226\n",
            "Val loss: 0.3700633645057678\n",
            "Training loss: 0.3660553793112437\n",
            "Val loss: 0.3699690103530884\n",
            "Training loss: 0.36623915036519367\n",
            "Val loss: 0.3698748052120209\n",
            "Training loss: 0.36903731028238934\n",
            "Val loss: 0.3697722256183624\n",
            "Training loss: 0.3661562502384186\n",
            "Val loss: 0.36967357993125916\n",
            "Training loss: 0.36694374680519104\n",
            "Val loss: 0.3695606291294098\n",
            "Training loss: 0.366750031709671\n",
            "Val loss: 0.3694725036621094\n",
            "Training loss: 0.3668748637040456\n",
            "Val loss: 0.36937108635902405\n",
            "Training loss: 0.367972989877065\n",
            "Val loss: 0.36927616596221924\n",
            "Training loss: 0.3642595112323761\n",
            "Val loss: 0.3691982328891754\n",
            "Training loss: 0.36671948432922363\n",
            "Val loss: 0.36912083625793457\n",
            "Training loss: 0.3641514281431834\n",
            "Val loss: 0.3690071105957031\n",
            "Training loss: 0.36499743660291034\n",
            "Val loss: 0.36890989542007446\n",
            "Training loss: 0.36493780215581256\n",
            "Val loss: 0.36881279945373535\n",
            "Training loss: 0.36445337533950806\n",
            "Val loss: 0.3687228560447693\n",
            "Training loss: 0.3655098080635071\n",
            "Val loss: 0.3686583340167999\n",
            "Training loss: 0.36375516653060913\n",
            "Val loss: 0.368592232465744\n",
            "Training loss: 0.36741910378138226\n",
            "Val loss: 0.3685171604156494\n",
            "Training loss: 0.3647000590960185\n",
            "Val loss: 0.3684336841106415\n",
            "Training loss: 0.3636239469051361\n",
            "Val loss: 0.3683643043041229\n",
            "Training loss: 0.3649155795574188\n",
            "Val loss: 0.3683026134967804\n",
            "Training loss: 0.3643554449081421\n",
            "Val loss: 0.3682154417037964\n",
            "Training loss: 0.3643932243188222\n",
            "Val loss: 0.3681153953075409\n",
            "Training loss: 0.3649715880552928\n",
            "Val loss: 0.36804652214050293\n",
            "Training loss: 0.3639248013496399\n",
            "Val loss: 0.36796241998672485\n",
            "Training loss: 0.36485356092453003\n",
            "Val loss: 0.36788541078567505\n",
            "Training loss: 0.36480356256167096\n",
            "Val loss: 0.367811381816864\n",
            "Training loss: 0.3625141183535258\n",
            "Val loss: 0.3677434027194977\n",
            "Training loss: 0.36603986223538715\n",
            "Val loss: 0.36766690015792847\n",
            "Training loss: 0.363937109708786\n",
            "Val loss: 0.36763444542884827\n",
            "Training loss: 0.3634830017884572\n",
            "Val loss: 0.36756497621536255\n",
            "Training loss: 0.36318297187487286\n",
            "Val loss: 0.3674761652946472\n",
            "Training loss: 0.36405227581659955\n",
            "Val loss: 0.36739465594291687\n",
            "Training loss: 0.3636385997136434\n",
            "Val loss: 0.3673413395881653\n",
            "Training loss: 0.362713485956192\n",
            "Val loss: 0.36729949712753296\n",
            "Training loss: 0.3629935085773468\n",
            "Val loss: 0.3672335743904114\n",
            "Training loss: 0.3630984326203664\n",
            "Val loss: 0.3671830892562866\n",
            "Training loss: 0.3619365294774373\n",
            "Val loss: 0.36711201071739197\n",
            "Training loss: 0.36251429716746014\n",
            "Val loss: 0.36705663800239563\n",
            "Training loss: 0.3629547158877055\n",
            "Val loss: 0.3669781982898712\n",
            "Training loss: 0.3628612458705902\n",
            "Val loss: 0.3669246733188629\n",
            "Training loss: 0.3649819493293762\n",
            "Val loss: 0.36681437492370605\n",
            "Training loss: 0.36156431833902997\n",
            "Val loss: 0.36674734950065613\n",
            "Training loss: 0.3619932333628337\n",
            "Val loss: 0.3666979670524597\n",
            "Training loss: 0.3594954013824463\n",
            "Val loss: 0.3666367530822754\n",
            "Training loss: 0.36065812905629474\n",
            "Val loss: 0.36660826206207275\n",
            "Training loss: 0.3616219063599904\n",
            "Val loss: 0.36656808853149414\n",
            "Training loss: 0.36284324526786804\n",
            "Val loss: 0.36648738384246826\n",
            "Training loss: 0.3618944187959035\n",
            "Val loss: 0.3664247393608093\n",
            "Training loss: 0.36282386382420856\n",
            "Val loss: 0.36631837487220764\n",
            "Training loss: 0.36181603868802387\n",
            "Val loss: 0.366193950176239\n",
            "Training loss: 0.3624650339285533\n",
            "Val loss: 0.36610132455825806\n",
            "Training loss: 0.3614727556705475\n",
            "Val loss: 0.36598894000053406\n",
            "Training loss: 0.36237825949986774\n",
            "Val loss: 0.36588481068611145\n",
            "Training loss: 0.3603910307089488\n",
            "Val loss: 0.3658013939857483\n",
            "Training loss: 0.3644132415453593\n",
            "Val loss: 0.3657274842262268\n",
            "Training loss: 0.3611862361431122\n",
            "Val loss: 0.3656434714794159\n",
            "Training loss: 0.3620985150337219\n",
            "Val loss: 0.36558082699775696\n",
            "Training loss: 0.3598241110642751\n",
            "Val loss: 0.36552250385284424\n",
            "Training loss: 0.36077311635017395\n",
            "Val loss: 0.3654458522796631\n",
            "Training loss: 0.3628196616967519\n",
            "Val loss: 0.3653632700443268\n",
            "Training loss: 0.3628169397513072\n",
            "Val loss: 0.3652755916118622\n",
            "Training loss: 0.36312835415204364\n",
            "Val loss: 0.36520856618881226\n",
            "Training loss: 0.3604831596215566\n",
            "Val loss: 0.3651571571826935\n",
            "Training loss: 0.3602656622727712\n",
            "Val loss: 0.36507582664489746\n",
            "Training loss: 0.35878483454386395\n",
            "Val loss: 0.3650171756744385\n",
            "Training loss: 0.3607627550760905\n",
            "Val loss: 0.3649536669254303\n",
            "Training loss: 0.36256088813145954\n",
            "Val loss: 0.36489856243133545\n",
            "Training loss: 0.3607259889443715\n",
            "Val loss: 0.3648429214954376\n",
            "Training loss: 0.36097170909245807\n",
            "Val loss: 0.36474597454071045\n",
            "Training loss: 0.35834232966105145\n",
            "Val loss: 0.3646947741508484\n",
            "Training loss: 0.3601954976717631\n",
            "Val loss: 0.3646470904350281\n",
            "Training loss: 0.3599475920200348\n",
            "Val loss: 0.36456966400146484\n",
            "Training loss: 0.3599531849225362\n",
            "Val loss: 0.3645002245903015\n",
            "Training loss: 0.3611647387345632\n",
            "Val loss: 0.364449679851532\n",
            "Training loss: 0.35996092359224957\n",
            "Val loss: 0.36440160870552063\n",
            "Training loss: 0.3606022596359253\n",
            "Val loss: 0.36436495184898376\n",
            "Training loss: 0.3596964379151662\n",
            "Val loss: 0.3643110990524292\n",
            "Training loss: 0.35924169421195984\n",
            "Val loss: 0.3642841875553131\n",
            "Training loss: 0.3607783416906993\n",
            "Val loss: 0.3642287254333496\n",
            "Training loss: 0.36000462373097736\n",
            "Val loss: 0.3641514480113983\n",
            "Training loss: 0.35816747943560284\n",
            "Val loss: 0.3641042709350586\n",
            "Training loss: 0.3586241404215495\n",
            "Val loss: 0.36404070258140564\n",
            "Training loss: 0.35972389578819275\n",
            "Val loss: 0.36395078897476196\n",
            "Training loss: 0.3586473862330119\n",
            "Val loss: 0.3638917803764343\n",
            "Training loss: 0.35922495524088544\n",
            "Val loss: 0.36380913853645325\n",
            "Training loss: 0.3619805574417114\n",
            "Val loss: 0.36372286081314087\n",
            "Training loss: 0.36214683453241986\n",
            "Val loss: 0.36369016766548157\n",
            "Training loss: 0.3593013385931651\n",
            "Val loss: 0.36363551020622253\n",
            "Training loss: 0.3577661414941152\n",
            "Val loss: 0.363611102104187\n",
            "Training loss: 0.3593008816242218\n",
            "Val loss: 0.36354175209999084\n",
            "Training loss: 0.35963478684425354\n",
            "Val loss: 0.3634869456291199\n",
            "Training loss: 0.35634498794873554\n",
            "Val loss: 0.36340826749801636\n",
            "Training loss: 0.3592236042022705\n",
            "Val loss: 0.3632946312427521\n",
            "Training loss: 0.3582264880339305\n",
            "Val loss: 0.3632237911224365\n",
            "Training loss: 0.35850808024406433\n",
            "Val loss: 0.36315441131591797\n",
            "Training loss: 0.3608367443084717\n",
            "Val loss: 0.3630787432193756\n",
            "Training loss: 0.3574543396631877\n",
            "Val loss: 0.3629967272281647\n",
            "Training loss: 0.3600277006626129\n",
            "Val loss: 0.3629015386104584\n",
            "Training loss: 0.358404407898585\n",
            "Val loss: 0.36286455392837524\n",
            "Training loss: 0.35792775948842365\n",
            "Val loss: 0.3628236651420593\n",
            "Training loss: 0.35906651616096497\n",
            "Val loss: 0.36275947093963623\n",
            "Training loss: 0.3578340212504069\n",
            "Val loss: 0.3626999855041504\n",
            "Training loss: 0.35690049330393475\n",
            "Val loss: 0.3626879155635834\n",
            "Training loss: 0.3588702181975047\n",
            "Val loss: 0.36264458298683167\n",
            "Training loss: 0.35824689269065857\n",
            "Val loss: 0.3625943958759308\n",
            "Training loss: 0.3561130960782369\n",
            "Val loss: 0.3625643849372864\n",
            "Training loss: 0.358532836039861\n",
            "Val loss: 0.3625278174877167\n",
            "Training loss: 0.3573493758837382\n",
            "Val loss: 0.3625147342681885\n",
            "Training loss: 0.3567160964012146\n",
            "Val loss: 0.3624359369277954\n",
            "Training loss: 0.3567831019560496\n",
            "Val loss: 0.3623732328414917\n",
            "Training loss: 0.3571850160757701\n",
            "Val loss: 0.3623066544532776\n",
            "Training loss: 0.3571211099624634\n",
            "Val loss: 0.36224398016929626\n",
            "Training loss: 0.3595071037610372\n",
            "Val loss: 0.36216238141059875\n",
            "Training loss: 0.3575967748959859\n",
            "Val loss: 0.3620971441268921\n",
            "Training loss: 0.3578232129414876\n",
            "Val loss: 0.3620520234107971\n",
            "Training loss: 0.35798044006029767\n",
            "Val loss: 0.36199241876602173\n",
            "Training loss: 0.3568614621957143\n",
            "Val loss: 0.36194634437561035\n",
            "Training loss: 0.3576909005641937\n",
            "Val loss: 0.361902117729187\n",
            "Training loss: 0.35787567496299744\n",
            "Val loss: 0.3618102967739105\n",
            "Training loss: 0.35576356450716656\n",
            "Val loss: 0.36176183819770813\n",
            "Training loss: 0.35755935311317444\n",
            "Val loss: 0.3617100119590759\n",
            "Training loss: 0.35543093085289\n",
            "Val loss: 0.36163029074668884\n",
            "Training loss: 0.35820310314496356\n",
            "Val loss: 0.36153843998908997\n",
            "Training loss: 0.3582459092140198\n",
            "Val loss: 0.3614591956138611\n",
            "Training loss: 0.3566786249478658\n",
            "Val loss: 0.3613997995853424\n",
            "Training loss: 0.3566357692082723\n",
            "Val loss: 0.3613237738609314\n",
            "Training loss: 0.35715503493944806\n",
            "Val loss: 0.3612426519393921\n",
            "Training loss: 0.3580271204312642\n",
            "Val loss: 0.3611558675765991\n",
            "Training loss: 0.35528457164764404\n",
            "Val loss: 0.36112189292907715\n",
            "Training loss: 0.35611625512441\n",
            "Val loss: 0.3610660433769226\n",
            "Training loss: 0.3564607898394267\n",
            "Val loss: 0.36096328496932983\n",
            "Training loss: 0.35509897271792096\n",
            "Val loss: 0.36093655228614807\n",
            "Training loss: 0.3554997245470683\n",
            "Val loss: 0.3609157204627991\n",
            "Training loss: 0.3568091591199239\n",
            "Val loss: 0.3608483672142029\n",
            "Training loss: 0.3581948975721995\n",
            "Val loss: 0.36080417037010193\n",
            "Training loss: 0.35774457454681396\n",
            "Val loss: 0.3607768714427948\n",
            "Training loss: 0.35689132412274677\n",
            "Val loss: 0.36073723435401917\n",
            "Training loss: 0.3562157948811849\n",
            "Val loss: 0.3606737554073334\n",
            "Training loss: 0.3575068712234497\n",
            "Val loss: 0.3605630397796631\n",
            "Training loss: 0.3564147353172302\n",
            "Val loss: 0.3604702055454254\n",
            "Training loss: 0.355413685242335\n",
            "Val loss: 0.36040350794792175\n",
            "Training loss: 0.35709519187609357\n",
            "Val loss: 0.3603241741657257\n",
            "Training loss: 0.35571661591529846\n",
            "Val loss: 0.36027756333351135\n",
            "Training loss: 0.35599198937416077\n",
            "Val loss: 0.3602279722690582\n",
            "Training loss: 0.3537519872188568\n",
            "Val loss: 0.3602072298526764\n",
            "Training loss: 0.3544410765171051\n",
            "Val loss: 0.3601472079753876\n",
            "Training loss: 0.3564754128456116\n",
            "Val loss: 0.3601190447807312\n",
            "Training loss: 0.35528939962387085\n",
            "Val loss: 0.3600711524486542\n",
            "Training loss: 0.35760555664698285\n",
            "Val loss: 0.3600098192691803\n",
            "Training loss: 0.3553650577863057\n",
            "Val loss: 0.3599744141101837\n",
            "Training loss: 0.3549010157585144\n",
            "Val loss: 0.35996556282043457\n",
            "Training loss: 0.35488227009773254\n",
            "Val loss: 0.3599279224872589\n",
            "Training loss: 0.35522838433583576\n",
            "Val loss: 0.35993438959121704\n",
            "Training loss: 0.3581623335679372\n",
            "Val loss: 0.35994330048561096\n",
            "Training loss: 0.35677380363146466\n",
            "Val loss: 0.3599800169467926\n",
            "Training loss: 0.35653971632321674\n",
            "Val loss: 0.35996687412261963\n",
            "Training loss: 0.3549991846084595\n",
            "Val loss: 0.3599318563938141\n",
            "Training loss: 0.3556470771630605\n",
            "Val loss: 0.35989660024642944\n",
            "Training loss: 0.3561561107635498\n",
            "Val loss: 0.35983237624168396\n",
            "Training loss: 0.35450461506843567\n",
            "Val loss: 0.35982784628868103\n",
            "Training loss: 0.35768325130144757\n",
            "Val loss: 0.3598659932613373\n",
            "Training loss: 0.353963166475296\n",
            "Val loss: 0.359767884016037\n",
            "Training loss: 0.3565222422281901\n",
            "Val loss: 0.35965725779533386\n",
            "Training loss: 0.3553013503551483\n",
            "Val loss: 0.3595771789550781\n",
            "Training loss: 0.35452882448832196\n",
            "Val loss: 0.3594852387905121\n",
            "Training loss: 0.3555024067560832\n",
            "Val loss: 0.3594403564929962\n",
            "Training loss: 0.3547138273715973\n",
            "Val loss: 0.35939428210258484\n",
            "Training loss: 0.35705111424128216\n",
            "Val loss: 0.35931602120399475\n",
            "Training loss: 0.35442864894866943\n",
            "Val loss: 0.3592836558818817\n",
            "Training loss: 0.35517338911692303\n",
            "Val loss: 0.35927045345306396\n",
            "Training loss: 0.3542771538098653\n",
            "Val loss: 0.3592274487018585\n",
            "Training loss: 0.35592490434646606\n",
            "Val loss: 0.3591825067996979\n",
            "Training loss: 0.3546106318632762\n",
            "Val loss: 0.35916242003440857\n",
            "Training loss: 0.35667191942532855\n",
            "Val loss: 0.35911521315574646\n",
            "Training loss: 0.35351933042208356\n",
            "Val loss: 0.35908418893814087\n",
            "Training loss: 0.35575802127520245\n",
            "Val loss: 0.359029620885849\n",
            "Training loss: 0.35651060938835144\n",
            "Val loss: 0.3590227961540222\n",
            "Training loss: 0.35532549023628235\n",
            "Val loss: 0.35899609327316284\n",
            "Training loss: 0.35691022872924805\n",
            "Val loss: 0.3589504361152649\n",
            "Training loss: 0.35620900988578796\n",
            "Val loss: 0.3589172065258026\n",
            "Training loss: 0.3547692696253459\n",
            "Val loss: 0.3588826656341553\n",
            "Training loss: 0.35463371872901917\n",
            "Val loss: 0.3588334918022156\n",
            "Training loss: 0.35504939158757526\n",
            "Val loss: 0.3587914705276489\n",
            "Training loss: 0.35569538672765094\n",
            "Val loss: 0.3587300777435303\n",
            "Training loss: 0.353878657023112\n",
            "Val loss: 0.35868412256240845\n",
            "Training loss: 0.35560113191604614\n",
            "Val loss: 0.3586469888687134\n",
            "Training loss: 0.35305365920066833\n",
            "Val loss: 0.35862791538238525\n",
            "Training loss: 0.3547521233558655\n",
            "Val loss: 0.3586365878582001\n",
            "Training loss: 0.35531917214393616\n",
            "Val loss: 0.3586007058620453\n",
            "Training loss: 0.3551132182280223\n",
            "Val loss: 0.35856786370277405\n",
            "Training loss: 0.35429154833157855\n",
            "Val loss: 0.35855433344841003\n",
            "Training loss: 0.3549061020215352\n",
            "Val loss: 0.3584989905357361\n",
            "Training loss: 0.3555297354857127\n",
            "Val loss: 0.3584906756877899\n",
            "Training loss: 0.3544185360272725\n",
            "Val loss: 0.3584478497505188\n",
            "Training loss: 0.35566969712575275\n",
            "Val loss: 0.35838645696640015\n",
            "Training loss: 0.3543211817741394\n",
            "Val loss: 0.35833948850631714\n",
            "Training loss: 0.35240838925043744\n",
            "Val loss: 0.3582703769207001\n",
            "Training loss: 0.35384981830914813\n",
            "Val loss: 0.35824474692344666\n",
            "Training loss: 0.3533870279788971\n",
            "Val loss: 0.3581988513469696\n",
            "Training loss: 0.35362739364306134\n",
            "Val loss: 0.35813045501708984\n",
            "Training loss: 0.3552570343017578\n",
            "Val loss: 0.3580276668071747\n",
            "Training loss: 0.3531488577524821\n",
            "Val loss: 0.35799553990364075\n",
            "Training loss: 0.3531781534353892\n",
            "Val loss: 0.3579033315181732\n",
            "Training loss: 0.3566981454690297\n",
            "Val loss: 0.3578987121582031\n",
            "Training loss: 0.3540329833825429\n",
            "Val loss: 0.3578324019908905\n",
            "Training loss: 0.35510923465092975\n",
            "Val loss: 0.35775506496429443\n",
            "Training loss: 0.3548370699087779\n",
            "Val loss: 0.3577035367488861\n",
            "Training loss: 0.35452714562416077\n",
            "Val loss: 0.35771799087524414\n",
            "Training loss: 0.3545541266600291\n",
            "Val loss: 0.3576885163784027\n",
            "Training loss: 0.3528532882531484\n",
            "Val loss: 0.3576638400554657\n",
            "Training loss: 0.3530687789122264\n",
            "Val loss: 0.3576258420944214\n",
            "Training loss: 0.35404019554456073\n",
            "Val loss: 0.3576328456401825\n",
            "Training loss: 0.35562388102213544\n",
            "Val loss: 0.35761404037475586\n",
            "Training loss: 0.35421008865038556\n",
            "Val loss: 0.35758712887763977\n",
            "Training loss: 0.3542591830094655\n",
            "Val loss: 0.3575670123100281\n",
            "Training loss: 0.3569419781366984\n",
            "Val loss: 0.3575150668621063\n",
            "Training loss: 0.35449275374412537\n",
            "Val loss: 0.35748621821403503\n",
            "Training loss: 0.3546273112297058\n",
            "Val loss: 0.3574821650981903\n",
            "Training loss: 0.3537125488122304\n",
            "Val loss: 0.3574161231517792\n",
            "Training loss: 0.35447800159454346\n",
            "Val loss: 0.35739561915397644\n",
            "Training loss: 0.35353856285413104\n",
            "Val loss: 0.3573437035083771\n",
            "Training loss: 0.35412004590034485\n",
            "Val loss: 0.35733819007873535\n",
            "Training loss: 0.35414767265319824\n",
            "Val loss: 0.35730981826782227\n",
            "Training loss: 0.35417358080546063\n",
            "Val loss: 0.3572816848754883\n",
            "Training loss: 0.3533010482788086\n",
            "Val loss: 0.35724279284477234\n",
            "Training loss: 0.3520969748497009\n",
            "Val loss: 0.3572104871273041\n",
            "Training loss: 0.352865606546402\n",
            "Val loss: 0.35716187953948975\n",
            "Training loss: 0.35295387109120685\n",
            "Val loss: 0.35713711380958557\n",
            "Training loss: 0.35480570793151855\n",
            "Val loss: 0.35710471868515015\n",
            "Training loss: 0.35568434993426007\n",
            "Val loss: 0.3570822477340698\n",
            "Training loss: 0.35265429814656574\n",
            "Val loss: 0.3570443391799927\n",
            "Training loss: 0.35258140166600543\n",
            "Val loss: 0.3570457696914673\n",
            "Training loss: 0.35212116440137226\n",
            "Val loss: 0.3570517599582672\n",
            "Training loss: 0.35426729917526245\n",
            "Val loss: 0.3570551574230194\n",
            "Training loss: 0.3533695836861928\n",
            "Val loss: 0.35704728960990906\n",
            "Training loss: 0.3529665966828664\n",
            "Val loss: 0.35701784491539\n",
            "Training loss: 0.3526699940363566\n",
            "Val loss: 0.35702112317085266\n",
            "Training loss: 0.353620578845342\n",
            "Val loss: 0.35703781247138977\n",
            "Training loss: 0.3541856010754903\n",
            "Val loss: 0.35699254274368286\n",
            "Training loss: 0.35364288091659546\n",
            "Val loss: 0.3569694757461548\n",
            "Training loss: 0.35298221309979755\n",
            "Val loss: 0.35692524909973145\n",
            "Training loss: 0.3531080385049184\n",
            "Val loss: 0.3569110929965973\n",
            "Training loss: 0.3530786832173665\n",
            "Val loss: 0.35691943764686584\n",
            "Training loss: 0.3544913629690806\n",
            "Val loss: 0.35691532492637634\n",
            "Training loss: 0.35205655296643573\n",
            "Val loss: 0.35691243410110474\n",
            "Training loss: 0.35248051087061566\n",
            "Val loss: 0.35690370202064514\n",
            "Training loss: 0.35134028395016986\n",
            "Val loss: 0.3568786680698395\n",
            "Training loss: 0.35498297214508057\n",
            "Val loss: 0.35690465569496155\n",
            "Training loss: 0.35314027468363446\n",
            "Val loss: 0.35688379406929016\n",
            "Training loss: 0.35103825728098553\n",
            "Val loss: 0.3568255305290222\n",
            "Training loss: 0.3528437713781993\n",
            "Val loss: 0.3567872941493988\n",
            "Training loss: 0.35343945026397705\n",
            "Val loss: 0.3567860424518585\n",
            "Training loss: 0.35243234038352966\n",
            "Val loss: 0.3567948341369629\n",
            "Training loss: 0.3554193576176961\n",
            "Val loss: 0.3567716181278229\n",
            "Training loss: 0.35284977157910663\n",
            "Val loss: 0.3567501902580261\n",
            "Training loss: 0.3529441754023234\n",
            "Val loss: 0.3566978871822357\n",
            "Training loss: 0.352691908677419\n",
            "Val loss: 0.35664594173431396\n",
            "Training loss: 0.3529532353083293\n",
            "Val loss: 0.356577068567276\n",
            "Training loss: 0.3551187515258789\n",
            "Val loss: 0.3565445840358734\n",
            "Training loss: 0.3537244995435079\n",
            "Val loss: 0.3565429449081421\n",
            "Training loss: 0.35265928506851196\n",
            "Val loss: 0.3565446734428406\n",
            "Training loss: 0.3545583387215932\n",
            "Val loss: 0.3565504550933838\n",
            "Training loss: 0.352629433075587\n",
            "Val loss: 0.3565128445625305\n",
            "Training loss: 0.3510158161322276\n",
            "Val loss: 0.35651326179504395\n",
            "Training loss: 0.353258619705836\n",
            "Val loss: 0.3564892113208771\n",
            "Training loss: 0.3533172905445099\n",
            "Val loss: 0.3564600646495819\n",
            "Training loss: 0.3576297461986542\n",
            "Val loss: 0.3563900589942932\n",
            "Training loss: 0.35353219509124756\n",
            "Val loss: 0.35638079047203064\n",
            "Training loss: 0.3523007531960805\n",
            "Val loss: 0.356344610452652\n",
            "Training loss: 0.3525257309277852\n",
            "Val loss: 0.3563218116760254\n",
            "Training loss: 0.3508980969587962\n",
            "Val loss: 0.35629066824913025\n",
            "Training loss: 0.3540627459685008\n",
            "Val loss: 0.3562738299369812\n",
            "Training loss: 0.3521211047967275\n",
            "Val loss: 0.3562561273574829\n",
            "Training loss: 0.3541822334130605\n",
            "Val loss: 0.3562500774860382\n",
            "Training loss: 0.3523438473542531\n",
            "Val loss: 0.35616064071655273\n",
            "Training loss: 0.3528922696908315\n",
            "Val loss: 0.3561629056930542\n",
            "Training loss: 0.354189137617747\n",
            "Val loss: 0.35607829689979553\n",
            "Training loss: 0.3545951346556346\n",
            "Val loss: 0.35602661967277527\n",
            "Training loss: 0.3518633743127187\n",
            "Val loss: 0.3560371398925781\n",
            "Training loss: 0.3527344763278961\n",
            "Val loss: 0.3560295104980469\n",
            "Training loss: 0.3548938532670339\n",
            "Val loss: 0.3559788167476654\n",
            "Training loss: 0.3525030215581258\n",
            "Val loss: 0.3560149371623993\n",
            "Training loss: 0.3521726429462433\n",
            "Val loss: 0.3560352921485901\n",
            "Training loss: 0.35098718603452045\n",
            "Val loss: 0.35607898235321045\n",
            "Training loss: 0.3539140820503235\n",
            "Val loss: 0.35607391595840454\n",
            "Training loss: 0.35232946276664734\n",
            "Val loss: 0.3560270369052887\n",
            "Training loss: 0.35230688254038495\n",
            "Val loss: 0.3560056686401367\n",
            "Training loss: 0.3536281883716583\n",
            "Val loss: 0.3560173511505127\n",
            "Training loss: 0.353659192721049\n",
            "Val loss: 0.35600706934928894\n",
            "Training loss: 0.3520851532618205\n",
            "Val loss: 0.3559888005256653\n",
            "Training loss: 0.35284145673116046\n",
            "Val loss: 0.3559516370296478\n",
            "Training loss: 0.35436851779619855\n",
            "Val loss: 0.3559550940990448\n",
            "Training loss: 0.35252581040064496\n",
            "Val loss: 0.35591262578964233\n",
            "Training loss: 0.3521704177061717\n",
            "Val loss: 0.35591328144073486\n",
            "Training loss: 0.35275739431381226\n",
            "Val loss: 0.3558996915817261\n",
            "Training loss: 0.3529447217782338\n",
            "Val loss: 0.3558880388736725\n",
            "Training loss: 0.3504460354646047\n",
            "Val loss: 0.35591891407966614\n",
            "Training loss: 0.3515775998433431\n",
            "Val loss: 0.35590875148773193\n",
            "Training loss: 0.3528762459754944\n",
            "Val loss: 0.35587701201438904\n",
            "Training loss: 0.35228342811266583\n",
            "Val loss: 0.3558807671070099\n",
            "Training loss: 0.35278870662053424\n",
            "Val loss: 0.35588040947914124\n",
            "Training loss: 0.3524178663889567\n",
            "Val loss: 0.355895459651947\n",
            "Training loss: 0.3529401520888011\n",
            "Val loss: 0.3558498024940491\n",
            "Training loss: 0.3519837061564128\n",
            "Val loss: 0.3558526039123535\n",
            "Training loss: 0.351408710082372\n",
            "Val loss: 0.35584184527397156\n",
            "Training loss: 0.3525797526041667\n",
            "Val loss: 0.3557717800140381\n",
            "Training loss: 0.3521312475204468\n",
            "Val loss: 0.3557482361793518\n",
            "Training loss: 0.3520711064338684\n",
            "Val loss: 0.35576269030570984\n",
            "Training loss: 0.3516608377297719\n",
            "Val loss: 0.3556666076183319\n",
            "Training loss: 0.3532875080903371\n",
            "Val loss: 0.3555942177772522\n",
            "Training loss: 0.3519944449265798\n",
            "Val loss: 0.35550862550735474\n",
            "Training loss: 0.35281580686569214\n",
            "Val loss: 0.35545098781585693\n",
            "Training loss: 0.350901593764623\n",
            "Val loss: 0.35543105006217957\n",
            "Training loss: 0.35039270917574566\n",
            "Val loss: 0.35543349385261536\n",
            "Training loss: 0.3530014952023824\n",
            "Val loss: 0.35548052191734314\n",
            "Training loss: 0.3518859048684438\n",
            "Val loss: 0.35552844405174255\n",
            "Training loss: 0.3519894778728485\n",
            "Val loss: 0.35549527406692505\n",
            "Training loss: 0.3532623052597046\n",
            "Val loss: 0.35544878244400024\n",
            "Training loss: 0.3529023329416911\n",
            "Val loss: 0.3554668128490448\n",
            "Training loss: 0.3512683113416036\n",
            "Val loss: 0.355446457862854\n",
            "Training loss: 0.35063016414642334\n",
            "Val loss: 0.3554510176181793\n",
            "Training loss: 0.3521829644838969\n",
            "Val loss: 0.3554357588291168\n",
            "Training loss: 0.3518523673216502\n",
            "Val loss: 0.3553815186023712\n",
            "Training loss: 0.3506652315457662\n",
            "Val loss: 0.35531046986579895\n",
            "Training loss: 0.351906418800354\n",
            "Val loss: 0.35527127981185913\n",
            "Training loss: 0.35035960872968036\n",
            "Val loss: 0.35521793365478516\n",
            "Training loss: 0.3515365918477376\n",
            "Val loss: 0.35515323281288147\n",
            "Training loss: 0.3510078291098277\n",
            "Val loss: 0.35510626435279846\n",
            "Training loss: 0.35155774156252545\n",
            "Val loss: 0.35512399673461914\n",
            "Training loss: 0.3519059220949809\n",
            "Val loss: 0.35509148240089417\n",
            "Training loss: 0.35237165292104083\n",
            "Val loss: 0.3550558090209961\n",
            "Training loss: 0.35169782241185504\n",
            "Val loss: 0.3550228774547577\n",
            "Training loss: 0.3511767586072286\n",
            "Val loss: 0.3550128638744354\n",
            "Training loss: 0.3519145945707957\n",
            "Val loss: 0.3550458550453186\n",
            "Training loss: 0.3521215220292409\n",
            "Val loss: 0.3550513982772827\n",
            "Training loss: 0.3531115750471751\n",
            "Val loss: 0.3549683392047882\n",
            "Training loss: 0.35025136669476825\n",
            "Val loss: 0.3549584746360779\n",
            "Training loss: 0.35259958108266193\n",
            "Val loss: 0.35489657521247864\n",
            "Training loss: 0.35171956817309064\n",
            "Val loss: 0.35486510396003723\n",
            "Training loss: 0.351394921541214\n",
            "Val loss: 0.35480251908302307\n",
            "Training loss: 0.351510226726532\n",
            "Val loss: 0.3547467589378357\n",
            "Training loss: 0.35560381412506104\n",
            "Val loss: 0.3547358214855194\n",
            "Training loss: 0.3518291215101878\n",
            "Val loss: 0.35471251606941223\n",
            "Training loss: 0.35086239377657574\n",
            "Val loss: 0.35469284653663635\n",
            "Training loss: 0.3483692208925883\n",
            "Val loss: 0.3547132611274719\n",
            "Training loss: 0.3512397309144338\n",
            "Val loss: 0.35469087958335876\n",
            "Training loss: 0.35006681084632874\n",
            "Val loss: 0.35470372438430786\n",
            "Training loss: 0.34948153297106427\n",
            "Val loss: 0.3546791076660156\n",
            "Training loss: 0.3521318236986796\n",
            "Val loss: 0.3546866178512573\n",
            "Training loss: 0.3515029350916545\n",
            "Val loss: 0.3546886146068573\n",
            "Training loss: 0.35091401139895123\n",
            "Val loss: 0.35465148091316223\n",
            "Training loss: 0.35073596239089966\n",
            "Val loss: 0.3546340763568878\n",
            "Training loss: 0.3507448534170787\n",
            "Val loss: 0.35463985800743103\n",
            "Training loss: 0.3509914179642995\n",
            "Val loss: 0.3545912802219391\n",
            "Training loss: 0.3497287333011627\n",
            "Val loss: 0.35453692078590393\n",
            "Training loss: 0.3508451581001282\n",
            "Val loss: 0.35451602935791016\n",
            "Training loss: 0.3511304259300232\n",
            "Val loss: 0.35449257493019104\n",
            "Training loss: 0.35227187474568683\n",
            "Val loss: 0.3544997274875641\n",
            "Training loss: 0.3525988260904948\n",
            "Val loss: 0.35439932346343994\n",
            "Training loss: 0.35048550367355347\n",
            "Val loss: 0.3543700873851776\n",
            "Training loss: 0.3507036864757538\n",
            "Val loss: 0.35434094071388245\n",
            "Training loss: 0.35009663303693134\n",
            "Val loss: 0.3543112576007843\n",
            "Training loss: 0.3531861404577891\n",
            "Val loss: 0.35421374440193176\n",
            "Training loss: 0.35135077436765033\n",
            "Val loss: 0.3542131185531616\n",
            "Training loss: 0.3504410187403361\n",
            "Val loss: 0.3541540503501892\n",
            "Training loss: 0.35037784775098163\n",
            "Val loss: 0.3541351854801178\n",
            "Training loss: 0.3501671055952708\n",
            "Val loss: 0.3540380001068115\n",
            "Training loss: 0.35017411907513935\n",
            "Val loss: 0.35398226976394653\n",
            "Training loss: 0.3508446017901103\n",
            "Val loss: 0.35395053029060364\n",
            "Training loss: 0.3527444700400035\n",
            "Val loss: 0.35384678840637207\n",
            "Training loss: 0.3519453803698222\n",
            "Val loss: 0.35381266474723816\n",
            "Training loss: 0.35394002000490826\n",
            "Val loss: 0.3538026809692383\n",
            "Training loss: 0.3521558443705241\n",
            "Val loss: 0.3537755608558655\n",
            "Training loss: 0.35263408223787945\n",
            "Val loss: 0.35376784205436707\n",
            "Training loss: 0.3510182897249858\n",
            "Val loss: 0.3537323474884033\n",
            "Training loss: 0.3518998821576436\n",
            "Val loss: 0.35371410846710205\n",
            "Training loss: 0.35244617859522503\n",
            "Val loss: 0.3536554276943207\n",
            "Training loss: 0.3500898877779643\n",
            "Val loss: 0.3536064922809601\n",
            "Training loss: 0.3495860695838928\n",
            "Val loss: 0.3535540699958801\n",
            "Training loss: 0.3492438892523448\n",
            "Val loss: 0.35347694158554077\n",
            "Training loss: 0.3522142171859741\n",
            "Val loss: 0.35342758893966675\n",
            "Training loss: 0.3488316436608632\n",
            "Val loss: 0.35333573818206787\n",
            "Training loss: 0.3510601222515106\n",
            "Val loss: 0.35330435633659363\n",
            "Training loss: 0.35145048300425213\n",
            "Val loss: 0.35327449440956116\n",
            "Training loss: 0.35229381918907166\n",
            "Val loss: 0.3532102704048157\n",
            "Training loss: 0.3506473700205485\n",
            "Val loss: 0.3531924784183502\n",
            "Training loss: 0.3517044583956401\n",
            "Val loss: 0.353153258562088\n",
            "Training loss: 0.34830187757809955\n",
            "Val loss: 0.35315290093421936\n",
            "Training loss: 0.34997748335202533\n",
            "Val loss: 0.35314851999282837\n",
            "Training loss: 0.3505312502384186\n",
            "Val loss: 0.35316795110702515\n",
            "Training loss: 0.3503006199995677\n",
            "Val loss: 0.3531743586063385\n",
            "Training loss: 0.35162582000096637\n",
            "Val loss: 0.3531971573829651\n",
            "Training loss: 0.3499608337879181\n",
            "Val loss: 0.35320737957954407\n",
            "Training loss: 0.35057611266771954\n",
            "Val loss: 0.35320454835891724\n",
            "Training loss: 0.35073474049568176\n",
            "Val loss: 0.3532077670097351\n",
            "Training loss: 0.3504615326722463\n",
            "Val loss: 0.3532203435897827\n",
            "Training loss: 0.3493161102135976\n",
            "Val loss: 0.35321733355522156\n",
            "Training loss: 0.3507330119609833\n",
            "Val loss: 0.3531975746154785\n",
            "Training loss: 0.3487424850463867\n",
            "Val loss: 0.35318759083747864\n",
            "Training loss: 0.3487808307011922\n",
            "Val loss: 0.35320377349853516\n",
            "Training loss: 0.351410448551178\n",
            "Val loss: 0.3531249463558197\n",
            "Training loss: 0.35052262743314105\n",
            "Val loss: 0.35312867164611816\n",
            "Training loss: 0.34995559851328534\n",
            "Val loss: 0.3530917763710022\n",
            "Training loss: 0.3510424892107646\n",
            "Val loss: 0.35309532284736633\n",
            "Training loss: 0.3538587788740794\n",
            "Val loss: 0.3530395030975342\n",
            "Training loss: 0.35157907009124756\n",
            "Val loss: 0.3529217541217804\n",
            "Training loss: 0.35003138581911725\n",
            "Val loss: 0.35285112261772156\n",
            "Training loss: 0.35086238384246826\n",
            "Val loss: 0.35273900628089905\n",
            "Training loss: 0.3509780863920848\n",
            "Val loss: 0.3526422381401062\n",
            "Training loss: 0.3521923820177714\n",
            "Val loss: 0.3525718152523041\n",
            "Training loss: 0.3506503701210022\n",
            "Val loss: 0.35251346230506897\n",
            "Training loss: 0.3490898211797078\n",
            "Val loss: 0.3524613082408905\n",
            "Training loss: 0.3507542510827382\n",
            "Val loss: 0.3523918092250824\n",
            "Training loss: 0.3494968314965566\n",
            "Val loss: 0.3523656129837036\n",
            "Training loss: 0.3493304153283437\n",
            "Val loss: 0.3523315489292145\n",
            "Training loss: 0.35212881366411847\n",
            "Val loss: 0.3523089587688446\n",
            "Training loss: 0.3499579032262166\n",
            "Val loss: 0.3522758185863495\n",
            "Training loss: 0.3508680860201518\n",
            "Val loss: 0.35224470496177673\n",
            "Training loss: 0.35061978300412494\n",
            "Val loss: 0.3522171974182129\n",
            "Training loss: 0.3536752760410309\n",
            "Val loss: 0.3522789776325226\n",
            "Training loss: 0.3499288558959961\n",
            "Val loss: 0.352223664522171\n",
            "Training loss: 0.35042964418729144\n",
            "Val loss: 0.35223355889320374\n",
            "Training loss: 0.3485555648803711\n",
            "Val loss: 0.3522096276283264\n",
            "Training loss: 0.34943222006162006\n",
            "Val loss: 0.3521493375301361\n",
            "Training loss: 0.3498018781344096\n",
            "Val loss: 0.35210543870925903\n",
            "Training loss: 0.3527838885784149\n",
            "Val loss: 0.35206493735313416\n",
            "Training loss: 0.35247182846069336\n",
            "Val loss: 0.3520249128341675\n",
            "Training loss: 0.35097964604695636\n",
            "Val loss: 0.3519324064254761\n",
            "Training loss: 0.3488282759984334\n",
            "Val loss: 0.3518582880496979\n",
            "Training loss: 0.34793118635813397\n",
            "Val loss: 0.35179075598716736\n",
            "Training loss: 0.3505423168341319\n",
            "Val loss: 0.35177502036094666\n",
            "Training loss: 0.3513321280479431\n",
            "Val loss: 0.35167330503463745\n",
            "Training loss: 0.34795334935188293\n",
            "Val loss: 0.35161688923835754\n",
            "Training loss: 0.34907583395640057\n",
            "Val loss: 0.3515792787075043\n",
            "Training loss: 0.349357932806015\n",
            "Val loss: 0.35152289271354675\n",
            "Training loss: 0.3501381079355876\n",
            "Val loss: 0.3515053987503052\n",
            "Training loss: 0.3537663320700328\n",
            "Val loss: 0.3514081537723541\n",
            "Training loss: 0.3489599625269572\n",
            "Val loss: 0.3513597249984741\n",
            "Training loss: 0.34778521458307904\n",
            "Val loss: 0.3513997793197632\n",
            "Training loss: 0.348715861638387\n",
            "Val loss: 0.35141029953956604\n",
            "Training loss: 0.34897064169247943\n",
            "Val loss: 0.35139596462249756\n",
            "Training loss: 0.34895939628283185\n",
            "Val loss: 0.35141271352767944\n",
            "Training loss: 0.3482822875181834\n",
            "Val loss: 0.35135576128959656\n",
            "Training loss: 0.3505573868751526\n",
            "Val loss: 0.3513060212135315\n",
            "Training loss: 0.34987826148668927\n",
            "Val loss: 0.35123011469841003\n",
            "Training loss: 0.3487042287985484\n",
            "Val loss: 0.3512331247329712\n",
            "Training loss: 0.34894341230392456\n",
            "Val loss: 0.35123008489608765\n",
            "Training loss: 0.34792081514994305\n",
            "Val loss: 0.3511732220649719\n",
            "Training loss: 0.35080382227897644\n",
            "Val loss: 0.351118803024292\n",
            "Training loss: 0.34750744700431824\n",
            "Val loss: 0.35111692547798157\n",
            "Training loss: 0.35108403364817303\n",
            "Val loss: 0.3511418104171753\n",
            "Training loss: 0.348953644434611\n",
            "Val loss: 0.351134717464447\n",
            "Training loss: 0.34916752576828003\n",
            "Val loss: 0.35110270977020264\n",
            "Training loss: 0.34936244289080304\n",
            "Val loss: 0.3510337471961975\n",
            "Training loss: 0.3499475320180257\n",
            "Val loss: 0.3509625494480133\n",
            "Training loss: 0.35104377071062726\n",
            "Val loss: 0.3509838879108429\n",
            "Training loss: 0.3485025266806285\n",
            "Val loss: 0.35097259283065796\n",
            "Training loss: 0.34885914127031964\n",
            "Val loss: 0.35091185569763184\n",
            "Training loss: 0.350119670232137\n",
            "Val loss: 0.3508450984954834\n",
            "Training loss: 0.348079115152359\n",
            "Val loss: 0.3508349061012268\n",
            "Training loss: 0.3473821481068929\n",
            "Val loss: 0.3508249819278717\n",
            "Training loss: 0.34823999802271527\n",
            "Val loss: 0.3508108854293823\n",
            "Training loss: 0.3472166657447815\n",
            "Val loss: 0.3507780432701111\n",
            "Training loss: 0.34838610887527466\n",
            "Val loss: 0.3507455885410309\n",
            "Training loss: 0.3478926122188568\n",
            "Val loss: 0.3507285714149475\n",
            "Training loss: 0.34915629029273987\n",
            "Val loss: 0.35073593258857727\n",
            "Training loss: 0.34792694449424744\n",
            "Val loss: 0.35072019696235657\n",
            "Training loss: 0.34827566146850586\n",
            "Val loss: 0.3506912291049957\n",
            "Training loss: 0.3475435674190521\n",
            "Val loss: 0.35069531202316284\n",
            "Training loss: 0.3495084544022878\n",
            "Val loss: 0.35064324736595154\n",
            "Training loss: 0.34889286756515503\n",
            "Val loss: 0.35065513849258423\n",
            "Training loss: 0.3495839635531108\n",
            "Val loss: 0.35064923763275146\n",
            "Training loss: 0.34854504466056824\n",
            "Val loss: 0.35062918066978455\n",
            "Training loss: 0.3479597369829814\n",
            "Val loss: 0.3505738079547882\n",
            "Training loss: 0.34932516018549603\n",
            "Val loss: 0.3505209684371948\n",
            "Training loss: 0.34790627161661786\n",
            "Val loss: 0.35047611594200134\n",
            "Training loss: 0.3480065067609151\n",
            "Val loss: 0.35044535994529724\n",
            "Training loss: 0.34854474663734436\n",
            "Val loss: 0.35042789578437805\n",
            "Training loss: 0.3476547598838806\n",
            "Val loss: 0.3504558503627777\n",
            "Training loss: 0.3484920660654704\n",
            "Val loss: 0.3504902124404907\n",
            "Training loss: 0.34975619117418927\n",
            "Val loss: 0.3504236936569214\n",
            "Training loss: 0.3479489286740621\n",
            "Val loss: 0.35040223598480225\n",
            "Training loss: 0.34977321823438007\n",
            "Val loss: 0.3503074049949646\n",
            "Training loss: 0.3489193419615428\n",
            "Val loss: 0.3502923250198364\n",
            "Training loss: 0.3478896717230479\n",
            "Val loss: 0.35027995705604553\n",
            "Training loss: 0.3494398792584737\n",
            "Val loss: 0.3502504825592041\n",
            "Training loss: 0.34799516201019287\n",
            "Val loss: 0.35025402903556824\n",
            "Training loss: 0.3475114305814107\n",
            "Val loss: 0.35028454661369324\n",
            "Training loss: 0.348808874686559\n",
            "Val loss: 0.3502451479434967\n",
            "Training loss: 0.3487677176793416\n",
            "Val loss: 0.35026198625564575\n",
            "Training loss: 0.3488004108270009\n",
            "Val loss: 0.35023292899131775\n",
            "Training loss: 0.3482339382171631\n",
            "Val loss: 0.3502253592014313\n",
            "Training loss: 0.34689682722091675\n",
            "Val loss: 0.3502459228038788\n",
            "Training loss: 0.34841341773668927\n",
            "Val loss: 0.35029393434524536\n",
            "Training loss: 0.35028841098149616\n",
            "Val loss: 0.3502737879753113\n",
            "Training loss: 0.3491210639476776\n",
            "Val loss: 0.3502740263938904\n",
            "Training loss: 0.3497101167837779\n",
            "Val loss: 0.35022613406181335\n",
            "Training loss: 0.34626972675323486\n",
            "Val loss: 0.3502340614795685\n",
            "Training loss: 0.3470837374528249\n",
            "Val loss: 0.35022208094596863\n",
            "Training loss: 0.347060630718867\n",
            "Val loss: 0.35023024678230286\n",
            "Training loss: 0.34894360105196637\n",
            "Val loss: 0.35025379061698914\n",
            "Training loss: 0.3476381003856659\n",
            "Val loss: 0.35023775696754456\n",
            "Training loss: 0.34731359283129376\n",
            "Val loss: 0.35023799538612366\n",
            "Training loss: 0.3491834799448649\n",
            "Val loss: 0.3502495586872101\n",
            "Training loss: 0.349042405684789\n",
            "Val loss: 0.35020995140075684\n",
            "Training loss: 0.3523631989955902\n",
            "Val loss: 0.3501719534397125\n",
            "Training loss: 0.3473559518655141\n",
            "Val loss: 0.35021474957466125\n",
            "Training loss: 0.3481229444344838\n",
            "Val loss: 0.35026785731315613\n",
            "Training loss: 0.3473115563392639\n",
            "Val loss: 0.3502606749534607\n",
            "Training loss: 0.34892956415812176\n",
            "Val loss: 0.35021713376045227\n",
            "Training loss: 0.35046544671058655\n",
            "Val loss: 0.3501792252063751\n",
            "Training loss: 0.34782053033510846\n",
            "Val loss: 0.350130170583725\n",
            "Training loss: 0.34836531678835553\n",
            "Val loss: 0.35011130571365356\n",
            "Training loss: 0.34933151801427204\n",
            "Val loss: 0.35009288787841797\n",
            "Training loss: 0.3472171127796173\n",
            "Val loss: 0.35011976957321167\n",
            "Training loss: 0.34908226132392883\n",
            "Val loss: 0.35009321570396423\n",
            "Training loss: 0.34876641631126404\n",
            "Val loss: 0.350067675113678\n",
            "Training loss: 0.34836069742838544\n",
            "Val loss: 0.3500801622867584\n",
            "Training loss: 0.34870365262031555\n",
            "Val loss: 0.3500950336456299\n",
            "Training loss: 0.347410927216212\n",
            "Val loss: 0.3500598967075348\n",
            "Training loss: 0.3483516772588094\n",
            "Val loss: 0.35002854466438293\n",
            "Training loss: 0.3487635354200999\n",
            "Val loss: 0.35003143548965454\n",
            "Training loss: 0.34769882758458454\n",
            "Val loss: 0.3500072956085205\n",
            "Training loss: 0.3478635946909587\n",
            "Val loss: 0.35001257061958313\n",
            "Training loss: 0.3458908597628276\n",
            "Val loss: 0.35005608201026917\n",
            "Training loss: 0.34835859139760333\n",
            "Val loss: 0.3500624895095825\n",
            "Training loss: 0.34858788053194684\n",
            "Val loss: 0.35004305839538574\n",
            "Training loss: 0.3481071988741557\n",
            "Val loss: 0.3500267267227173\n",
            "Training loss: 0.34780625502268475\n",
            "Val loss: 0.3500085473060608\n",
            "Training loss: 0.34999438126881915\n",
            "Val loss: 0.3500320017337799\n",
            "Training loss: 0.34664634863535565\n",
            "Val loss: 0.3500320613384247\n",
            "Training loss: 0.34745388229688007\n",
            "Val loss: 0.3500346541404724\n",
            "Training loss: 0.34659181038538617\n",
            "Val loss: 0.3500223755836487\n",
            "Training loss: 0.3472093939781189\n",
            "Val loss: 0.3500356078147888\n",
            "Training loss: 0.3477492332458496\n",
            "Val loss: 0.35002103447914124\n",
            "Training loss: 0.34880682826042175\n",
            "Val loss: 0.350014328956604\n",
            "Training loss: 0.3470764458179474\n",
            "Val loss: 0.3499653935432434\n",
            "Training loss: 0.3466828068097432\n",
            "Val loss: 0.3499000370502472\n",
            "Training loss: 0.3476664523283641\n",
            "Val loss: 0.3498689830303192\n",
            "Training loss: 0.34723902742067975\n",
            "Val loss: 0.34980663657188416\n",
            "Training loss: 0.3486061692237854\n",
            "Val loss: 0.34978270530700684\n",
            "Training loss: 0.3461437225341797\n",
            "Val loss: 0.3497674763202667\n",
            "Training loss: 0.34842684864997864\n",
            "Val loss: 0.3496979773044586\n",
            "Training loss: 0.34737632671991986\n",
            "Val loss: 0.34962618350982666\n",
            "Training loss: 0.34760292371114093\n",
            "Val loss: 0.349612295627594\n",
            "Training loss: 0.3486498196919759\n",
            "Val loss: 0.34963876008987427\n",
            "Training loss: 0.34829941391944885\n",
            "Val loss: 0.349658727645874\n",
            "Training loss: 0.3473983605702718\n",
            "Val loss: 0.3496725857257843\n",
            "Training loss: 0.34799524148305255\n",
            "Val loss: 0.34969469904899597\n",
            "Training loss: 0.34709225098292035\n",
            "Val loss: 0.34971728920936584\n",
            "Training loss: 0.34666311740875244\n",
            "Val loss: 0.3496534824371338\n",
            "Training loss: 0.3463960289955139\n",
            "Val loss: 0.349625825881958\n",
            "Training loss: 0.3482280671596527\n",
            "Val loss: 0.34962940216064453\n",
            "Training loss: 0.34846476713816327\n",
            "Val loss: 0.34961190819740295\n",
            "Training loss: 0.3479912579059601\n",
            "Val loss: 0.34960463643074036\n",
            "Training loss: 0.34746500849723816\n",
            "Val loss: 0.34959718585014343\n",
            "Training loss: 0.34879671533902484\n",
            "Val loss: 0.3496076464653015\n",
            "Training loss: 0.34710020820299786\n",
            "Val loss: 0.34964749217033386\n",
            "Training loss: 0.34747472405433655\n",
            "Val loss: 0.34964776039123535\n",
            "Training loss: 0.3468215962251027\n",
            "Val loss: 0.3496354818344116\n",
            "Training loss: 0.3463825086752574\n",
            "Val loss: 0.349636971950531\n",
            "Training loss: 0.34734411040941876\n",
            "Val loss: 0.3496559262275696\n",
            "Training loss: 0.347366730372111\n",
            "Val loss: 0.34970587491989136\n",
            "Training loss: 0.3489687939484914\n",
            "Val loss: 0.3497059643268585\n",
            "Training loss: 0.3490646878878276\n",
            "Val loss: 0.3496764004230499\n",
            "Training loss: 0.3472764988740285\n",
            "Val loss: 0.3497604429721832\n",
            "Training loss: 0.34630141655604046\n",
            "Val loss: 0.34981513023376465\n",
            "Training loss: 0.34895289937655133\n",
            "Val loss: 0.3498266935348511\n",
            "Training loss: 0.3470750351746877\n",
            "Val loss: 0.3498463034629822\n",
            "Training loss: 0.3461204965909322\n",
            "Val loss: 0.3498859405517578\n",
            "Training loss: 0.3462987144788106\n",
            "Val loss: 0.3499297797679901\n",
            "Training loss: 0.34642799695332843\n",
            "Val loss: 0.3499564826488495\n",
            "Training loss: 0.34872103730837506\n",
            "Val loss: 0.34991079568862915\n",
            "Training loss: 0.3462151487668355\n",
            "Val loss: 0.3499075770378113\n",
            "Training loss: 0.34715932607650757\n",
            "Val loss: 0.34991493821144104\n",
            "Training loss: 0.34762584169705707\n",
            "Val loss: 0.34991738200187683\n",
            "Training loss: 0.3478711048762004\n",
            "Val loss: 0.3499242067337036\n",
            "Training loss: 0.34613679846127826\n",
            "Val loss: 0.3498794138431549\n",
            "Training loss: 0.3477189739545186\n",
            "Val loss: 0.3498784899711609\n",
            "Training loss: 0.3450966378053029\n",
            "Val loss: 0.3498733341693878\n",
            "Training loss: 0.3465810716152191\n",
            "Val loss: 0.34986385703086853\n",
            "Training loss: 0.3459985653559367\n",
            "Val loss: 0.3498348295688629\n",
            "Training loss: 0.3475637932618459\n",
            "Val loss: 0.3497574031352997\n",
            "Training loss: 0.3465462227662404\n",
            "Val loss: 0.3496488928794861\n",
            "Training loss: 0.34649590651194256\n",
            "Val loss: 0.34955069422721863\n",
            "Training loss: 0.3489132324854533\n",
            "Val loss: 0.34948548674583435\n",
            "Training loss: 0.347057302792867\n",
            "Val loss: 0.34947821497917175\n",
            "Training loss: 0.34697797894477844\n",
            "Val loss: 0.3494457006454468\n",
            "Training loss: 0.34741219878196716\n",
            "Val loss: 0.34938859939575195\n",
            "Training loss: 0.34519335627555847\n",
            "Val loss: 0.34934478998184204\n",
            "Training loss: 0.34810274839401245\n",
            "Val loss: 0.3493231236934662\n",
            "Training loss: 0.34692898392677307\n",
            "Val loss: 0.3493102490901947\n",
            "Training loss: 0.34788785378138226\n",
            "Val loss: 0.3493337035179138\n",
            "Training loss: 0.34701716899871826\n",
            "Val loss: 0.3492715358734131\n",
            "Training loss: 0.3471956253051758\n",
            "Val loss: 0.3492485582828522\n",
            "Training loss: 0.34774375955263775\n",
            "Val loss: 0.3491416871547699\n",
            "Training loss: 0.34560005863507587\n",
            "Val loss: 0.34909358620643616\n",
            "Training loss: 0.34641459584236145\n",
            "Val loss: 0.34908580780029297\n",
            "Training loss: 0.34553028146425885\n",
            "Val loss: 0.34908029437065125\n",
            "Training loss: 0.34764451781908673\n",
            "Val loss: 0.34900274872779846\n",
            "Training loss: 0.34578821063041687\n",
            "Val loss: 0.34900611639022827\n",
            "Training loss: 0.34420283635457355\n",
            "Val loss: 0.34900936484336853\n",
            "Training loss: 0.34553276499112445\n",
            "Val loss: 0.34905579686164856\n",
            "Training loss: 0.34626614054044086\n",
            "Val loss: 0.3490244150161743\n",
            "Training loss: 0.34643713633219403\n",
            "Val loss: 0.3489673435688019\n",
            "Training loss: 0.34635071953137714\n",
            "Val loss: 0.34893563389778137\n",
            "Training loss: 0.34684428572654724\n",
            "Val loss: 0.34893056750297546\n",
            "Training loss: 0.346447358528773\n",
            "Val loss: 0.34897521138191223\n",
            "Training loss: 0.3460862437884013\n",
            "Val loss: 0.34899264574050903\n",
            "Training loss: 0.34679590662320453\n",
            "Val loss: 0.34897664189338684\n",
            "Training loss: 0.3459692696730296\n",
            "Val loss: 0.3489421308040619\n",
            "Training loss: 0.34483957290649414\n",
            "Val loss: 0.34899142384529114\n",
            "Training loss: 0.34702082475026447\n",
            "Val loss: 0.34900206327438354\n",
            "Training loss: 0.34627853830655414\n",
            "Val loss: 0.3489950895309448\n",
            "Training loss: 0.34518201152483624\n",
            "Val loss: 0.34899312257766724\n",
            "Training loss: 0.34976835052172345\n",
            "Val loss: 0.3489753305912018\n",
            "Training loss: 0.34766289591789246\n",
            "Val loss: 0.3489486873149872\n",
            "Training loss: 0.3459682762622833\n",
            "Val loss: 0.3489712178707123\n",
            "Training loss: 0.347891906897227\n",
            "Val loss: 0.3489902913570404\n",
            "Training loss: 0.345619797706604\n",
            "Val loss: 0.3489891290664673\n",
            "Training loss: 0.34534228841463727\n",
            "Val loss: 0.3490124046802521\n",
            "Training loss: 0.3456425964832306\n",
            "Val loss: 0.348998486995697\n",
            "Training loss: 0.3455057938893636\n",
            "Val loss: 0.34897008538246155\n",
            "Training loss: 0.34719499945640564\n",
            "Val loss: 0.348964661359787\n",
            "Training loss: 0.34571532408396405\n",
            "Val loss: 0.34893301129341125\n",
            "Training loss: 0.34927671154340106\n",
            "Val loss: 0.34888583421707153\n",
            "Training loss: 0.348453422387441\n",
            "Val loss: 0.34890177845954895\n",
            "Training loss: 0.3454168140888214\n",
            "Val loss: 0.34889811277389526\n",
            "Training loss: 0.34726553161938983\n",
            "Val loss: 0.3488648235797882\n",
            "Training loss: 0.34643398722012836\n",
            "Val loss: 0.34886378049850464\n",
            "Training loss: 0.3501213788986206\n",
            "Val loss: 0.34880515933036804\n",
            "Training loss: 0.3463156720002492\n",
            "Val loss: 0.34875214099884033\n",
            "Training loss: 0.34653183817863464\n",
            "Val loss: 0.34872815012931824\n",
            "Training loss: 0.34525907039642334\n",
            "Val loss: 0.3486711382865906\n",
            "Training loss: 0.3455243706703186\n",
            "Val loss: 0.3486695885658264\n",
            "Training loss: 0.3442467351754506\n",
            "Val loss: 0.3486754596233368\n",
            "Training loss: 0.3470483422279358\n",
            "Val loss: 0.3486866056919098\n",
            "Training loss: 0.344548761844635\n",
            "Val loss: 0.3486444354057312\n",
            "Training loss: 0.3492277264595032\n",
            "Val loss: 0.3486327528953552\n",
            "Training loss: 0.3459156354268392\n",
            "Val loss: 0.34866154193878174\n",
            "Training loss: 0.34499143560727435\n",
            "Val loss: 0.34875816106796265\n",
            "Training loss: 0.34559278686841327\n",
            "Val loss: 0.3487863838672638\n",
            "Training loss: 0.3460034529368083\n",
            "Val loss: 0.348732054233551\n",
            "Training loss: 0.34679044286410016\n",
            "Val loss: 0.34870681166648865\n",
            "Training loss: 0.34696294864018756\n",
            "Val loss: 0.3487177789211273\n",
            "Training loss: 0.3448728621006012\n",
            "Val loss: 0.34876134991645813\n",
            "Training loss: 0.34820979833602905\n",
            "Val loss: 0.34881338477134705\n",
            "Training loss: 0.3451311985651652\n",
            "Val loss: 0.34877777099609375\n",
            "Training loss: 0.346729576587677\n",
            "Val loss: 0.3487345576286316\n",
            "Training loss: 0.3465466996033986\n",
            "Val loss: 0.34866148233413696\n",
            "Training loss: 0.3440467119216919\n",
            "Val loss: 0.34858670830726624\n",
            "Training loss: 0.34388697147369385\n",
            "Val loss: 0.3484288454055786\n",
            "Training loss: 0.3442410926024119\n",
            "Val loss: 0.3482864499092102\n",
            "Training loss: 0.3448831041653951\n",
            "Val loss: 0.3481658399105072\n",
            "Training loss: 0.3448195457458496\n",
            "Val loss: 0.3480328917503357\n",
            "Training loss: 0.34493744373321533\n",
            "Val loss: 0.347881019115448\n",
            "Training loss: 0.34550819794336957\n",
            "Val loss: 0.3477810323238373\n",
            "Training loss: 0.34461702903111774\n",
            "Val loss: 0.3476761281490326\n",
            "Training loss: 0.3471545875072479\n",
            "Val loss: 0.3475487530231476\n",
            "Training loss: 0.3468381464481354\n",
            "Val loss: 0.34740057587623596\n",
            "Training loss: 0.34439872701962787\n",
            "Val loss: 0.34725141525268555\n",
            "Training loss: 0.3435165286064148\n",
            "Val loss: 0.34716278314590454\n",
            "Training loss: 0.34252627690633136\n",
            "Val loss: 0.34703829884529114\n",
            "Training loss: 0.3454830050468445\n",
            "Val loss: 0.346971720457077\n",
            "Training loss: 0.3456997474034627\n",
            "Val loss: 0.34694400429725647\n",
            "Training loss: 0.34386488795280457\n",
            "Val loss: 0.34691905975341797\n",
            "Training loss: 0.3451632459958394\n",
            "Val loss: 0.3468758463859558\n",
            "Training loss: 0.3450617790222168\n",
            "Val loss: 0.34684622287750244\n",
            "Training loss: 0.34306779503822327\n",
            "Val loss: 0.34679657220840454\n",
            "Training loss: 0.34407968322436017\n",
            "Val loss: 0.3467210829257965\n",
            "Training loss: 0.3431009848912557\n",
            "Val loss: 0.34667596220970154\n",
            "Training loss: 0.3447408278783162\n",
            "Val loss: 0.3466487228870392\n",
            "Training loss: 0.3453770677248637\n",
            "Val loss: 0.3466114401817322\n",
            "Training loss: 0.3448159595330556\n",
            "Val loss: 0.3465636968612671\n",
            "Training loss: 0.3425299922625224\n",
            "Val loss: 0.3465758264064789\n",
            "Training loss: 0.34518783291180927\n",
            "Val loss: 0.34651824831962585\n",
            "Training loss: 0.3434058626492818\n",
            "Val loss: 0.34652042388916016\n",
            "Training loss: 0.3435891469319661\n",
            "Val loss: 0.3464249074459076\n",
            "Training loss: 0.34408626953760785\n",
            "Val loss: 0.3464280962944031\n",
            "Training loss: 0.3433711032072703\n",
            "Val loss: 0.3464466631412506\n",
            "Training loss: 0.3460862338542938\n",
            "Val loss: 0.34642356634140015\n",
            "Training loss: 0.34368929266929626\n",
            "Val loss: 0.3464275300502777\n",
            "Training loss: 0.34439818064371747\n",
            "Val loss: 0.34641650319099426\n",
            "Training loss: 0.34408289194107056\n",
            "Val loss: 0.34639784693717957\n",
            "Training loss: 0.34383230408032733\n",
            "Val loss: 0.3463692367076874\n",
            "Training loss: 0.34218095739682514\n",
            "Val loss: 0.3463260233402252\n",
            "Training loss: 0.34306373198827106\n",
            "Val loss: 0.34625646471977234\n",
            "Training loss: 0.3429611027240753\n",
            "Val loss: 0.3461301922798157\n",
            "Training loss: 0.343716303507487\n",
            "Val loss: 0.3461146354675293\n",
            "Training loss: 0.3450799286365509\n",
            "Val loss: 0.34612616896629333\n",
            "Training loss: 0.34438396493593854\n",
            "Val loss: 0.34615394473075867\n",
            "Training loss: 0.3442581097284953\n",
            "Val loss: 0.3461446166038513\n",
            "Training loss: 0.34332120418548584\n",
            "Val loss: 0.34612780809402466\n",
            "Training loss: 0.34336339433987934\n",
            "Val loss: 0.3461153507232666\n",
            "Training loss: 0.34805657466252643\n",
            "Val loss: 0.3460918962955475\n",
            "Training loss: 0.3428770899772644\n",
            "Val loss: 0.3460853397846222\n",
            "Training loss: 0.3455033202966054\n",
            "Val loss: 0.3460864722728729\n",
            "Training loss: 0.34499456485112506\n",
            "Val loss: 0.3460223376750946\n",
            "Training loss: 0.343933363755544\n",
            "Val loss: 0.3460544943809509\n",
            "Training loss: 0.34257997075716656\n",
            "Val loss: 0.3460688591003418\n",
            "Training loss: 0.34373236695925397\n",
            "Val loss: 0.3460909128189087\n",
            "Training loss: 0.34431275725364685\n",
            "Val loss: 0.34611380100250244\n",
            "Training loss: 0.34379076957702637\n",
            "Val loss: 0.34611576795578003\n",
            "Training loss: 0.343421349922816\n",
            "Val loss: 0.34606409072875977\n",
            "Training loss: 0.3429018755753835\n",
            "Val loss: 0.34605082869529724\n",
            "Training loss: 0.34419363737106323\n",
            "Val loss: 0.34604188799858093\n",
            "Training loss: 0.3451744516690572\n",
            "Val loss: 0.345957487821579\n",
            "Training loss: 0.3450275957584381\n",
            "Val loss: 0.3458879590034485\n",
            "Training loss: 0.34367730220158893\n",
            "Val loss: 0.34588995575904846\n",
            "Training loss: 0.34498755137125653\n",
            "Val loss: 0.3459089696407318\n",
            "Training loss: 0.3473951319853465\n",
            "Val loss: 0.34597325325012207\n",
            "Training loss: 0.3433310091495514\n",
            "Val loss: 0.345949649810791\n",
            "Training loss: 0.3446706334749858\n",
            "Val loss: 0.3459210991859436\n",
            "Training loss: 0.3426337242126465\n",
            "Val loss: 0.3458680212497711\n",
            "Training loss: 0.34251124660174054\n",
            "Val loss: 0.3458567261695862\n",
            "Training loss: 0.3433297077814738\n",
            "Val loss: 0.3458532989025116\n",
            "Training loss: 0.3446608781814575\n",
            "Val loss: 0.34585073590278625\n",
            "Training loss: 0.34279416998227435\n",
            "Val loss: 0.3458521068096161\n",
            "Training loss: 0.3458935519059499\n",
            "Val loss: 0.34585991501808167\n",
            "Training loss: 0.34338803092638653\n",
            "Val loss: 0.34582754969596863\n",
            "Training loss: 0.3451999028523763\n",
            "Val loss: 0.3457329273223877\n",
            "Training loss: 0.3464500804742177\n",
            "Val loss: 0.34567326307296753\n",
            "Training loss: 0.34395914276440936\n",
            "Val loss: 0.34567201137542725\n",
            "Training loss: 0.34373152256011963\n",
            "Val loss: 0.3456849753856659\n",
            "Training loss: 0.34232985973358154\n",
            "Val loss: 0.34561076760292053\n",
            "Training loss: 0.34380627671877545\n",
            "Val loss: 0.34559065103530884\n",
            "Training loss: 0.3440541923046112\n",
            "Val loss: 0.3455478250980377\n",
            "Training loss: 0.3408450384934743\n",
            "Val loss: 0.3455391228199005\n",
            "Training loss: 0.3429554800192515\n",
            "Val loss: 0.3455156683921814\n",
            "Training loss: 0.34481630722681683\n",
            "Val loss: 0.3455241620540619\n",
            "Training loss: 0.3465651373068492\n",
            "Val loss: 0.3455495238304138\n",
            "Training loss: 0.3420485158761342\n",
            "Val loss: 0.34562361240386963\n",
            "Training loss: 0.3449381987253825\n",
            "Val loss: 0.3456844985485077\n",
            "Training loss: 0.34373929103215534\n",
            "Val loss: 0.34574732184410095\n",
            "Training loss: 0.343331257502238\n",
            "Val loss: 0.3457271158695221\n",
            "Training loss: 0.34391364455223083\n",
            "Val loss: 0.3457373380661011\n",
            "Training loss: 0.34425673882166546\n",
            "Val loss: 0.34581851959228516\n",
            "Training loss: 0.3445987304051717\n",
            "Val loss: 0.34576570987701416\n",
            "Training loss: 0.3432747920354207\n",
            "Val loss: 0.34572622179985046\n",
            "Training loss: 0.34329522649447125\n",
            "Val loss: 0.34576037526130676\n",
            "Training loss: 0.3413219650586446\n",
            "Val loss: 0.3457384705543518\n",
            "Training loss: 0.3438657025496165\n",
            "Val loss: 0.34570232033729553\n",
            "Training loss: 0.3454563319683075\n",
            "Val loss: 0.3456156253814697\n",
            "Training loss: 0.3439298967520396\n",
            "Val loss: 0.34556177258491516\n",
            "Training loss: 0.3437192936738332\n",
            "Val loss: 0.34548071026802063\n",
            "Training loss: 0.3420899510383606\n",
            "Val loss: 0.34544065594673157\n",
            "Training loss: 0.34298325578371686\n",
            "Val loss: 0.34540295600891113\n",
            "Training loss: 0.3409385879834493\n",
            "Val loss: 0.3453856110572815\n",
            "Training loss: 0.3430046538511912\n",
            "Val loss: 0.3453555703163147\n",
            "Training loss: 0.34315598011016846\n",
            "Val loss: 0.34537869691848755\n",
            "Training loss: 0.34461963176727295\n",
            "Val loss: 0.3453327715396881\n",
            "Training loss: 0.3437359730402629\n",
            "Val loss: 0.34529823064804077\n",
            "Training loss: 0.342020720243454\n",
            "Val loss: 0.34526526927948\n",
            "Training loss: 0.34414894382158917\n",
            "Val loss: 0.3452337980270386\n",
            "Training loss: 0.34214894970258075\n",
            "Val loss: 0.34527167677879333\n",
            "Training loss: 0.3453373312950134\n",
            "Val loss: 0.34528008103370667\n",
            "Training loss: 0.34193511803944904\n",
            "Val loss: 0.3452965319156647\n",
            "Training loss: 0.3441160519917806\n",
            "Val loss: 0.34529533982276917\n",
            "Training loss: 0.34272714455922443\n",
            "Val loss: 0.34533971548080444\n",
            "Training loss: 0.3412557045618693\n",
            "Val loss: 0.34535959362983704\n",
            "Training loss: 0.3423033853371938\n",
            "Val loss: 0.34533217549324036\n",
            "Training loss: 0.3401467502117157\n",
            "Val loss: 0.3453863561153412\n",
            "Training loss: 0.34442267815272015\n",
            "Val loss: 0.34546467661857605\n",
            "Training loss: 0.34262646238009137\n",
            "Val loss: 0.3454500734806061\n",
            "Training loss: 0.34198542435963947\n",
            "Val loss: 0.3454650938510895\n",
            "Training loss: 0.3420346677303314\n",
            "Val loss: 0.3454746901988983\n",
            "Training loss: 0.3425419827302297\n",
            "Val loss: 0.3454999327659607\n",
            "Training loss: 0.34370837608973187\n",
            "Val loss: 0.3454943299293518\n",
            "Training loss: 0.3442399303118388\n",
            "Val loss: 0.3455379605293274\n",
            "Training loss: 0.3441429336865743\n",
            "Val loss: 0.34546801447868347\n",
            "Training loss: 0.3419313033421834\n",
            "Val loss: 0.34543734788894653\n",
            "Training loss: 0.34072760740915936\n",
            "Val loss: 0.34546202421188354\n",
            "Training loss: 0.34541626771291095\n",
            "Val loss: 0.3454296290874481\n",
            "Training loss: 0.3442189594109853\n",
            "Val loss: 0.3453696370124817\n",
            "Training loss: 0.3428524931271871\n",
            "Val loss: 0.34529024362564087\n",
            "Training loss: 0.3422488172849019\n",
            "Val loss: 0.3452078104019165\n",
            "Training loss: 0.34410027662913006\n",
            "Val loss: 0.3452013432979584\n",
            "Training loss: 0.343387911717097\n",
            "Val loss: 0.34520816802978516\n",
            "Training loss: 0.3413458665211995\n",
            "Val loss: 0.3452891707420349\n",
            "Training loss: 0.3438360293706258\n",
            "Val loss: 0.34528475999832153\n",
            "Training loss: 0.34420735637346905\n",
            "Val loss: 0.3452111482620239\n",
            "Training loss: 0.34125014146169025\n",
            "Val loss: 0.34525179862976074\n",
            "Training loss: 0.3416554828484853\n",
            "Val loss: 0.3452494144439697\n",
            "Training loss: 0.34199848771095276\n",
            "Val loss: 0.345196008682251\n",
            "Training loss: 0.34169427553812665\n",
            "Val loss: 0.3451307415962219\n",
            "Training loss: 0.3442953824996948\n",
            "Val loss: 0.345109760761261\n",
            "Training loss: 0.3422139684359233\n",
            "Val loss: 0.3451675474643707\n",
            "Training loss: 0.3441519836584727\n",
            "Val loss: 0.3452201187610626\n",
            "Training loss: 0.34147534767786664\n",
            "Val loss: 0.3452582061290741\n",
            "Training loss: 0.3418144981066386\n",
            "Val loss: 0.34526312351226807\n",
            "Training loss: 0.3427092134952545\n",
            "Val loss: 0.34528422355651855\n",
            "Training loss: 0.34324588378270465\n",
            "Val loss: 0.3453129827976227\n",
            "Training loss: 0.3412040074666341\n",
            "Val loss: 0.34531038999557495\n",
            "Training loss: 0.34246759613355\n",
            "Val loss: 0.3453468382358551\n",
            "Training loss: 0.34259920318921405\n",
            "Val loss: 0.3453444242477417\n",
            "Training loss: 0.3416887621084849\n",
            "Val loss: 0.34536272287368774\n",
            "Training loss: 0.34550636013348895\n",
            "Val loss: 0.34535282850265503\n",
            "Training loss: 0.3425684968630473\n",
            "Val loss: 0.3453477621078491\n",
            "Training loss: 0.3432795802752177\n",
            "Val loss: 0.34527310729026794\n",
            "Training loss: 0.34346378842989606\n",
            "Val loss: 0.3451686203479767\n",
            "Training loss: 0.34338393807411194\n",
            "Val loss: 0.34506189823150635\n",
            "Training loss: 0.3421595096588135\n",
            "Val loss: 0.34504377841949463\n",
            "Training loss: 0.34037327766418457\n",
            "Val loss: 0.3449963331222534\n",
            "Training loss: 0.3427614172299703\n",
            "Val loss: 0.3450077772140503\n",
            "Training loss: 0.3440668781598409\n",
            "Val loss: 0.345065176486969\n",
            "Training loss: 0.3393083413441976\n",
            "Val loss: 0.345061719417572\n",
            "Training loss: 0.3427742024262746\n",
            "Val loss: 0.3450763523578644\n",
            "Training loss: 0.34357431530952454\n",
            "Val loss: 0.34514787793159485\n",
            "Training loss: 0.34274179736773175\n",
            "Val loss: 0.3450973629951477\n",
            "Training loss: 0.3420106569925944\n",
            "Val loss: 0.3450170159339905\n",
            "Training loss: 0.34103378653526306\n",
            "Val loss: 0.34493619203567505\n",
            "Training loss: 0.3413892388343811\n",
            "Val loss: 0.34479784965515137\n",
            "Training loss: 0.33952364325523376\n",
            "Val loss: 0.3447091281414032\n",
            "Training loss: 0.34080466628074646\n",
            "Val loss: 0.34461045265197754\n",
            "Training loss: 0.34241631627082825\n",
            "Val loss: 0.34460869431495667\n",
            "Training loss: 0.3417791426181793\n",
            "Val loss: 0.34467780590057373\n",
            "Training loss: 0.34107006589571637\n",
            "Val loss: 0.3447730839252472\n",
            "Training loss: 0.3422137002150218\n",
            "Val loss: 0.34487423300743103\n",
            "Training loss: 0.3417319754759471\n",
            "Val loss: 0.3448812663555145\n",
            "Training loss: 0.34285130103429157\n",
            "Val loss: 0.34492382407188416\n",
            "Training loss: 0.3422221839427948\n",
            "Val loss: 0.34497198462486267\n",
            "Training loss: 0.3423905273278554\n",
            "Val loss: 0.34500837326049805\n",
            "Training loss: 0.3418491284052531\n",
            "Val loss: 0.3450622856616974\n",
            "Training loss: 0.3416869342327118\n",
            "Val loss: 0.3450786769390106\n",
            "Training loss: 0.3410059412320455\n",
            "Val loss: 0.34504371881484985\n",
            "Training loss: 0.3412230312824249\n",
            "Val loss: 0.34500762820243835\n",
            "Training loss: 0.3423691689968109\n",
            "Val loss: 0.34507349133491516\n",
            "Training loss: 0.34363680084546405\n",
            "Val loss: 0.3451215624809265\n",
            "Training loss: 0.3417003055413564\n",
            "Val loss: 0.3450940251350403\n",
            "Training loss: 0.3425550063451131\n",
            "Val loss: 0.3450980484485626\n",
            "Training loss: 0.3464272618293762\n",
            "Val loss: 0.34502068161964417\n",
            "Training loss: 0.34504802028338116\n",
            "Val loss: 0.34500518441200256\n",
            "Training loss: 0.3430657585461934\n",
            "Val loss: 0.3450417220592499\n",
            "Training loss: 0.3429100811481476\n",
            "Val loss: 0.3450842797756195\n",
            "Training loss: 0.3411751389503479\n",
            "Val loss: 0.3450571298599243\n",
            "Training loss: 0.3427733878294627\n",
            "Val loss: 0.3450915515422821\n",
            "Training loss: 0.34201040863990784\n",
            "Val loss: 0.3450620770454407\n",
            "Training loss: 0.3417866826057434\n",
            "Val loss: 0.34497901797294617\n",
            "Training loss: 0.3414930800596873\n",
            "Val loss: 0.34495803713798523\n",
            "Training loss: 0.3439615269502004\n",
            "Val loss: 0.34489938616752625\n",
            "Training loss: 0.3414777417977651\n",
            "Val loss: 0.3448478877544403\n",
            "Training loss: 0.34193889300028485\n",
            "Val loss: 0.3448891341686249\n",
            "Training loss: 0.33982304732004803\n",
            "Val loss: 0.3449581563472748\n",
            "Training loss: 0.34212832649548847\n",
            "Val loss: 0.3449651896953583\n",
            "Training loss: 0.34178970257441205\n",
            "Val loss: 0.34492042660713196\n",
            "Training loss: 0.3420400818188985\n",
            "Val loss: 0.3449191749095917\n",
            "Training loss: 0.3439652721087138\n",
            "Val loss: 0.34492892026901245\n",
            "Training loss: 0.34092382589975995\n",
            "Val loss: 0.344924658536911\n",
            "Training loss: 0.3431877593199412\n",
            "Val loss: 0.34494152665138245\n",
            "Training loss: 0.34155945976575214\n",
            "Val loss: 0.34488531947135925\n",
            "Training loss: 0.33952059348424274\n",
            "Val loss: 0.3448050022125244\n",
            "Training loss: 0.3402389883995056\n",
            "Val loss: 0.344790518283844\n",
            "Training loss: 0.3426041007041931\n",
            "Val loss: 0.34482985734939575\n",
            "Training loss: 0.3413309057553609\n",
            "Val loss: 0.34484970569610596\n",
            "Training loss: 0.3423854609330495\n",
            "Val loss: 0.3448585569858551\n",
            "Training loss: 0.34439168373743695\n",
            "Val loss: 0.3447856605052948\n",
            "Training loss: 0.3410796622435252\n",
            "Val loss: 0.34476789832115173\n",
            "Training loss: 0.34069517254829407\n",
            "Val loss: 0.3447786569595337\n",
            "Training loss: 0.34252314766248065\n",
            "Val loss: 0.3448442220687866\n",
            "Training loss: 0.3410467207431793\n",
            "Val loss: 0.34486353397369385\n",
            "Training loss: 0.34264928102493286\n",
            "Val loss: 0.3447602093219757\n",
            "Training loss: 0.34304770827293396\n",
            "Val loss: 0.3446948826313019\n",
            "Training loss: 0.3404129445552826\n",
            "Val loss: 0.3446960747241974\n",
            "Training loss: 0.34397796789805096\n",
            "Val loss: 0.3446837067604065\n",
            "Training loss: 0.3390147884686788\n",
            "Val loss: 0.3446483314037323\n",
            "Training loss: 0.34234316150347394\n",
            "Val loss: 0.3446830213069916\n",
            "Training loss: 0.3405372202396393\n",
            "Val loss: 0.3446245789527893\n",
            "Training loss: 0.3424318830172221\n",
            "Val loss: 0.3446029722690582\n",
            "Training loss: 0.34127705295880634\n",
            "Val loss: 0.34463590383529663\n",
            "Training loss: 0.3429255584875743\n",
            "Val loss: 0.34464505314826965\n",
            "Training loss: 0.3423558672269185\n",
            "Val loss: 0.34466561675071716\n",
            "Training loss: 0.3425176938374837\n",
            "Val loss: 0.3447076976299286\n",
            "Training loss: 0.3433787226676941\n",
            "Val loss: 0.3447154462337494\n",
            "Training loss: 0.34137322505315143\n",
            "Val loss: 0.3447156548500061\n",
            "Training loss: 0.3391546110312144\n",
            "Val loss: 0.3447563648223877\n",
            "Training loss: 0.3472804029782613\n",
            "Val loss: 0.34473592042922974\n",
            "Training loss: 0.3400014837582906\n",
            "Val loss: 0.34472060203552246\n",
            "Training loss: 0.34089810649553937\n",
            "Val loss: 0.34476438164711\n",
            "Training loss: 0.34234808882077533\n",
            "Val loss: 0.3447030782699585\n",
            "Training loss: 0.3427031834920247\n",
            "Val loss: 0.344708651304245\n",
            "Training loss: 0.34099331498146057\n",
            "Val loss: 0.3447389304637909\n",
            "Training loss: 0.3408167064189911\n",
            "Val loss: 0.3446806073188782\n",
            "Training loss: 0.3451382319132487\n",
            "Val loss: 0.34455519914627075\n",
            "Training loss: 0.34136492013931274\n",
            "Val loss: 0.3445225954055786\n",
            "Training loss: 0.34074388941129047\n",
            "Val loss: 0.3445284366607666\n",
            "Training loss: 0.34084293246269226\n",
            "Val loss: 0.34450849890708923\n",
            "Training loss: 0.34312082330385846\n",
            "Val loss: 0.34447818994522095\n",
            "Training loss: 0.3403618435064952\n",
            "Val loss: 0.3444405496120453\n",
            "Training loss: 0.34020455678304035\n",
            "Val loss: 0.3444714844226837\n",
            "Training loss: 0.34055662155151367\n",
            "Val loss: 0.3445013165473938\n",
            "Training loss: 0.3398219843705495\n",
            "Val loss: 0.3445195257663727\n",
            "Training loss: 0.3407890796661377\n",
            "Val loss: 0.34455999732017517\n",
            "Training loss: 0.3413451910018921\n",
            "Val loss: 0.34454894065856934\n",
            "Training loss: 0.3415064613024394\n",
            "Val loss: 0.34454041719436646\n",
            "Training loss: 0.3396765987078349\n",
            "Val loss: 0.344490647315979\n",
            "Training loss: 0.3409242630004883\n",
            "Val loss: 0.3444964587688446\n",
            "Training loss: 0.3440934717655182\n",
            "Val loss: 0.3444298207759857\n",
            "Training loss: 0.3429264525572459\n",
            "Val loss: 0.34430626034736633\n",
            "Training loss: 0.33895085255304974\n",
            "Val loss: 0.34424012899398804\n",
            "Training loss: 0.3419411977132161\n",
            "Val loss: 0.3442334830760956\n",
            "Training loss: 0.34043623010317486\n",
            "Val loss: 0.3442486822605133\n",
            "Training loss: 0.3409787913163503\n",
            "Val loss: 0.3442673981189728\n",
            "Training loss: 0.3414575755596161\n",
            "Val loss: 0.3441942036151886\n",
            "Training loss: 0.3430274724960327\n",
            "Val loss: 0.34417030215263367\n",
            "Training loss: 0.34142398834228516\n",
            "Val loss: 0.34414196014404297\n",
            "Training loss: 0.3408932189146678\n",
            "Val loss: 0.34414440393447876\n",
            "Training loss: 0.3418044149875641\n",
            "Val loss: 0.3442144989967346\n",
            "Training loss: 0.3401804765065511\n",
            "Val loss: 0.34425118565559387\n",
            "Training loss: 0.3429904083410899\n",
            "Val loss: 0.34431397914886475\n",
            "Training loss: 0.3420562545458476\n",
            "Val loss: 0.3443847596645355\n",
            "Training loss: 0.34030521909395856\n",
            "Val loss: 0.34433141350746155\n",
            "Training loss: 0.3392457862695058\n",
            "Val loss: 0.34435516595840454\n",
            "Training loss: 0.33913468321164447\n",
            "Val loss: 0.3443872630596161\n",
            "Training loss: 0.3408961395422618\n",
            "Val loss: 0.3443596065044403\n",
            "Training loss: 0.33897383014361065\n",
            "Val loss: 0.3443247079849243\n",
            "Training loss: 0.33899184068044025\n",
            "Val loss: 0.34435200691223145\n",
            "Training loss: 0.3445176879564921\n",
            "Val loss: 0.3442816138267517\n",
            "Training loss: 0.3410046398639679\n",
            "Val loss: 0.34424129128456116\n",
            "Training loss: 0.34111777941385907\n",
            "Val loss: 0.3441890776157379\n",
            "Training loss: 0.34224332372347516\n",
            "Val loss: 0.34414055943489075\n",
            "Training loss: 0.342164009809494\n",
            "Val loss: 0.34417828917503357\n",
            "Training loss: 0.33974690238634747\n",
            "Val loss: 0.3441767692565918\n",
            "Training loss: 0.33798396587371826\n",
            "Val loss: 0.344185471534729\n",
            "Training loss: 0.34114302198092145\n",
            "Val loss: 0.34418684244155884\n",
            "Training loss: 0.3399878839651744\n",
            "Val loss: 0.3441714346408844\n",
            "Training loss: 0.33912894129753113\n",
            "Val loss: 0.344271719455719\n",
            "Training loss: 0.3396684130032857\n",
            "Val loss: 0.34430062770843506\n",
            "Training loss: 0.33965470393498737\n",
            "Val loss: 0.34433692693710327\n",
            "Training loss: 0.33995089928309125\n",
            "Val loss: 0.34443485736846924\n",
            "Training loss: 0.3382691542307536\n",
            "Val loss: 0.344414621591568\n",
            "Training loss: 0.3395396073659261\n",
            "Val loss: 0.3443569242954254\n",
            "Training loss: 0.33858110507329303\n",
            "Val loss: 0.3443877696990967\n",
            "Training loss: 0.3426807423432668\n",
            "Val loss: 0.34432724118232727\n",
            "Training loss: 0.34171796838442486\n",
            "Val loss: 0.34423330426216125\n",
            "Training loss: 0.34070953726768494\n",
            "Val loss: 0.3442278206348419\n",
            "Training loss: 0.34020939469337463\n",
            "Val loss: 0.3442905843257904\n",
            "Training loss: 0.34116782744725543\n",
            "Val loss: 0.34432676434516907\n",
            "Training loss: 0.34196534752845764\n",
            "Val loss: 0.34428760409355164\n",
            "Training loss: 0.3421318332354228\n",
            "Val loss: 0.34424370527267456\n",
            "Training loss: 0.34202439586321515\n",
            "Val loss: 0.34410974383354187\n",
            "Training loss: 0.3395894368489583\n",
            "Val loss: 0.34405753016471863\n",
            "Training loss: 0.34161628286043805\n",
            "Val loss: 0.344005823135376\n",
            "Training loss: 0.3403347432613373\n",
            "Val loss: 0.34394916892051697\n",
            "Training loss: 0.3408018747965495\n",
            "Val loss: 0.34392082691192627\n",
            "Training loss: 0.33963243166605633\n",
            "Val loss: 0.3439314663410187\n",
            "Training loss: 0.34002843499183655\n",
            "Val loss: 0.3439512252807617\n",
            "Training loss: 0.3396870791912079\n",
            "Val loss: 0.343866765499115\n",
            "Training loss: 0.3430557946364085\n",
            "Val loss: 0.3438238799571991\n",
            "Training loss: 0.34080294768015545\n",
            "Val loss: 0.34395915269851685\n",
            "Training loss: 0.33677904804547626\n",
            "Val loss: 0.3440198302268982\n",
            "Training loss: 0.3409932553768158\n",
            "Val loss: 0.34400883316993713\n",
            "Training loss: 0.3409789800643921\n",
            "Val loss: 0.3440574109554291\n",
            "Training loss: 0.33972054719924927\n",
            "Val loss: 0.34404298663139343\n",
            "Training loss: 0.3411881426970164\n",
            "Val loss: 0.34402942657470703\n",
            "Training loss: 0.34398751457532245\n",
            "Val loss: 0.34397274255752563\n",
            "Training loss: 0.3390364150206248\n",
            "Val loss: 0.34396272897720337\n",
            "Training loss: 0.34028639396031696\n",
            "Val loss: 0.34396108984947205\n",
            "Training loss: 0.34230806430180866\n",
            "Val loss: 0.34385380148887634\n",
            "Training loss: 0.3398250341415405\n",
            "Val loss: 0.3438356816768646\n",
            "Training loss: 0.33891965945561725\n",
            "Val loss: 0.34388044476509094\n",
            "Training loss: 0.3401426374912262\n",
            "Val loss: 0.34380194544792175\n",
            "Training loss: 0.33870725830396015\n",
            "Val loss: 0.343837708234787\n",
            "Training loss: 0.3398866355419159\n",
            "Val loss: 0.3438354730606079\n",
            "Training loss: 0.3397345244884491\n",
            "Val loss: 0.3438064754009247\n",
            "Training loss: 0.34142210086186725\n",
            "Val loss: 0.34380409121513367\n",
            "Training loss: 0.3403183122475942\n",
            "Val loss: 0.34374216198921204\n",
            "Training loss: 0.34048258264859516\n",
            "Val loss: 0.34371230006217957\n",
            "Training loss: 0.3396620949109395\n",
            "Val loss: 0.34366852045059204\n",
            "Training loss: 0.3407602906227112\n",
            "Val loss: 0.34371089935302734\n",
            "Training loss: 0.3419196605682373\n",
            "Val loss: 0.3436914384365082\n",
            "Training loss: 0.33809828758239746\n",
            "Val loss: 0.34367480874061584\n",
            "Training loss: 0.33959253629048664\n",
            "Val loss: 0.34364330768585205\n",
            "Training loss: 0.3375715911388397\n",
            "Val loss: 0.3435903787612915\n",
            "Training loss: 0.3382764458656311\n",
            "Val loss: 0.3435208201408386\n",
            "Training loss: 0.3392873704433441\n",
            "Val loss: 0.34344688057899475\n",
            "Training loss: 0.3403233786424001\n",
            "Val loss: 0.3434428870677948\n",
            "Training loss: 0.339772234360377\n",
            "Val loss: 0.3434073328971863\n",
            "Training loss: 0.33882905046145123\n",
            "Val loss: 0.34342366456985474\n",
            "Training loss: 0.33849355578422546\n",
            "Val loss: 0.3434671461582184\n",
            "Training loss: 0.3389667471249898\n",
            "Val loss: 0.3434683680534363\n",
            "Training loss: 0.33996181686719257\n",
            "Val loss: 0.3435320556163788\n",
            "Training loss: 0.33826027313868207\n",
            "Val loss: 0.34350574016571045\n",
            "Training loss: 0.34133830666542053\n",
            "Val loss: 0.3435523808002472\n",
            "Training loss: 0.3411455651124318\n",
            "Val loss: 0.34352150559425354\n",
            "Training loss: 0.3376711706320445\n",
            "Val loss: 0.3435233235359192\n",
            "Training loss: 0.33853686849276227\n",
            "Val loss: 0.34351402521133423\n",
            "Training loss: 0.3402727047602336\n",
            "Val loss: 0.34350162744522095\n",
            "Training loss: 0.33941514293352765\n",
            "Val loss: 0.3435254693031311\n",
            "Training loss: 0.3417371114095052\n",
            "Val loss: 0.34353312849998474\n",
            "Training loss: 0.33914914727211\n",
            "Val loss: 0.34362852573394775\n",
            "Training loss: 0.34221219023068744\n",
            "Val loss: 0.343508780002594\n",
            "Training loss: 0.33965185284614563\n",
            "Val loss: 0.3435088098049164\n",
            "Training loss: 0.3400333921114604\n",
            "Val loss: 0.3434704542160034\n",
            "Training loss: 0.33795875310897827\n",
            "Val loss: 0.34351181983947754\n",
            "Training loss: 0.3404659032821655\n",
            "Val loss: 0.343506395816803\n",
            "Training loss: 0.3421957592169444\n",
            "Val loss: 0.3435189723968506\n",
            "Training loss: 0.34059203664461773\n",
            "Val loss: 0.343475878238678\n",
            "Training loss: 0.34277581175168353\n",
            "Val loss: 0.34349149465560913\n",
            "Training loss: 0.3390501141548157\n",
            "Val loss: 0.34348341822624207\n",
            "Training loss: 0.33970927198727924\n",
            "Val loss: 0.34353160858154297\n",
            "Training loss: 0.3401740590731303\n",
            "Val loss: 0.3434603214263916\n",
            "Training loss: 0.33691046635309857\n",
            "Val loss: 0.3433820903301239\n",
            "Training loss: 0.33934446175893146\n",
            "Val loss: 0.34334567189216614\n",
            "Training loss: 0.3414383331934611\n",
            "Val loss: 0.34328824281692505\n",
            "Training loss: 0.339799036582311\n",
            "Val loss: 0.3432817757129669\n",
            "Training loss: 0.3399016261100769\n",
            "Val loss: 0.34320178627967834\n",
            "Training loss: 0.33965348203976947\n",
            "Val loss: 0.34313565492630005\n",
            "Training loss: 0.340141365925471\n",
            "Val loss: 0.3431232273578644\n",
            "Training loss: 0.33924094835917157\n",
            "Val loss: 0.3430408835411072\n",
            "Training loss: 0.34260619680086773\n",
            "Val loss: 0.3429662585258484\n",
            "Training loss: 0.34021419286727905\n",
            "Val loss: 0.3429904580116272\n",
            "Training loss: 0.33793696761131287\n",
            "Val loss: 0.34304946660995483\n",
            "Training loss: 0.3373142381509145\n",
            "Val loss: 0.3430650234222412\n",
            "Training loss: 0.33726079265276593\n",
            "Val loss: 0.3431258499622345\n",
            "Training loss: 0.33904800812403363\n",
            "Val loss: 0.3432058095932007\n",
            "Training loss: 0.3386240402857463\n",
            "Val loss: 0.343231737613678\n",
            "Training loss: 0.33929933110872906\n",
            "Val loss: 0.3432339131832123\n",
            "Training loss: 0.33873629570007324\n",
            "Val loss: 0.34320205450057983\n",
            "Training loss: 0.3390185435612996\n",
            "Val loss: 0.34319034218788147\n",
            "Training loss: 0.33946295579274494\n",
            "Val loss: 0.34315457940101624\n",
            "Training loss: 0.3408020834128062\n",
            "Val loss: 0.3430790603160858\n",
            "Training loss: 0.3391866485277812\n",
            "Val loss: 0.34305304288864136\n",
            "Training loss: 0.33984198172887164\n",
            "Val loss: 0.3429785668849945\n",
            "Training loss: 0.3380833864212036\n",
            "Val loss: 0.34294211864471436\n",
            "Training loss: 0.3390933076540629\n",
            "Val loss: 0.34293538331985474\n",
            "Training loss: 0.33887364466985065\n",
            "Val loss: 0.3429931402206421\n",
            "Training loss: 0.33871572216351825\n",
            "Val loss: 0.34298089146614075\n",
            "Training loss: 0.33756859103838605\n",
            "Val loss: 0.3430570065975189\n",
            "Training loss: 0.34143338600794476\n",
            "Val loss: 0.34301623702049255\n",
            "Training loss: 0.3368155062198639\n",
            "Val loss: 0.34303563833236694\n",
            "Training loss: 0.3395656645298004\n",
            "Val loss: 0.34305477142333984\n",
            "Training loss: 0.34092026948928833\n",
            "Val loss: 0.3429795503616333\n",
            "Training loss: 0.3387203514575958\n",
            "Val loss: 0.3428965210914612\n",
            "Training loss: 0.33767178654670715\n",
            "Val loss: 0.3428758978843689\n",
            "Training loss: 0.3382098972797394\n",
            "Val loss: 0.3428547978401184\n",
            "Training loss: 0.3401663601398468\n",
            "Val loss: 0.34283655881881714\n",
            "Training loss: 0.338471919298172\n",
            "Val loss: 0.34286046028137207\n",
            "Training loss: 0.3383588989575704\n",
            "Val loss: 0.3428652286529541\n",
            "Training loss: 0.338904470205307\n",
            "Val loss: 0.3428160548210144\n",
            "Training loss: 0.33867116769154865\n",
            "Val loss: 0.34281662106513977\n",
            "Training loss: 0.33871716260910034\n",
            "Val loss: 0.3428359031677246\n",
            "Training loss: 0.33929980794588727\n",
            "Val loss: 0.3428572416305542\n",
            "Training loss: 0.33754317959149677\n",
            "Val loss: 0.34281930327415466\n",
            "Training loss: 0.33905049165089923\n",
            "Val loss: 0.3427931070327759\n",
            "Training loss: 0.3378537098566691\n",
            "Val loss: 0.34283924102783203\n",
            "Training loss: 0.3389713068803151\n",
            "Val loss: 0.3428402245044708\n",
            "Training loss: 0.33782915274302167\n",
            "Val loss: 0.342828631401062\n",
            "Training loss: 0.33937935034434\n",
            "Val loss: 0.3428329527378082\n",
            "Training loss: 0.34044236938158673\n",
            "Val loss: 0.34288182854652405\n",
            "Training loss: 0.3379615247249603\n",
            "Val loss: 0.3429236114025116\n",
            "Training loss: 0.34637224674224854\n",
            "Val loss: 0.34275296330451965\n",
            "Training loss: 0.3444300691286723\n",
            "Val loss: 0.3428359031677246\n",
            "Training loss: 0.33920784791310626\n",
            "Val loss: 0.3428836166858673\n",
            "Training loss: 0.33775267004966736\n",
            "Val loss: 0.3428536653518677\n",
            "Training loss: 0.3408077557881673\n",
            "Val loss: 0.3428029417991638\n",
            "Training loss: 0.33847440282503766\n",
            "Val loss: 0.34274035692214966\n",
            "Training loss: 0.3410582443078359\n",
            "Val loss: 0.3426625728607178\n",
            "Training loss: 0.3388950725396474\n",
            "Val loss: 0.3426401615142822\n",
            "Training loss: 0.33996633688608807\n",
            "Val loss: 0.3426065444946289\n",
            "Training loss: 0.34057725469271344\n",
            "Val loss: 0.34257569909095764\n",
            "Training loss: 0.3386830687522888\n",
            "Val loss: 0.34253478050231934\n",
            "Training loss: 0.33840087056159973\n",
            "Val loss: 0.34251168370246887\n",
            "Training loss: 0.3376222352186839\n",
            "Val loss: 0.34250059723854065\n",
            "Training loss: 0.33830593029658\n",
            "Val loss: 0.3424749970436096\n",
            "Training loss: 0.3391343255837758\n",
            "Val loss: 0.34242191910743713\n",
            "Training loss: 0.3382986783981323\n",
            "Val loss: 0.3424927294254303\n",
            "Training loss: 0.33868998289108276\n",
            "Val loss: 0.3424299359321594\n",
            "Training loss: 0.33959315220514935\n",
            "Val loss: 0.34243103861808777\n",
            "Training loss: 0.34037430087725323\n",
            "Val loss: 0.3423662483692169\n",
            "Training loss: 0.33763599395751953\n",
            "Val loss: 0.34234264492988586\n",
            "Training loss: 0.33728991945584613\n",
            "Val loss: 0.34237685799598694\n",
            "Training loss: 0.336212416489919\n",
            "Val loss: 0.34245437383651733\n",
            "Training loss: 0.33895309766133624\n",
            "Val loss: 0.34242045879364014\n",
            "Training loss: 0.3388683497905731\n",
            "Val loss: 0.3423925042152405\n",
            "Training loss: 0.3371826112270355\n",
            "Val loss: 0.3423267602920532\n",
            "Training loss: 0.34000471234321594\n",
            "Val loss: 0.3423311710357666\n",
            "Training loss: 0.34097928802172345\n",
            "Val loss: 0.34224966168403625\n",
            "Training loss: 0.34106728434562683\n",
            "Val loss: 0.34231001138687134\n",
            "Training loss: 0.3389275868733724\n",
            "Val loss: 0.3422293961048126\n",
            "Training loss: 0.3369862735271454\n",
            "Val loss: 0.3421850800514221\n",
            "Training loss: 0.3390643397967021\n",
            "Val loss: 0.34219124913215637\n",
            "Training loss: 0.3385155399640401\n",
            "Val loss: 0.34216320514678955\n",
            "Training loss: 0.3393424650033315\n",
            "Val loss: 0.34218865633010864\n",
            "Training loss: 0.33699289957682294\n",
            "Val loss: 0.34218841791152954\n",
            "Training loss: 0.33873621622721356\n",
            "Val loss: 0.34218353033065796\n",
            "Training loss: 0.337059626976649\n",
            "Val loss: 0.34207430481910706\n",
            "Training loss: 0.33846386273701984\n",
            "Val loss: 0.3420637547969818\n",
            "Training loss: 0.33983899156252545\n",
            "Val loss: 0.3420458436012268\n",
            "Training loss: 0.3371310830116272\n",
            "Val loss: 0.3420332670211792\n",
            "Training loss: 0.34166186054547626\n",
            "Val loss: 0.3420396149158478\n",
            "Training loss: 0.33983302116394043\n",
            "Val loss: 0.34207046031951904\n",
            "Training loss: 0.33904126286506653\n",
            "Val loss: 0.34209105372428894\n",
            "Training loss: 0.3383730153242747\n",
            "Val loss: 0.3420967757701874\n",
            "Training loss: 0.339945654074351\n",
            "Val loss: 0.34211838245391846\n",
            "Training loss: 0.33912550409634906\n",
            "Val loss: 0.34212660789489746\n",
            "Training loss: 0.33605048060417175\n",
            "Val loss: 0.34213659167289734\n",
            "Training loss: 0.3380809227625529\n",
            "Val loss: 0.3421848714351654\n",
            "Training loss: 0.3364272614320119\n",
            "Val loss: 0.34217900037765503\n",
            "Training loss: 0.3384588261445363\n",
            "Val loss: 0.34217241406440735\n",
            "Training loss: 0.3365638653437297\n",
            "Val loss: 0.34216949343681335\n",
            "Training loss: 0.3374584913253784\n",
            "Val loss: 0.34217968583106995\n",
            "Training loss: 0.3381566007932027\n",
            "Val loss: 0.3421803116798401\n",
            "Training loss: 0.3415773908297221\n",
            "Val loss: 0.34211382269859314\n",
            "Training loss: 0.3360113004843394\n",
            "Val loss: 0.34214264154434204\n",
            "Training loss: 0.340359091758728\n",
            "Val loss: 0.3421030044555664\n",
            "Training loss: 0.33769817153612774\n",
            "Val loss: 0.3420208692550659\n",
            "Training loss: 0.3360551794370015\n",
            "Val loss: 0.3419795334339142\n",
            "Training loss: 0.3372296293576558\n",
            "Val loss: 0.34201061725616455\n",
            "Training loss: 0.336752990881602\n",
            "Val loss: 0.34197038412094116\n",
            "Training loss: 0.3387064238389333\n",
            "Val loss: 0.3419404625892639\n",
            "Training loss: 0.33857495586077374\n",
            "Val loss: 0.3420104682445526\n",
            "Training loss: 0.3355282048384349\n",
            "Val loss: 0.34206917881965637\n",
            "Training loss: 0.33540953199068707\n",
            "Val loss: 0.3421344459056854\n",
            "Training loss: 0.3371940354506175\n",
            "Val loss: 0.34216615557670593\n",
            "Training loss: 0.33940136432647705\n",
            "Val loss: 0.3421550691127777\n",
            "Training loss: 0.3379455904165904\n",
            "Val loss: 0.3420621156692505\n",
            "Training loss: 0.33770490686098736\n",
            "Val loss: 0.3420267105102539\n",
            "Training loss: 0.33829590678215027\n",
            "Val loss: 0.3420201539993286\n",
            "Training loss: 0.339829425017039\n",
            "Val loss: 0.34193459153175354\n",
            "Training loss: 0.3377949297428131\n",
            "Val loss: 0.34193938970565796\n",
            "Training loss: 0.33723384141921997\n",
            "Val loss: 0.3419123589992523\n",
            "Training loss: 0.337200532356898\n",
            "Val loss: 0.3419579565525055\n",
            "Training loss: 0.33573689063390094\n",
            "Val loss: 0.34189802408218384\n",
            "Training loss: 0.3405122756958008\n",
            "Val loss: 0.34193655848503113\n",
            "Training loss: 0.33836175004641217\n",
            "Val loss: 0.3418806195259094\n",
            "Training loss: 0.33852000037829083\n",
            "Val loss: 0.34183967113494873\n",
            "Training loss: 0.33767467737197876\n",
            "Val loss: 0.3418941795825958\n",
            "Training loss: 0.33629079659779865\n",
            "Val loss: 0.3418745994567871\n",
            "Training loss: 0.3345495065053304\n",
            "Val loss: 0.34182700514793396\n",
            "Training loss: 0.3373454511165619\n",
            "Val loss: 0.3417818546295166\n",
            "Training loss: 0.33651243646939594\n",
            "Val loss: 0.3417661786079407\n",
            "Training loss: 0.3361948033173879\n",
            "Val loss: 0.3417665362358093\n",
            "Training loss: 0.33751214543978375\n",
            "Val loss: 0.34172821044921875\n",
            "Training loss: 0.33630650242169696\n",
            "Val loss: 0.341743141412735\n",
            "Training loss: 0.3378036618232727\n",
            "Val loss: 0.3417195975780487\n",
            "Training loss: 0.3384919265906016\n",
            "Val loss: 0.3416751027107239\n",
            "Training loss: 0.3374216953913371\n",
            "Val loss: 0.34163665771484375\n",
            "Training loss: 0.33898188670476276\n",
            "Val loss: 0.3416769504547119\n",
            "Training loss: 0.3366960287094116\n",
            "Val loss: 0.34161967039108276\n",
            "Training loss: 0.33807694911956787\n",
            "Val loss: 0.34157636761665344\n",
            "Training loss: 0.3376210629940033\n",
            "Val loss: 0.3415641784667969\n",
            "Training loss: 0.3360087176163991\n",
            "Val loss: 0.34157663583755493\n",
            "Training loss: 0.33554285764694214\n",
            "Val loss: 0.34145063161849976\n",
            "Training loss: 0.34035388628641766\n",
            "Val loss: 0.3413883149623871\n",
            "Training loss: 0.3378250300884247\n",
            "Val loss: 0.3413779139518738\n",
            "Training loss: 0.3369066119194031\n",
            "Val loss: 0.3413693606853485\n",
            "Training loss: 0.33572809894879657\n",
            "Val loss: 0.34133365750312805\n",
            "Training loss: 0.3409286638100942\n",
            "Val loss: 0.34127023816108704\n",
            "Training loss: 0.3352358937263489\n",
            "Val loss: 0.3412037491798401\n",
            "Training loss: 0.3384053309758504\n",
            "Val loss: 0.3411470353603363\n",
            "Training loss: 0.33526506026585895\n",
            "Val loss: 0.3411763906478882\n",
            "Training loss: 0.33765365680058795\n",
            "Val loss: 0.3410787582397461\n",
            "Training loss: 0.3425522744655609\n",
            "Val loss: 0.3410114347934723\n",
            "Training loss: 0.339860071738561\n",
            "Val loss: 0.3409743309020996\n",
            "Training loss: 0.3386420210202535\n",
            "Val loss: 0.3409256935119629\n",
            "Training loss: 0.3359229862689972\n",
            "Val loss: 0.34087610244750977\n",
            "Training loss: 0.33643226822217304\n",
            "Val loss: 0.3407781720161438\n",
            "Training loss: 0.3372386395931244\n",
            "Val loss: 0.34073179960250854\n",
            "Training loss: 0.3353760540485382\n",
            "Val loss: 0.34074145555496216\n",
            "Training loss: 0.33767929673194885\n",
            "Val loss: 0.3407537043094635\n",
            "Training loss: 0.3374719520409902\n",
            "Val loss: 0.34078842401504517\n",
            "Training loss: 0.33737753828366596\n",
            "Val loss: 0.34082525968551636\n",
            "Training loss: 0.3353133698304494\n",
            "Val loss: 0.3408747613430023\n",
            "Training loss: 0.34001301725705463\n",
            "Val loss: 0.34092509746551514\n",
            "Training loss: 0.3374673028786977\n",
            "Val loss: 0.3408411741256714\n",
            "Training loss: 0.3383309245109558\n",
            "Val loss: 0.34084874391555786\n",
            "Training loss: 0.3362125555674235\n",
            "Val loss: 0.3407745063304901\n",
            "Training loss: 0.3386004368464152\n",
            "Val loss: 0.3407121002674103\n",
            "Training loss: 0.33585062623023987\n",
            "Val loss: 0.3407095670700073\n",
            "Training loss: 0.3385690251986186\n",
            "Val loss: 0.3406432569026947\n",
            "Training loss: 0.3403776486714681\n",
            "Val loss: 0.34059959650039673\n",
            "Training loss: 0.33688337604204815\n",
            "Val loss: 0.34055787324905396\n",
            "Training loss: 0.3377396762371063\n",
            "Val loss: 0.3405107259750366\n",
            "Training loss: 0.33733415603637695\n",
            "Val loss: 0.3404802083969116\n",
            "Training loss: 0.33698247869809467\n",
            "Val loss: 0.34045863151550293\n",
            "Training loss: 0.33741910258928937\n",
            "Val loss: 0.34047022461891174\n",
            "Training loss: 0.33574679493904114\n",
            "Val loss: 0.3404381573200226\n",
            "Training loss: 0.33659927050272626\n",
            "Val loss: 0.3404507637023926\n",
            "Training loss: 0.33812830845514935\n",
            "Val loss: 0.3405023515224457\n",
            "Training loss: 0.33727015058199566\n",
            "Val loss: 0.3405630588531494\n",
            "Training loss: 0.3389153281847636\n",
            "Val loss: 0.3405281901359558\n",
            "Training loss: 0.3369304835796356\n",
            "Val loss: 0.3404947519302368\n",
            "Training loss: 0.33753450711568195\n",
            "Val loss: 0.3404979109764099\n",
            "Training loss: 0.3379379212856293\n",
            "Val loss: 0.3405115604400635\n",
            "Training loss: 0.3369913498560588\n",
            "Val loss: 0.3404800593852997\n",
            "Training loss: 0.3356226583321889\n",
            "Val loss: 0.34047532081604004\n",
            "Training loss: 0.33976199229558307\n",
            "Val loss: 0.3403717279434204\n",
            "Training loss: 0.33573225140571594\n",
            "Val loss: 0.34040552377700806\n",
            "Training loss: 0.33739494283994037\n",
            "Val loss: 0.3404073119163513\n",
            "Training loss: 0.3361237446467082\n",
            "Val loss: 0.34035763144493103\n",
            "Training loss: 0.3345240354537964\n",
            "Val loss: 0.3402971625328064\n",
            "Training loss: 0.33631328741709393\n",
            "Val loss: 0.3402310609817505\n",
            "Training loss: 0.3363087276617686\n",
            "Val loss: 0.34014737606048584\n",
            "Training loss: 0.3364937702814738\n",
            "Val loss: 0.3401729464530945\n",
            "Training loss: 0.33761651317278546\n",
            "Val loss: 0.340181440114975\n",
            "Training loss: 0.33452104528745014\n",
            "Val loss: 0.34022876620292664\n",
            "Training loss: 0.3401563862959544\n",
            "Val loss: 0.34016379714012146\n",
            "Training loss: 0.3398292462031047\n",
            "Val loss: 0.34006306529045105\n",
            "Training loss: 0.3366411328315735\n",
            "Val loss: 0.3400864601135254\n",
            "Training loss: 0.3378227154413859\n",
            "Val loss: 0.34010085463523865\n",
            "Training loss: 0.3352808952331543\n",
            "Val loss: 0.34009459614753723\n",
            "Training loss: 0.33651621143023175\n",
            "Val loss: 0.3401029109954834\n",
            "Training loss: 0.3362690607706706\n",
            "Val loss: 0.3400360643863678\n",
            "Training loss: 0.33691805601119995\n",
            "Val loss: 0.34005972743034363\n",
            "Training loss: 0.33779459198315936\n",
            "Val loss: 0.3401305377483368\n",
            "Training loss: 0.3395322660605113\n",
            "Val loss: 0.34013763070106506\n",
            "Training loss: 0.3352097074190776\n",
            "Val loss: 0.3401806056499481\n",
            "Training loss: 0.33722039063771564\n",
            "Val loss: 0.34014925360679626\n",
            "Training loss: 0.3350942333539327\n",
            "Val loss: 0.3401733338832855\n",
            "Training loss: 0.33404003580411273\n",
            "Val loss: 0.34006258845329285\n",
            "Training loss: 0.3355903625488281\n",
            "Val loss: 0.3399934768676758\n",
            "Training loss: 0.3362589677174886\n",
            "Val loss: 0.3399249315261841\n",
            "Training loss: 0.33569703499476117\n",
            "Val loss: 0.33992359042167664\n",
            "Training loss: 0.3352847596009572\n",
            "Val loss: 0.3398977518081665\n",
            "Training loss: 0.3352416753768921\n",
            "Val loss: 0.3399026691913605\n",
            "Training loss: 0.3366033434867859\n",
            "Val loss: 0.339821457862854\n",
            "Training loss: 0.3340320885181427\n",
            "Val loss: 0.3397718071937561\n",
            "Training loss: 0.3333820104598999\n",
            "Val loss: 0.3397088944911957\n",
            "Training loss: 0.33481210470199585\n",
            "Val loss: 0.33966076374053955\n",
            "Training loss: 0.332863450050354\n",
            "Val loss: 0.3396555483341217\n",
            "Training loss: 0.3349992533524831\n",
            "Val loss: 0.3396087884902954\n",
            "Training loss: 0.33675307035446167\n",
            "Val loss: 0.33959197998046875\n",
            "Training loss: 0.3349275588989258\n",
            "Val loss: 0.339593768119812\n",
            "Training loss: 0.33603954315185547\n",
            "Val loss: 0.33956706523895264\n",
            "Training loss: 0.33731117844581604\n",
            "Val loss: 0.3395629823207855\n",
            "Training loss: 0.3373811940352122\n",
            "Val loss: 0.3395216464996338\n",
            "Training loss: 0.33404917518297833\n",
            "Val loss: 0.3395194709300995\n",
            "Training loss: 0.3349774181842804\n",
            "Val loss: 0.33955520391464233\n",
            "Training loss: 0.33580149213473004\n",
            "Val loss: 0.33955371379852295\n",
            "Training loss: 0.3351137538750966\n",
            "Val loss: 0.3394581377506256\n",
            "Training loss: 0.3356448809305827\n",
            "Val loss: 0.339415967464447\n",
            "Training loss: 0.33618982632954914\n",
            "Val loss: 0.3393683135509491\n",
            "Training loss: 0.3336467544237773\n",
            "Val loss: 0.33931055665016174\n",
            "Training loss: 0.33682077129681903\n",
            "Val loss: 0.33923277258872986\n",
            "Training loss: 0.33506367603937787\n",
            "Val loss: 0.3392367660999298\n",
            "Training loss: 0.33601171771685284\n",
            "Val loss: 0.3392091989517212\n",
            "Training loss: 0.3356817166010539\n",
            "Val loss: 0.33922678232192993\n",
            "Training loss: 0.3347519636154175\n",
            "Val loss: 0.3391450345516205\n",
            "Training loss: 0.3384994665781657\n",
            "Val loss: 0.33909517526626587\n",
            "Training loss: 0.33524197340011597\n",
            "Val loss: 0.3390258252620697\n",
            "Training loss: 0.3345344563325246\n",
            "Val loss: 0.3390319347381592\n",
            "Training loss: 0.3340914448102315\n",
            "Val loss: 0.3389703333377838\n",
            "Training loss: 0.3350950578848521\n",
            "Val loss: 0.338909387588501\n",
            "Training loss: 0.3369606037934621\n",
            "Val loss: 0.33887436985969543\n",
            "Training loss: 0.33665650089581806\n",
            "Val loss: 0.3387928009033203\n",
            "Training loss: 0.33531155188878375\n",
            "Val loss: 0.338783860206604\n",
            "Training loss: 0.3379208544890086\n",
            "Val loss: 0.33886340260505676\n",
            "Training loss: 0.33537904421488446\n",
            "Val loss: 0.33892059326171875\n",
            "Training loss: 0.33526408672332764\n",
            "Val loss: 0.33889931440353394\n",
            "Training loss: 0.3384176294008891\n",
            "Val loss: 0.3389049470424652\n",
            "Training loss: 0.3345904052257538\n",
            "Val loss: 0.3388676047325134\n",
            "Training loss: 0.3331111669540405\n",
            "Val loss: 0.3388252556324005\n",
            "Training loss: 0.3354701300462087\n",
            "Val loss: 0.3387574553489685\n",
            "Training loss: 0.3330817719300588\n",
            "Val loss: 0.3387758433818817\n",
            "Training loss: 0.33526119589805603\n",
            "Val loss: 0.33880314230918884\n",
            "Training loss: 0.3337111175060272\n",
            "Val loss: 0.33880043029785156\n",
            "Training loss: 0.33521023392677307\n",
            "Val loss: 0.3387807607650757\n",
            "Training loss: 0.34078942735989887\n",
            "Val loss: 0.3387185335159302\n",
            "Training loss: 0.33513810237248737\n",
            "Val loss: 0.33860790729522705\n",
            "Training loss: 0.3371550639470418\n",
            "Val loss: 0.33861109614372253\n",
            "Training loss: 0.33311445514361065\n",
            "Val loss: 0.3385656476020813\n",
            "Training loss: 0.33652199308077496\n",
            "Val loss: 0.33846309781074524\n",
            "Training loss: 0.3366774419943492\n",
            "Val loss: 0.33832207322120667\n",
            "Training loss: 0.33340534567832947\n",
            "Val loss: 0.338328093290329\n",
            "Training loss: 0.33492294947306317\n",
            "Val loss: 0.3383350670337677\n",
            "Training loss: 0.3356538712978363\n",
            "Val loss: 0.338377982378006\n",
            "Training loss: 0.3332174817721049\n",
            "Val loss: 0.33846989274024963\n",
            "Training loss: 0.33579423030217487\n",
            "Val loss: 0.3384988307952881\n",
            "Training loss: 0.3354408343633016\n",
            "Val loss: 0.33850133419036865\n",
            "Training loss: 0.3338320553302765\n",
            "Val loss: 0.3384408950805664\n",
            "Training loss: 0.3364510734875997\n",
            "Val loss: 0.338421106338501\n",
            "Training loss: 0.33660125732421875\n",
            "Val loss: 0.33838924765586853\n",
            "Training loss: 0.33580918113390607\n",
            "Val loss: 0.33840999007225037\n",
            "Training loss: 0.3360946873823802\n",
            "Val loss: 0.33840030431747437\n",
            "Training loss: 0.3340688149134318\n",
            "Val loss: 0.33842185139656067\n",
            "Training loss: 0.33479011058807373\n",
            "Val loss: 0.33842217922210693\n",
            "Training loss: 0.33558542529741925\n",
            "Val loss: 0.33842021226882935\n",
            "Training loss: 0.33657199144363403\n",
            "Val loss: 0.33839547634124756\n",
            "Training loss: 0.33436207969983417\n",
            "Val loss: 0.3384244441986084\n",
            "Training loss: 0.3352912664413452\n",
            "Val loss: 0.3384495973587036\n",
            "Training loss: 0.336630642414093\n",
            "Val loss: 0.33845290541648865\n",
            "Training loss: 0.3352022171020508\n",
            "Val loss: 0.3384772837162018\n",
            "Training loss: 0.3332031766573588\n",
            "Val loss: 0.33854955434799194\n",
            "Training loss: 0.33701642354329425\n",
            "Val loss: 0.3385274410247803\n",
            "Training loss: 0.33480508128801983\n",
            "Val loss: 0.33851858973503113\n",
            "Training loss: 0.33457016944885254\n",
            "Val loss: 0.33849313855171204\n",
            "Training loss: 0.3347264329592387\n",
            "Val loss: 0.3384329378604889\n",
            "Training loss: 0.3403519093990326\n",
            "Val loss: 0.33837151527404785\n",
            "Training loss: 0.33404292662938434\n",
            "Val loss: 0.33839815855026245\n",
            "Training loss: 0.33469801147778827\n",
            "Val loss: 0.3383801281452179\n",
            "Training loss: 0.33492417136828107\n",
            "Val loss: 0.33834347128868103\n",
            "Training loss: 0.3353496889273326\n",
            "Val loss: 0.33834150433540344\n",
            "Training loss: 0.33638447523117065\n",
            "Val loss: 0.33837103843688965\n",
            "Training loss: 0.3382615347703298\n",
            "Val loss: 0.3383798599243164\n",
            "Training loss: 0.33442527055740356\n",
            "Val loss: 0.33838796615600586\n",
            "Training loss: 0.3368326425552368\n",
            "Val loss: 0.33840346336364746\n",
            "Training loss: 0.33258168896039325\n",
            "Val loss: 0.33843424916267395\n",
            "Training loss: 0.33457546432813007\n",
            "Val loss: 0.3383823037147522\n",
            "Training loss: 0.33646132548650104\n",
            "Val loss: 0.3383656144142151\n",
            "Training loss: 0.33482056856155396\n",
            "Val loss: 0.33837389945983887\n",
            "Training loss: 0.33726417024930316\n",
            "Val loss: 0.33834683895111084\n",
            "Training loss: 0.33659444252649945\n",
            "Val loss: 0.33840176463127136\n",
            "Training loss: 0.33350251118342084\n",
            "Val loss: 0.33841362595558167\n",
            "Training loss: 0.33435487747192383\n",
            "Val loss: 0.3383748531341553\n",
            "Training loss: 0.33354851603507996\n",
            "Val loss: 0.3384155333042145\n",
            "Training loss: 0.3334712286790212\n",
            "Val loss: 0.33837422728538513\n",
            "Training loss: 0.3358500798543294\n",
            "Val loss: 0.33837127685546875\n",
            "Training loss: 0.3344183564186096\n",
            "Val loss: 0.3384207487106323\n",
            "Training loss: 0.33351075649261475\n",
            "Val loss: 0.33849382400512695\n",
            "Training loss: 0.3333079020182292\n",
            "Val loss: 0.33845171332359314\n",
            "Training loss: 0.33079500993092853\n",
            "Val loss: 0.338394433259964\n",
            "Training loss: 0.3347485363483429\n",
            "Val loss: 0.33840030431747437\n",
            "Training loss: 0.3388638099034627\n",
            "Val loss: 0.33840903639793396\n",
            "Training loss: 0.33593278129895526\n",
            "Val loss: 0.338519811630249\n",
            "Training loss: 0.3349070151646932\n",
            "Val loss: 0.3385256826877594\n",
            "Training loss: 0.33471670746803284\n",
            "Val loss: 0.33852797746658325\n",
            "Training loss: 0.33355480432510376\n",
            "Val loss: 0.33851853013038635\n",
            "Training loss: 0.33572013179461163\n",
            "Val loss: 0.33848822116851807\n",
            "Training loss: 0.33380067348480225\n",
            "Val loss: 0.33853188157081604\n",
            "Training loss: 0.3341174324353536\n",
            "Val loss: 0.3385414779186249\n",
            "Training loss: 0.33599719405174255\n",
            "Val loss: 0.3385331332683563\n",
            "Training loss: 0.3329932490984599\n",
            "Val loss: 0.33855462074279785\n",
            "Training loss: 0.33353374401728314\n",
            "Val loss: 0.3385705053806305\n",
            "Training loss: 0.3351712127526601\n",
            "Val loss: 0.33862563967704773\n",
            "Training loss: 0.33483795324961346\n",
            "Val loss: 0.33862408995628357\n",
            "Training loss: 0.33505364259084064\n",
            "Val loss: 0.33869028091430664\n",
            "Training loss: 0.33664433161417645\n",
            "Val loss: 0.33865123987197876\n",
            "Training loss: 0.33898792664210003\n",
            "Val loss: 0.3386426568031311\n",
            "Training loss: 0.3369264006614685\n",
            "Val loss: 0.3386446535587311\n",
            "Training loss: 0.33408215641975403\n",
            "Val loss: 0.33857694268226624\n",
            "Training loss: 0.3343937595685323\n",
            "Val loss: 0.33852991461753845\n",
            "Training loss: 0.33439309398333233\n",
            "Val loss: 0.33847466111183167\n",
            "Training loss: 0.3351272741953532\n",
            "Val loss: 0.33850032091140747\n",
            "Training loss: 0.3334348301092784\n",
            "Val loss: 0.3385125696659088\n",
            "Training loss: 0.33463669816652936\n",
            "Val loss: 0.3385210335254669\n",
            "Training loss: 0.335160364707311\n",
            "Val loss: 0.3385169208049774\n",
            "Training loss: 0.33521785338719684\n",
            "Val loss: 0.3386157155036926\n",
            "Training loss: 0.33604345719019574\n",
            "Val loss: 0.3386004567146301\n",
            "Training loss: 0.3344661792119344\n",
            "Val loss: 0.3385941982269287\n",
            "Training loss: 0.3374074101448059\n",
            "Val loss: 0.3385924994945526\n",
            "Training loss: 0.34022385875384015\n",
            "Val loss: 0.33847716450691223\n",
            "Training loss: 0.33369388182957965\n",
            "Val loss: 0.33851978182792664\n",
            "Training loss: 0.33503464857737225\n",
            "Val loss: 0.33851954340934753\n",
            "Training loss: 0.33510926365852356\n",
            "Val loss: 0.3385407626628876\n",
            "Training loss: 0.33395371834437054\n",
            "Val loss: 0.3385155498981476\n",
            "Training loss: 0.3325546582539876\n",
            "Val loss: 0.3385094106197357\n",
            "Training loss: 0.33180655042330426\n",
            "Val loss: 0.3385244905948639\n",
            "Training loss: 0.33307992418607074\n",
            "Val loss: 0.3385751247406006\n",
            "Training loss: 0.3333050310611725\n",
            "Val loss: 0.33855336904525757\n",
            "Training loss: 0.33453654249509174\n",
            "Val loss: 0.338535875082016\n",
            "Training loss: 0.3350021541118622\n",
            "Val loss: 0.33853617310523987\n",
            "Training loss: 0.335293451944987\n",
            "Val loss: 0.33853885531425476\n",
            "Training loss: 0.33692437410354614\n",
            "Val loss: 0.33862993121147156\n",
            "Training loss: 0.3361753722031911\n",
            "Val loss: 0.33859819173812866\n",
            "Training loss: 0.3357994059721629\n",
            "Val loss: 0.33857277035713196\n",
            "Training loss: 0.3331839243570964\n",
            "Val loss: 0.3385414481163025\n",
            "Training loss: 0.33262057105700177\n",
            "Val loss: 0.3385292887687683\n",
            "Training loss: 0.33979283769925434\n",
            "Val loss: 0.33858057856559753\n",
            "Training loss: 0.33507423599561054\n",
            "Val loss: 0.3385641872882843\n",
            "Training loss: 0.33355972170829773\n",
            "Val loss: 0.3385024964809418\n",
            "Training loss: 0.3339039882024129\n",
            "Val loss: 0.3384738266468048\n",
            "Training loss: 0.33466050028800964\n",
            "Val loss: 0.3383926451206207\n",
            "Training loss: 0.33428959051767987\n",
            "Val loss: 0.338401198387146\n",
            "Training loss: 0.3344745834668477\n",
            "Val loss: 0.3383360505104065\n",
            "Training loss: 0.3353581329186757\n",
            "Val loss: 0.3383261263370514\n",
            "Training loss: 0.33316392699877423\n",
            "Val loss: 0.3383842408657074\n",
            "Training loss: 0.33346378803253174\n",
            "Val loss: 0.3384292423725128\n",
            "Training loss: 0.3325836757818858\n",
            "Val loss: 0.3385065793991089\n",
            "Training loss: 0.33393048246701557\n",
            "Val loss: 0.3385528028011322\n",
            "Training loss: 0.33318905035654706\n",
            "Val loss: 0.3384675085544586\n",
            "Training loss: 0.3338525394598643\n",
            "Val loss: 0.3383753299713135\n",
            "Training loss: 0.3336590031782786\n",
            "Val loss: 0.33837735652923584\n",
            "Training loss: 0.33385013540585834\n",
            "Val loss: 0.33833131194114685\n",
            "Training loss: 0.33262725671132404\n",
            "Val loss: 0.3382040560245514\n",
            "Training loss: 0.334344486395518\n",
            "Val loss: 0.3381554186344147\n",
            "Training loss: 0.3353784382343292\n",
            "Val loss: 0.3381567597389221\n",
            "Training loss: 0.3332682053248088\n",
            "Val loss: 0.3381488025188446\n",
            "Training loss: 0.33671648303667706\n",
            "Val loss: 0.33818504214286804\n",
            "Training loss: 0.3359769880771637\n",
            "Val loss: 0.3382120132446289\n",
            "Training loss: 0.33669041593869525\n",
            "Val loss: 0.33820217847824097\n",
            "Training loss: 0.3343492348988851\n",
            "Val loss: 0.3382529616355896\n",
            "Training loss: 0.33735543489456177\n",
            "Val loss: 0.3382717967033386\n",
            "Training loss: 0.3357536892096202\n",
            "Val loss: 0.33830732107162476\n",
            "Training loss: 0.33287495374679565\n",
            "Val loss: 0.33834338188171387\n",
            "Training loss: 0.3369005620479584\n",
            "Val loss: 0.33838093280792236\n",
            "Training loss: 0.33571524421374005\n",
            "Val loss: 0.33846601843833923\n",
            "Training loss: 0.33354934056599933\n",
            "Val loss: 0.33852991461753845\n",
            "Training loss: 0.3337327341238658\n",
            "Val loss: 0.3385339379310608\n",
            "Training loss: 0.33511953552563983\n",
            "Val loss: 0.33852794766426086\n",
            "Training loss: 0.3373928467432658\n",
            "Val loss: 0.33842095732688904\n",
            "Training loss: 0.33581556876500446\n",
            "Val loss: 0.3383800983428955\n",
            "Training loss: 0.33398409684499103\n",
            "Val loss: 0.3383653163909912\n",
            "Training loss: 0.33391212423642475\n",
            "Val loss: 0.3382972776889801\n",
            "Training loss: 0.33203279972076416\n",
            "Val loss: 0.3383854925632477\n",
            "Training loss: 0.3381422758102417\n",
            "Val loss: 0.33845284581184387\n",
            "Training loss: 0.33669353524843854\n",
            "Val loss: 0.3385169208049774\n",
            "Training loss: 0.3326614002386729\n",
            "Val loss: 0.33857211470603943\n",
            "Training loss: 0.3339596390724182\n",
            "Val loss: 0.33860328793525696\n",
            "Training loss: 0.3361356357733409\n",
            "Val loss: 0.338517963886261\n",
            "Training loss: 0.33395599325497943\n",
            "Val loss: 0.33842307329177856\n",
            "Training loss: 0.3314264416694641\n",
            "Val loss: 0.3384033143520355\n",
            "Training loss: 0.3353002369403839\n",
            "Val loss: 0.3383013904094696\n",
            "Training loss: 0.3377000192801158\n",
            "Val loss: 0.3382914066314697\n",
            "Training loss: 0.3341936568419139\n",
            "Val loss: 0.3382727801799774\n",
            "Training loss: 0.3355010549227397\n",
            "Val loss: 0.33822861313819885\n",
            "Training loss: 0.3344835837682088\n",
            "Val loss: 0.3381368815898895\n",
            "Training loss: 0.33415211240450543\n",
            "Val loss: 0.3380891680717468\n",
            "Training loss: 0.3316026528676351\n",
            "Val loss: 0.3380756974220276\n",
            "Training loss: 0.33574529488881427\n",
            "Val loss: 0.33808302879333496\n",
            "Training loss: 0.33362117409706116\n",
            "Val loss: 0.33810824155807495\n",
            "Training loss: 0.3337055742740631\n",
            "Val loss: 0.3380875289440155\n",
            "Training loss: 0.33688147862752277\n",
            "Val loss: 0.3380354642868042\n",
            "Training loss: 0.3333154122034709\n",
            "Val loss: 0.33801841735839844\n",
            "Training loss: 0.3334486087163289\n",
            "Val loss: 0.3380403518676758\n",
            "Training loss: 0.3376226524511973\n",
            "Val loss: 0.3380202651023865\n",
            "Training loss: 0.3347204923629761\n",
            "Val loss: 0.33797380328178406\n",
            "Training loss: 0.3328023950258891\n",
            "Val loss: 0.33799469470977783\n",
            "Training loss: 0.33273855845133465\n",
            "Val loss: 0.3380352556705475\n",
            "Training loss: 0.3343205650647481\n",
            "Val loss: 0.3380477726459503\n",
            "Training loss: 0.3338566521803538\n",
            "Val loss: 0.3380368947982788\n",
            "Training loss: 0.3344167272249858\n",
            "Val loss: 0.33804112672805786\n",
            "Training loss: 0.3324688772360484\n",
            "Val loss: 0.3380088806152344\n",
            "Training loss: 0.33364249269167584\n",
            "Val loss: 0.3379901051521301\n",
            "Training loss: 0.3361736337343852\n",
            "Val loss: 0.33799782395362854\n",
            "Training loss: 0.33324090639750165\n",
            "Val loss: 0.3379867672920227\n",
            "Training loss: 0.33549824357032776\n",
            "Val loss: 0.3379759192466736\n",
            "Training loss: 0.3337106704711914\n",
            "Val loss: 0.3379382789134979\n",
            "Training loss: 0.3350446919600169\n",
            "Val loss: 0.33793574571609497\n",
            "Training loss: 0.3377691407998403\n",
            "Val loss: 0.3379078507423401\n",
            "Training loss: 0.3334464430809021\n",
            "Val loss: 0.3380058705806732\n",
            "Training loss: 0.33495370546976727\n",
            "Val loss: 0.3380502462387085\n",
            "Training loss: 0.3305729230244954\n",
            "Val loss: 0.3381176292896271\n",
            "Training loss: 0.33508671323458356\n",
            "Val loss: 0.3381255567073822\n",
            "Training loss: 0.33509918053944904\n",
            "Val loss: 0.33810901641845703\n",
            "Training loss: 0.3357376456260681\n",
            "Val loss: 0.3381347954273224\n",
            "Training loss: 0.3341773549715678\n",
            "Val loss: 0.33816325664520264\n",
            "Training loss: 0.33441051840782166\n",
            "Val loss: 0.33821070194244385\n",
            "Training loss: 0.33594221870104474\n",
            "Val loss: 0.33820870518684387\n",
            "Training loss: 0.3343721926212311\n",
            "Val loss: 0.3382488787174225\n",
            "Training loss: 0.33365389704704285\n",
            "Val loss: 0.33826348185539246\n",
            "Training loss: 0.3337939977645874\n",
            "Val loss: 0.33833232522010803\n",
            "Training loss: 0.33315038681030273\n",
            "Val loss: 0.3384041488170624\n",
            "Training loss: 0.33347806334495544\n",
            "Val loss: 0.33840620517730713\n",
            "Training loss: 0.3334754804770152\n",
            "Val loss: 0.33835741877555847\n",
            "Training loss: 0.3355974356333415\n",
            "Val loss: 0.3383234143257141\n",
            "Training loss: 0.33484145005544025\n",
            "Val loss: 0.3383219838142395\n",
            "Training loss: 0.33605199058850604\n",
            "Val loss: 0.33829668164253235\n",
            "Training loss: 0.3359684149424235\n",
            "Val loss: 0.3382096588611603\n",
            "Training loss: 0.33268778522809345\n",
            "Val loss: 0.33825308084487915\n",
            "Training loss: 0.3345743219057719\n",
            "Val loss: 0.3382468521595001\n",
            "Training loss: 0.3347051938374837\n",
            "Val loss: 0.3382238447666168\n",
            "Training loss: 0.3335520923137665\n",
            "Val loss: 0.3382362425327301\n",
            "Training loss: 0.33377339442571\n",
            "Val loss: 0.3382229506969452\n",
            "Training loss: 0.33412272731463116\n",
            "Val loss: 0.3381991982460022\n",
            "Training loss: 0.3325938681761424\n",
            "Val loss: 0.3382023274898529\n",
            "Training loss: 0.3374967575073242\n",
            "Val loss: 0.3381115198135376\n",
            "Training loss: 0.3359044094880422\n",
            "Val loss: 0.33807653188705444\n",
            "Training loss: 0.3350018759568532\n",
            "Val loss: 0.3381344974040985\n",
            "Training loss: 0.33282989263534546\n",
            "Val loss: 0.33821484446525574\n",
            "Training loss: 0.3322659432888031\n",
            "Val loss: 0.3382764160633087\n",
            "Training loss: 0.33363183339436847\n",
            "Val loss: 0.338303804397583\n",
            "Training loss: 0.3348381717999776\n",
            "Val loss: 0.33829018473625183\n",
            "Training loss: 0.3355412979920705\n",
            "Val loss: 0.33836543560028076\n",
            "Training loss: 0.3330400188763936\n",
            "Val loss: 0.3383302092552185\n",
            "Training loss: 0.3349558214346568\n",
            "Val loss: 0.338302344083786\n",
            "Training loss: 0.3353270987669627\n",
            "Val loss: 0.3382725417613983\n",
            "Training loss: 0.33520517746607464\n",
            "Val loss: 0.33826375007629395\n",
            "Training loss: 0.3339393138885498\n",
            "Val loss: 0.33829885721206665\n",
            "Training loss: 0.3340061803658803\n",
            "Val loss: 0.33831092715263367\n",
            "Training loss: 0.3331921497980754\n",
            "Val loss: 0.3383309543132782\n",
            "Training loss: 0.33459193507830304\n",
            "Val loss: 0.33833199739456177\n",
            "Training loss: 0.33252755800882977\n",
            "Val loss: 0.3383602201938629\n",
            "Training loss: 0.336213340361913\n",
            "Val loss: 0.3384833037853241\n",
            "Training loss: 0.33490628004074097\n",
            "Val loss: 0.3384258449077606\n",
            "Training loss: 0.335699866215388\n",
            "Val loss: 0.33833667635917664\n",
            "Training loss: 0.33485402663548786\n",
            "Val loss: 0.3383348286151886\n",
            "Training loss: 0.33432597915331524\n",
            "Val loss: 0.3383440375328064\n",
            "Training loss: 0.3372957209746043\n",
            "Val loss: 0.3382914364337921\n",
            "Training loss: 0.33340368668238324\n",
            "Val loss: 0.33828645944595337\n",
            "Training loss: 0.33617959419886273\n",
            "Val loss: 0.33836835622787476\n",
            "Training loss: 0.33467990159988403\n",
            "Val loss: 0.33838731050491333\n",
            "Training loss: 0.3353692889213562\n",
            "Val loss: 0.33848699927330017\n",
            "Training loss: 0.3337810734907786\n",
            "Val loss: 0.3384932279586792\n",
            "Training loss: 0.33395980795224506\n",
            "Val loss: 0.33851709961891174\n",
            "Training loss: 0.3343195716540019\n",
            "Val loss: 0.3385138511657715\n",
            "Training loss: 0.3336840371290843\n",
            "Val loss: 0.3385431170463562\n",
            "Training loss: 0.3317624529202779\n",
            "Val loss: 0.3385869860649109\n",
            "Training loss: 0.3347592850526174\n",
            "Val loss: 0.3386218547821045\n",
            "Training loss: 0.332869013150533\n",
            "Val loss: 0.3385835289955139\n",
            "Training loss: 0.3351376454035441\n",
            "Val loss: 0.3386474847793579\n",
            "Training loss: 0.33286721507708233\n",
            "Val loss: 0.33865833282470703\n",
            "Training loss: 0.33456440766652423\n",
            "Val loss: 0.33862340450286865\n",
            "Training loss: 0.33273807168006897\n",
            "Val loss: 0.3386361300945282\n",
            "Training loss: 0.33387991786003113\n",
            "Val loss: 0.33859702944755554\n",
            "Training loss: 0.33457804719607037\n",
            "Val loss: 0.3386452794075012\n",
            "Training loss: 0.33372925718625385\n",
            "Val loss: 0.33855172991752625\n",
            "Training loss: 0.3349042534828186\n",
            "Val loss: 0.3385881185531616\n",
            "Training loss: 0.3355133533477783\n",
            "Val loss: 0.3385961055755615\n",
            "Training loss: 0.3334248960018158\n",
            "Val loss: 0.33857211470603943\n",
            "Training loss: 0.33600478370984393\n",
            "Val loss: 0.33861368894577026\n",
            "Training loss: 0.33302341898282367\n",
            "Val loss: 0.33865752816200256\n",
            "Training loss: 0.33616969982783\n",
            "Val loss: 0.3385986387729645\n",
            "Training loss: 0.3323171337445577\n",
            "Val loss: 0.33861151337623596\n",
            "Training loss: 0.3369893928368886\n",
            "Val loss: 0.33864569664001465\n",
            "Training loss: 0.3365779022375743\n",
            "Val loss: 0.33859652280807495\n",
            "Training loss: 0.33505354324976605\n",
            "Val loss: 0.33855053782463074\n",
            "Training loss: 0.337774654229482\n",
            "Val loss: 0.3384832441806793\n",
            "Training loss: 0.3365774154663086\n",
            "Val loss: 0.3384236693382263\n",
            "Training loss: 0.3347363869349162\n",
            "Val loss: 0.3384508788585663\n",
            "Training loss: 0.3343785007794698\n",
            "Val loss: 0.3384748101234436\n",
            "Training loss: 0.3349082370599111\n",
            "Val loss: 0.3384957015514374\n",
            "Training loss: 0.33260298768679303\n",
            "Val loss: 0.33843791484832764\n",
            "Training loss: 0.3325011928876241\n",
            "Val loss: 0.3383358120918274\n",
            "Training loss: 0.33791158596674603\n",
            "Val loss: 0.3382731080055237\n",
            "Training loss: 0.3347136874993642\n",
            "Val loss: 0.33825695514678955\n",
            "Training loss: 0.3362554609775543\n",
            "Val loss: 0.3382285535335541\n",
            "Training loss: 0.3326604863007863\n",
            "Val loss: 0.3382183611392975\n",
            "Training loss: 0.33216432730356854\n",
            "Val loss: 0.3381659686565399\n",
            "Training loss: 0.33531370759010315\n",
            "Val loss: 0.33808720111846924\n",
            "Training loss: 0.3371553917725881\n",
            "Val loss: 0.33810144662857056\n",
            "Training loss: 0.3328060805797577\n",
            "Val loss: 0.3381476104259491\n",
            "Training loss: 0.33479737242062885\n",
            "Val loss: 0.33813416957855225\n",
            "Training loss: 0.3349091410636902\n",
            "Val loss: 0.33822211623191833\n",
            "Training loss: 0.33223992586135864\n",
            "Val loss: 0.33831098675727844\n",
            "Training loss: 0.3337295949459076\n",
            "Val loss: 0.3382476270198822\n",
            "Training loss: 0.3344312906265259\n",
            "Val loss: 0.33825910091400146\n",
            "Training loss: 0.3322582542896271\n",
            "Val loss: 0.3382760286331177\n",
            "Training loss: 0.33235450585683185\n",
            "Val loss: 0.33832424879074097\n",
            "Training loss: 0.3340642750263214\n",
            "Val loss: 0.3383225202560425\n",
            "Training loss: 0.33357909321784973\n",
            "Val loss: 0.3383024334907532\n",
            "Training loss: 0.33345826466878253\n",
            "Val loss: 0.3383122682571411\n",
            "Training loss: 0.3354398210843404\n",
            "Val loss: 0.3383166491985321\n",
            "Training loss: 0.33602866530418396\n",
            "Val loss: 0.3383536636829376\n",
            "Training loss: 0.3365001281102498\n",
            "Val loss: 0.3383329510688782\n",
            "Training loss: 0.33341431617736816\n",
            "Val loss: 0.33830639719963074\n",
            "Training loss: 0.33407140771547955\n",
            "Val loss: 0.3383714258670807\n",
            "Training loss: 0.3333095808823903\n",
            "Val loss: 0.33840736746788025\n",
            "Training loss: 0.3347916603088379\n",
            "Val loss: 0.3384071886539459\n",
            "Training loss: 0.3317032257715861\n",
            "Val loss: 0.3383692502975464\n",
            "Training loss: 0.3318752845128377\n",
            "Val loss: 0.33840450644493103\n",
            "Training loss: 0.3375472029050191\n",
            "Val loss: 0.3383861482143402\n",
            "Training loss: 0.33314446608225506\n",
            "Val loss: 0.3383989632129669\n",
            "Training loss: 0.3329638143380483\n",
            "Val loss: 0.3384286165237427\n",
            "Training loss: 0.33386621872584027\n",
            "Val loss: 0.33844736218452454\n",
            "Training loss: 0.33486270904541016\n",
            "Val loss: 0.33838382363319397\n",
            "Training loss: 0.3350168764591217\n",
            "Val loss: 0.3383771777153015\n",
            "Training loss: 0.3345329364140828\n",
            "Val loss: 0.33843764662742615\n",
            "Training loss: 0.3324979444344838\n",
            "Val loss: 0.3384445607662201\n",
            "Training loss: 0.33613653977711994\n",
            "Val loss: 0.3384331166744232\n",
            "Training loss: 0.33296284079551697\n",
            "Val loss: 0.33847251534461975\n",
            "Training loss: 0.33106767137845355\n",
            "Val loss: 0.338409960269928\n",
            "Training loss: 0.333097239335378\n",
            "Val loss: 0.3383723795413971\n",
            "Training loss: 0.33240658044815063\n",
            "Val loss: 0.33826982975006104\n",
            "Training loss: 0.333721657594045\n",
            "Val loss: 0.33820071816444397\n",
            "Training loss: 0.33263399203618366\n",
            "Val loss: 0.338158518075943\n",
            "Training loss: 0.33185168107350665\n",
            "Val loss: 0.3381076753139496\n",
            "Training loss: 0.3360895812511444\n",
            "Val loss: 0.3381290137767792\n",
            "Training loss: 0.3346002399921417\n",
            "Val loss: 0.3380720913410187\n",
            "Training loss: 0.3349108397960663\n",
            "Val loss: 0.3380448818206787\n",
            "Training loss: 0.3361941874027252\n",
            "Val loss: 0.33797353506088257\n",
            "Training loss: 0.3347170948982239\n",
            "Val loss: 0.3380624055862427\n",
            "Training loss: 0.3357377847035726\n",
            "Val loss: 0.33802253007888794\n",
            "Training loss: 0.33292383948961896\n",
            "Val loss: 0.338140606880188\n",
            "Training loss: 0.33398953080177307\n",
            "Val loss: 0.33821678161621094\n",
            "Training loss: 0.3362326721350352\n",
            "Val loss: 0.3382630944252014\n",
            "Training loss: 0.3363898992538452\n",
            "Val loss: 0.3382820188999176\n",
            "Training loss: 0.3347388704617818\n",
            "Val loss: 0.3383229970932007\n",
            "Training loss: 0.33204267422358197\n",
            "Val loss: 0.33833104372024536\n",
            "Training loss: 0.33453816175460815\n",
            "Val loss: 0.3383030891418457\n",
            "Training loss: 0.33459582924842834\n",
            "Val loss: 0.3382323980331421\n",
            "Training loss: 0.33345097303390503\n",
            "Val loss: 0.3382387161254883\n",
            "Training loss: 0.3340508242448171\n",
            "Val loss: 0.3381929397583008\n",
            "Training loss: 0.33442578713099164\n",
            "Val loss: 0.3382905423641205\n",
            "Training loss: 0.33388163646062213\n",
            "Val loss: 0.33831238746643066\n",
            "Training loss: 0.32986515760421753\n",
            "Val loss: 0.3383854329586029\n",
            "Training loss: 0.33674609661102295\n",
            "Val loss: 0.33840638399124146\n",
            "Training loss: 0.3369524876276652\n",
            "Val loss: 0.3383846879005432\n",
            "Training loss: 0.33244820435841876\n",
            "Val loss: 0.33840683102607727\n",
            "Training loss: 0.3326516846815745\n",
            "Val loss: 0.33845260739326477\n",
            "Training loss: 0.33472904562950134\n",
            "Val loss: 0.33845219016075134\n",
            "Training loss: 0.3340705931186676\n",
            "Val loss: 0.3384155333042145\n",
            "Training loss: 0.33286840716997784\n",
            "Val loss: 0.33840590715408325\n",
            "Training loss: 0.33483612537384033\n",
            "Val loss: 0.3384389281272888\n",
            "Training loss: 0.33930333455403644\n",
            "Val loss: 0.3384404182434082\n",
            "Training loss: 0.33390118678410846\n",
            "Val loss: 0.33849474787712097\n",
            "Training loss: 0.33079953988393146\n",
            "Val loss: 0.33848172426223755\n",
            "Training loss: 0.33509623010953266\n",
            "Val loss: 0.3384473919868469\n",
            "Training loss: 0.3339497943719228\n",
            "Val loss: 0.3385040760040283\n",
            "Training loss: 0.3357706069946289\n",
            "Val loss: 0.33845967054367065\n",
            "Training loss: 0.3333113491535187\n",
            "Val loss: 0.33838707208633423\n",
            "Training loss: 0.33499913414319354\n",
            "Val loss: 0.33838486671447754\n",
            "Training loss: 0.3316240111986796\n",
            "Val loss: 0.33834025263786316\n",
            "Training loss: 0.3364546298980713\n",
            "Val loss: 0.33843928575515747\n",
            "Training loss: 0.3340808351834615\n",
            "Val loss: 0.33843275904655457\n",
            "Training loss: 0.33807113766670227\n",
            "Val loss: 0.33836501836776733\n",
            "Training loss: 0.33497557044029236\n",
            "Val loss: 0.33837229013442993\n",
            "Training loss: 0.33282923698425293\n",
            "Val loss: 0.3383341431617737\n",
            "Training loss: 0.33247052629788715\n",
            "Val loss: 0.3382703959941864\n",
            "Training loss: 0.3326735595862071\n",
            "Val loss: 0.3382554352283478\n",
            "Training loss: 0.33163148164749146\n",
            "Val loss: 0.33821991086006165\n",
            "Training loss: 0.335153728723526\n",
            "Val loss: 0.33820611238479614\n",
            "Training loss: 0.3330266575018565\n",
            "Val loss: 0.3381713330745697\n",
            "Training loss: 0.33496345082918805\n",
            "Val loss: 0.33819735050201416\n",
            "Training loss: 0.33518654108047485\n",
            "Val loss: 0.33821189403533936\n",
            "Training loss: 0.3358747164408366\n",
            "Val loss: 0.33818864822387695\n",
            "Training loss: 0.3346446951230367\n",
            "Val loss: 0.3381210267543793\n",
            "Training loss: 0.33150728543599445\n",
            "Val loss: 0.33811721205711365\n",
            "Training loss: 0.3335588872432709\n",
            "Val loss: 0.3380969762802124\n",
            "Training loss: 0.33515199025472003\n",
            "Val loss: 0.3381412923336029\n",
            "Training loss: 0.3362855811913808\n",
            "Val loss: 0.338018536567688\n",
            "Training loss: 0.3349625567595164\n",
            "Val loss: 0.3380005955696106\n",
            "Training loss: 0.3370437224706014\n",
            "Val loss: 0.33800503611564636\n",
            "Training loss: 0.333830585082372\n",
            "Val loss: 0.3380194306373596\n",
            "Training loss: 0.33381520708401996\n",
            "Val loss: 0.3380638062953949\n",
            "Training loss: 0.3349919617176056\n",
            "Val loss: 0.3380926847457886\n",
            "Training loss: 0.3350526491800944\n",
            "Val loss: 0.33802127838134766\n",
            "Training loss: 0.3327978452046712\n",
            "Val loss: 0.3381228744983673\n",
            "Training loss: 0.3356270094712575\n",
            "Val loss: 0.3381260633468628\n",
            "Training loss: 0.3351086378097534\n",
            "Val loss: 0.3381328880786896\n",
            "Training loss: 0.3337061603864034\n",
            "Val loss: 0.33812180161476135\n",
            "Training loss: 0.33366648356119794\n",
            "Val loss: 0.338105708360672\n",
            "Training loss: 0.334153413772583\n",
            "Val loss: 0.33808428049087524\n",
            "Training loss: 0.3314145704110463\n",
            "Val loss: 0.33814871311187744\n",
            "Training loss: 0.3364616433779399\n",
            "Val loss: 0.33815497159957886\n",
            "Training loss: 0.3336974581082662\n",
            "Val loss: 0.338202565908432\n",
            "Training loss: 0.33636876940727234\n",
            "Val loss: 0.33814048767089844\n",
            "Training loss: 0.3332662085692088\n",
            "Val loss: 0.3381035029888153\n",
            "Training loss: 0.33459540208180744\n",
            "Val loss: 0.33807387948036194\n",
            "Training loss: 0.33519590894381207\n",
            "Val loss: 0.3380473852157593\n",
            "Training loss: 0.33435942729314166\n",
            "Val loss: 0.3380272090435028\n",
            "Training loss: 0.3345020314057668\n",
            "Val loss: 0.3381284475326538\n",
            "Training loss: 0.3366873562335968\n",
            "Val loss: 0.3380741775035858\n",
            "Training loss: 0.33369023601214093\n",
            "Val loss: 0.3381018340587616\n",
            "Training loss: 0.3332433303197225\n",
            "Val loss: 0.3380933403968811\n",
            "Training loss: 0.33438703417778015\n",
            "Val loss: 0.338074654340744\n",
            "Training loss: 0.3322991232077281\n",
            "Val loss: 0.33799293637275696\n",
            "Training loss: 0.3327549894650777\n",
            "Val loss: 0.33798468112945557\n",
            "Training loss: 0.332945982615153\n",
            "Val loss: 0.33802926540374756\n",
            "Training loss: 0.3312934736410777\n",
            "Val loss: 0.33811134099960327\n",
            "Training loss: 0.33553213874499005\n",
            "Val loss: 0.3380470275878906\n",
            "Training loss: 0.3316084643205007\n",
            "Val loss: 0.33803245425224304\n",
            "Training loss: 0.33647653460502625\n",
            "Val loss: 0.3380281329154968\n",
            "Training loss: 0.33197824160257977\n",
            "Val loss: 0.3380460739135742\n",
            "Training loss: 0.333915650844574\n",
            "Val loss: 0.3380371034145355\n",
            "Training loss: 0.3315155903498332\n",
            "Val loss: 0.33806097507476807\n",
            "Training loss: 0.33541756868362427\n",
            "Val loss: 0.33801132440567017\n",
            "Training loss: 0.3319002687931061\n",
            "Val loss: 0.33803626894950867\n",
            "Training loss: 0.3349598745505015\n",
            "Val loss: 0.3379945755004883\n",
            "Training loss: 0.3346056044101715\n",
            "Val loss: 0.337979257106781\n",
            "Training loss: 0.3358205159505208\n",
            "Val loss: 0.3380049467086792\n",
            "Training loss: 0.33525417248408\n",
            "Val loss: 0.33796951174736023\n",
            "Training loss: 0.3364115357398987\n",
            "Val loss: 0.3379437029361725\n",
            "Training loss: 0.3324308693408966\n",
            "Val loss: 0.3380095660686493\n",
            "Training loss: 0.3333524366219838\n",
            "Val loss: 0.3380442261695862\n",
            "Training loss: 0.3348342974980672\n",
            "Val loss: 0.3381099998950958\n",
            "Training loss: 0.3329025407632192\n",
            "Val loss: 0.33812451362609863\n",
            "Training loss: 0.33437682191530865\n",
            "Val loss: 0.33812859654426575\n",
            "Training loss: 0.33587217330932617\n",
            "Val loss: 0.33808496594429016\n",
            "Training loss: 0.33682963252067566\n",
            "Val loss: 0.33801043033599854\n",
            "Training loss: 0.3348294099171956\n",
            "Val loss: 0.337895005941391\n",
            "Training loss: 0.3322529196739197\n",
            "Val loss: 0.3378981053829193\n",
            "Training loss: 0.3338982065518697\n",
            "Val loss: 0.33787044882774353\n",
            "Training loss: 0.33510710795720416\n",
            "Val loss: 0.33785247802734375\n",
            "Training loss: 0.33470289905865985\n",
            "Val loss: 0.3378537595272064\n",
            "Training loss: 0.3345211446285248\n",
            "Val loss: 0.3378588557243347\n",
            "Training loss: 0.33413001894950867\n",
            "Val loss: 0.3379458785057068\n",
            "Training loss: 0.33198194702466327\n",
            "Val loss: 0.33790838718414307\n",
            "Training loss: 0.3345357080300649\n",
            "Val loss: 0.3379616141319275\n",
            "Training loss: 0.33387566606203717\n",
            "Val loss: 0.3379806578159332\n",
            "Training loss: 0.33539559443791706\n",
            "Val loss: 0.3380282521247864\n",
            "Training loss: 0.33360639214515686\n",
            "Val loss: 0.33801010251045227\n",
            "Training loss: 0.334173043568929\n",
            "Val loss: 0.3380032777786255\n",
            "Training loss: 0.33345837394396466\n",
            "Val loss: 0.3379737436771393\n",
            "Training loss: 0.33573808272679645\n",
            "Val loss: 0.3378792405128479\n",
            "Training loss: 0.3345134953657786\n",
            "Val loss: 0.337854266166687\n",
            "Training loss: 0.337001512447993\n",
            "Val loss: 0.3378075957298279\n",
            "Training loss: 0.3342222174008687\n",
            "Val loss: 0.33779647946357727\n",
            "Training loss: 0.33530980348587036\n",
            "Val loss: 0.33779579401016235\n",
            "Training loss: 0.3341110050678253\n",
            "Val loss: 0.3378274142742157\n",
            "Training loss: 0.33489566047986347\n",
            "Val loss: 0.3378043472766876\n",
            "Training loss: 0.3343153993288676\n",
            "Val loss: 0.33777740597724915\n",
            "Training loss: 0.3361508548259735\n",
            "Val loss: 0.3377825915813446\n",
            "Training loss: 0.3324000835418701\n",
            "Val loss: 0.33780142664909363\n",
            "Training loss: 0.33484723170598346\n",
            "Val loss: 0.3378213942050934\n",
            "Training loss: 0.33446147044499713\n",
            "Val loss: 0.3378075957298279\n",
            "Training loss: 0.3335075080394745\n",
            "Val loss: 0.3378477692604065\n",
            "Training loss: 0.3341667652130127\n",
            "Val loss: 0.33794069290161133\n",
            "Training loss: 0.3339538673559825\n",
            "Val loss: 0.33794060349464417\n",
            "Training loss: 0.3351714511712392\n",
            "Val loss: 0.3379557132720947\n",
            "Training loss: 0.3342055181662242\n",
            "Val loss: 0.3379930257797241\n",
            "Training loss: 0.3341928521792094\n",
            "Val loss: 0.3379991054534912\n",
            "Training loss: 0.3330458501974742\n",
            "Val loss: 0.3380092978477478\n",
            "Training loss: 0.33332250515619916\n",
            "Val loss: 0.33792322874069214\n",
            "Training loss: 0.33345261216163635\n",
            "Val loss: 0.3378930687904358\n",
            "Training loss: 0.3349851866563161\n",
            "Val loss: 0.3378746211528778\n",
            "Training loss: 0.3349464039007823\n",
            "Val loss: 0.33795028924942017\n",
            "Training loss: 0.3319510718186696\n",
            "Val loss: 0.3380358815193176\n",
            "Training loss: 0.33281583587328595\n",
            "Val loss: 0.3380478024482727\n",
            "Training loss: 0.3336998224258423\n",
            "Val loss: 0.3380100429058075\n",
            "Training loss: 0.33500754833221436\n",
            "Val loss: 0.33800390362739563\n",
            "Training loss: 0.34021023909250897\n",
            "Val loss: 0.33796772360801697\n",
            "Training loss: 0.33163830637931824\n",
            "Val loss: 0.33794188499450684\n",
            "Training loss: 0.3325118025143941\n",
            "Val loss: 0.33794116973876953\n",
            "Training loss: 0.33302133282025653\n",
            "Val loss: 0.3379199504852295\n",
            "Training loss: 0.3353291352589925\n",
            "Val loss: 0.33788272738456726\n",
            "Training loss: 0.33090200026830036\n",
            "Val loss: 0.3378673195838928\n",
            "Training loss: 0.33401792248090106\n",
            "Val loss: 0.3378906548023224\n",
            "Training loss: 0.3331667184829712\n",
            "Val loss: 0.3378717303276062\n",
            "Training loss: 0.3353269696235657\n",
            "Val loss: 0.33794260025024414\n",
            "Training loss: 0.33477939168612164\n",
            "Val loss: 0.3380492925643921\n",
            "Training loss: 0.33716310063997906\n",
            "Val loss: 0.3381170630455017\n",
            "Training loss: 0.3319817582766215\n",
            "Val loss: 0.3381243348121643\n",
            "Training loss: 0.3309967319170634\n",
            "Val loss: 0.3380841612815857\n",
            "Training loss: 0.33470730980237323\n",
            "Val loss: 0.3380458652973175\n",
            "Training loss: 0.3328494032224019\n",
            "Val loss: 0.33800578117370605\n",
            "Training loss: 0.3336406946182251\n",
            "Val loss: 0.3379754424095154\n",
            "Training loss: 0.3348435660203298\n",
            "Val loss: 0.3379693925380707\n",
            "Training loss: 0.3324991861979167\n",
            "Val loss: 0.3379875719547272\n",
            "Training loss: 0.3338730037212372\n",
            "Val loss: 0.3380105793476105\n",
            "Training loss: 0.333667516708374\n",
            "Val loss: 0.33803898096084595\n",
            "Training loss: 0.3366534908612569\n",
            "Val loss: 0.3380383849143982\n",
            "Training loss: 0.3356453776359558\n",
            "Val loss: 0.33804646134376526\n",
            "Training loss: 0.3323279321193695\n",
            "Val loss: 0.3380890190601349\n",
            "Training loss: 0.3316200176874797\n",
            "Val loss: 0.3381383419036865\n",
            "Training loss: 0.33360124627749127\n",
            "Val loss: 0.3381388187408447\n",
            "Training loss: 0.33166759212811786\n",
            "Val loss: 0.3381161689758301\n",
            "Training loss: 0.3334050079186757\n",
            "Val loss: 0.3380894362926483\n",
            "Training loss: 0.3322351276874542\n",
            "Val loss: 0.33804410696029663\n",
            "Training loss: 0.33551259835561115\n",
            "Val loss: 0.338006854057312\n",
            "Training loss: 0.33204275369644165\n",
            "Val loss: 0.33793720602989197\n",
            "Training loss: 0.33144039909044903\n",
            "Val loss: 0.3378373384475708\n",
            "Training loss: 0.3370678822199504\n",
            "Val loss: 0.3378298878669739\n",
            "Training loss: 0.3365837832291921\n",
            "Val loss: 0.3377875089645386\n",
            "Training loss: 0.3337044318517049\n",
            "Val loss: 0.3377559781074524\n",
            "Training loss: 0.3348650534947713\n",
            "Val loss: 0.33772599697113037\n",
            "Training loss: 0.33578550815582275\n",
            "Val loss: 0.3377164602279663\n",
            "Training loss: 0.33084171017011005\n",
            "Val loss: 0.33769530057907104\n",
            "Training loss: 0.33254073063532513\n",
            "Val loss: 0.3376713991165161\n",
            "Training loss: 0.33366596698760986\n",
            "Val loss: 0.33775556087493896\n",
            "Training loss: 0.33420148491859436\n",
            "Val loss: 0.3377431631088257\n",
            "Training loss: 0.3339342574278514\n",
            "Val loss: 0.33767956495285034\n",
            "Training loss: 0.33535240093866986\n",
            "Val loss: 0.33765920996665955\n",
            "Training loss: 0.33168378472328186\n",
            "Val loss: 0.33773231506347656\n",
            "Training loss: 0.33471683661142987\n",
            "Val loss: 0.3377544581890106\n",
            "Training loss: 0.33231566349665326\n",
            "Val loss: 0.33783426880836487\n",
            "Training loss: 0.3319310247898102\n",
            "Val loss: 0.3378366231918335\n",
            "Training loss: 0.33260278900464374\n",
            "Val loss: 0.3378538489341736\n",
            "Training loss: 0.3341030180454254\n",
            "Val loss: 0.33789992332458496\n",
            "Training loss: 0.3331456780433655\n",
            "Val loss: 0.3379726707935333\n",
            "Training loss: 0.3341979483763377\n",
            "Val loss: 0.3379456698894501\n",
            "Training loss: 0.3343900144100189\n",
            "Val loss: 0.3379614055156708\n",
            "Training loss: 0.3332979679107666\n",
            "Val loss: 0.3379218578338623\n",
            "Training loss: 0.3400837779045105\n",
            "Val loss: 0.33789747953414917\n",
            "Training loss: 0.3338165581226349\n",
            "Val loss: 0.3379116356372833\n",
            "Training loss: 0.3315887749195099\n",
            "Val loss: 0.3379427492618561\n",
            "Training loss: 0.3345085581143697\n",
            "Val loss: 0.3379043638706207\n",
            "Training loss: 0.33392950892448425\n",
            "Val loss: 0.3378565311431885\n",
            "Training loss: 0.3330701192220052\n",
            "Val loss: 0.3378249704837799\n",
            "Training loss: 0.3333212037881215\n",
            "Val loss: 0.337789386510849\n",
            "Training loss: 0.3319445252418518\n",
            "Val loss: 0.33776792883872986\n",
            "Training loss: 0.3319738705952962\n",
            "Val loss: 0.33775678277015686\n",
            "Training loss: 0.3315640489260356\n",
            "Val loss: 0.33770856261253357\n",
            "Training loss: 0.33254125714302063\n",
            "Val loss: 0.3377057909965515\n",
            "Training loss: 0.33280206720034283\n",
            "Val loss: 0.337642103433609\n",
            "Training loss: 0.33317431807518005\n",
            "Val loss: 0.3376639485359192\n",
            "Training loss: 0.33161721626917523\n",
            "Val loss: 0.33760619163513184\n",
            "Training loss: 0.3353063464164734\n",
            "Val loss: 0.3376132547855377\n",
            "Training loss: 0.33332111438115436\n",
            "Val loss: 0.3376213014125824\n",
            "Training loss: 0.33349354068438214\n",
            "Val loss: 0.3376743197441101\n",
            "Training loss: 0.3332599699497223\n",
            "Val loss: 0.33766135573387146\n",
            "Training loss: 0.3319688042004903\n",
            "Val loss: 0.3376815617084503\n",
            "Training loss: 0.3351253569126129\n",
            "Val loss: 0.337650865316391\n",
            "Training loss: 0.33239896098772687\n",
            "Val loss: 0.3376568555831909\n",
            "Training loss: 0.33245862523714703\n",
            "Val loss: 0.33763647079467773\n",
            "Training loss: 0.3336416681607564\n",
            "Val loss: 0.33771374821662903\n",
            "Training loss: 0.33261017004648846\n",
            "Val loss: 0.33780327439308167\n",
            "Training loss: 0.33292049169540405\n",
            "Val loss: 0.3378618657588959\n",
            "Training loss: 0.33489562074343365\n",
            "Val loss: 0.3380099833011627\n",
            "Training loss: 0.3355548679828644\n",
            "Val loss: 0.33799540996551514\n",
            "Training loss: 0.3327020804087321\n",
            "Val loss: 0.33799293637275696\n",
            "Training loss: 0.3321199119091034\n",
            "Val loss: 0.33796244859695435\n",
            "Training loss: 0.3356164793173472\n",
            "Val loss: 0.33792421221733093\n",
            "Training loss: 0.33184702197710675\n",
            "Val loss: 0.33804914355278015\n",
            "Training loss: 0.33724407354990643\n",
            "Val loss: 0.33811354637145996\n",
            "Training loss: 0.3332946201165517\n",
            "Val loss: 0.33813443779945374\n",
            "Training loss: 0.33682716886202496\n",
            "Val loss: 0.33810240030288696\n",
            "Training loss: 0.335299293200175\n",
            "Val loss: 0.33803269267082214\n",
            "Training loss: 0.33304447929064435\n",
            "Val loss: 0.3379661738872528\n",
            "Training loss: 0.3326299885908763\n",
            "Val loss: 0.3379543125629425\n",
            "Training loss: 0.3330024778842926\n",
            "Val loss: 0.33797675371170044\n",
            "Training loss: 0.3316699465115865\n",
            "Val loss: 0.3379897475242615\n",
            "Training loss: 0.3353103697299957\n",
            "Val loss: 0.33793291449546814\n",
            "Training loss: 0.3353972335656484\n",
            "Val loss: 0.3379064202308655\n",
            "Training loss: 0.3355660339196523\n",
            "Val loss: 0.33788028359413147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp5-aRPiBcFr",
        "colab_type": "text"
      },
      "source": [
        "###History"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IxZMbAaBmoP",
        "colab_type": "code",
        "outputId": "83be4862-f994-4ef4-dde4-e86326ec0cf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        }
      },
      "source": [
        "plt.plot(range(epoch),loss_,label=\"training set\")\n",
        "plt.plot(range(epoch),loss_test,label=\"val set\")\n",
        "plt.ylim([0,1])\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f27998f4588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU6THCbECfSL",
        "colab_type": "text"
      },
      "source": [
        "###Blind test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "witg-s-wCiS7",
        "colab_type": "code",
        "outputId": "692ef986-510e-442f-d3d1-ea1700f77ff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "threshold = 0.5\n",
        "\n",
        "testset = dataset[dset.index == 'JTB-120'].dropna()\n",
        "X_test = torch.Tensor(testset.iloc[:,1:8].values)\n",
        "Y_test = torch.Tensor(testset.iloc[:,-2].values)\n",
        "X_test = X_test.to(device)\n",
        "Y_test = Y_test.to(device)\n",
        "with torch.no_grad():\n",
        "      logps = model(X_test)\n",
        "predict = torch.exp(logps).cpu()\n",
        "Y_predict = (predict[:,0] >= threshold).numpy().astype(int)\n",
        "Y_predict = predict.numpy().flatten()\n",
        "#Y_predict = predict.argmax(1)\n",
        "print(Y_predict.shape)\n",
        "testset['FRAC_PREDICT']=predict[:,1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3032,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxuTC_Z2CmR9",
        "colab_type": "text"
      },
      "source": [
        "####Visualize result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJehbpfqCpcJ",
        "colab_type": "code",
        "outputId": "70bbbd71-ad6a-42fd-e732-46757142f09e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        }
      },
      "source": [
        "d = testset['DEPTH']\n",
        "plt.style.use('default')\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(3, 6)\n",
        "fig.gca().invert_yaxis()\n",
        "plt.tick_params(labelsize=10)\n",
        "plt.xlim([-0.1,2])\n",
        "plt.xlabel('Fracture probability',{'fontsize': 10, 'fontweight' : 5})\n",
        "plt.ylabel('Depth (m)',{'fontsize': 10, 'fontweight' : 5})\n",
        "plt.title(testset.index.any(),{'fontsize': 15, 'fontweight' : 5})\n",
        "#plt.scatter(testset['INTENSITY']/5,d,color='r',label='Actual')\n",
        "plt.plot(testset['FRAC_PREDICT'],d,label='Proba')\n",
        "#plt.plot(testset['SPI']+0.5,d,label='SPI')\n",
        "plt.plot(testset['Pce'],d,label='Pce')\n",
        "plt.plot(testset['Pb'],d,label='Pb')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f279941dac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 300x600 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef-B9xbY5eq2",
        "colab_type": "text"
      },
      "source": [
        "####Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8ntAxsP5hcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, mean_absolute_error, mean_squared_error, r2_score, explained_variance_score, mean_squared_log_error\n",
        "Y_test = Y_test.cpu()\n",
        "msle_val = mean_squared_log_error(predict[:,1],Y_test)\n",
        "mae_val = mean_absolute_error(predict[:,1],Y_test)\n",
        "variance_val = explained_variance_score(predict[:,1],Y_test)\n",
        "r2_val = r2_score(predict[:,1],Y_test)\n",
        "#acc_val = accuracy_score(predict[:,1],Y_test)\n",
        "#cm = confusion_matrix(Y_test, Y_predict)\n",
        "print('msle= ',msle_val,'\\nmae= ',mae_val,'\\nvar= ',variance_val,'\\nr2= ',r2_val)\n",
        "print(cm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt3BBGRlqaCu",
        "colab_type": "text"
      },
      "source": [
        "### Save model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOYOlgf1qggy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "print(\"Optimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "model_save_name = 'model2604.pt'\n",
        "path = F\"/content/gdrive/My Drive/FracDet/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}